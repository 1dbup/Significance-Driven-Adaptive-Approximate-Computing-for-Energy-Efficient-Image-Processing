@article{Zhou2013,
abstract = {Linux CPUFreq subsystem provides a framework for implementing Dynamic$\backslash$nVoltage and Frequency Scaling (DVFS) to prolong batter life of mobile$\backslash$ndevices. Instead of creating hardware specific CPUFreq driver for$\backslash$nevery single ARM System on Chip (SoC) from different vendors, this$\backslash$npaper presents the design and implementation of a generic CPUFreq$\backslash$ndriver. Managing the hardware specific clock and voltage details$\backslash$nvia Linux Common Clock Framework and Regulator subsystem, the driver$\backslash$ncan scale CPU frequency and voltage in a generic way, and thus should$\backslash$nwork for the majority of the ARM SoCs today. Freescale i.MX6 Quad$\backslash$nwas taken as the target hardware to develop and test the driver.$\backslash$nA measurement on the hardware reports 37{\%} CPU power saving in a$\backslash$ntypical video playback application. The feedback from Linux community$\backslash$ntells that the driver works for OMAP and Calxeda processors as well,$\backslash$nand hence the driver was merged into Linux 3.7 release as a generic$\backslash$nCPUFreq driver for ARM SoCs.},
author = {Zhou, Lei and Lv, Qiang and Guo, Shengchao},
doi = {10.3991/ijoe.v9iS6.2797},
issn = {18681646},
journal = {Int. J. Online Eng.},
keywords = {ARM,CPUFreq,DVFS,I.MX6 Quad},
number = {SPECIALISSUE.6},
pages = {29--32},
title = {{A generic linux CPUFreq driver for ARM SoCs}},
volume = {9},
year = {2013}
}
@article{Zhang2005,
author = {Zhang, Yun and Abu-khzam, Faisal N and Baldwin, Nicole E and Chesler, Elissa J and Langston, Michael A and Samatova, Nagiza F},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2005 - Genome-Scale Computational Approaches to Memory-Intensive Applications in.pdf:pdf},
isbn = {1595930612},
journal = {Supercomputing2005},
number = {c},
pages = {12--16},
title = {{Genome-Scale Computational Approaches to Memory-Intensive Applications in}},
year = {2005}
}
@article{Zhang2013,
abstract = {We study the performance portability of OpenCL across diverse architectures including NVIDIA GPU, Intel Ivy Bridge CPU, and AMD Fusion APU. We present detailed performance analysis at assem- bly level on three exemplar OpenCL benchmarks: SGEMM, SpMV, and FFT. We also identify a number of tuning knobs that are critical to performance portability, including threads-data mapping, data layout, tiling size, data caching, and operation-specific factors. We further demonstrate that proper tuning could improve the OpenCL portable performance from the current 15{\%} to a potential 67{\%} of the state-of-the-art performance on the Ivy Bridge CPU. Finally, we evaluate the current OpenCL programming model, and propose a list of extensions that improve performance portability.},
author = {Zhang, Yao and {Sinclair II}, Mark and Chien, Andrew A},
doi = {10.1007/978-3-642-38750-0_11},
isbn = {978-3-642-38750-0},
issn = {03029743},
journal = {Supercomputing},
pages = {136--150},
title = {{Improving Performance Portability in OpenCL Programs}},
volume = {7905},
year = {2013}
}
@article{Zhang2011,
abstract = {DC/DC conversion has been an integral part of the power delivery chain in energy harvesting systems because the conventionally targeted synchronous computation load demands stable Vdd, which cannot in general be supplied by power harvesters directly. However, asynchronous computation loads, in addition to their potential power-saving capabilities, can be made tolerant to a much wider range of Vdd variance. This may open up opportunities for much more energy efficient methods of power delivery to be adopted. This paper presents in-depth investigations into the behavior and performance of different power delivery methods driving both asynchronous and synchronous load for the first time. A novel power delivery method, which employs a capacitor bank for adaptively storing the energy from power harvesters depending on load and source conditions, is developed. Its advantages, especially when driving asynchronous loads, are demonstrated through comprehensive comparative analysis.},
author = {Zhang, Xuefu and Shang, Delong and Xia, Fei and Yakovlev, Alex},
doi = {10.1109/ASYNC.2011.16},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2011 - A Novel Power Delivery Method for Asynchronous Loads in Energy Harvesting Systems(2).pdf:pdf},
isbn = {978-1-61284-973-7},
issn = {15504832},
journal = {2011 17th IEEE Int. Symp. Asynchronous Circuits Syst.},
keywords = {Switched capacitor DC/DC converter,asynchronous circuits,capacitor bank,energy harvesting,intelligent task and power scheduling,piezoelectric element},
month = {dec},
number = {4},
pages = {89--98},
publisher = {ACM},
title = {{A Novel Power Delivery Method for Asynchronous Loads in Energy Harvesting Systems}},
url = {http://ieeexplore.ieee.org/document/5770572/ http://dl.acm.org/citation.cfm?id=2043643.2043646},
volume = {7},
year = {2011}
}
@article{Zhang2015ApproxANN,
abstract = {Artificial Neural networks (ANNs) are one of the most well-established machine learning techniques and have a wide range of applications, such as Recognition, Mining and Synthesis (RMS). As many of these applications are inherently error-tolerant, in this work, we propose a novel approximate computing frame-work for ANN, namely ApproxANN. When compared to ex-isting solutions, ApproxANN considers approximation for both computation and memory accesses, thereby achieving more en-ergy savings. To be specific, ApproxANN characterizes the im-pact of neurons on the output quality in an effective and effi-cient manner, and judiciously determine how to approximate the computation and memory accesses of certain less critical neurons to achieve the maximum energy efficiency gain under a given quality constraint. Experimental results on various ANN applications with different datasets demonstrate the efficacy of the proposed solution.},
annote = {ApproxANN approximates the computation and memory accesses of certain less critical neurons based the criticality analysis to obtain energy efficiency under quality constraint.
A design methodolgy for Artificial Neural Networks utilising approximation techniques},
author = {Zhang, Qian and Wang, Ting and Tian, Ye and Yuan, Feng and Xu, Qiang},
doi = {10.7873/DATE.2015.0618},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2015 - ApproxANN An Approximate Computing Framework for Artificial Neural Network.pdf:pdf},
isbn = {9783981537048},
issn = {15301591},
journal = {Des. Autom. Test Eur. Conf. Exhib. (DATE), 2015},
number = {ii},
pages = {701--706},
title = {{ApproxANN: An Approximate Computing Framework for Artificial Neural Network}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7092478},
volume = {2015-},
year = {2015}
}
@article{Zhang,
author = {Zhang, Charles},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang - Unknown - Mars A 64-core ARMv8 Processor.pdf:pdf},
title = {{Mars : A 64-core ARMv8 Processor}}
}
@article{Zahnstecher,
author = {Zahnstecher, Brian},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zahnstecher - Unknown - Designer ' s Quick Guide to Power-Efficient Systems.pdf:pdf},
title = {{Designer ' s Quick Guide to Power-Efficient Systems}}
}
@article{Zahnstecher,
author = {Zahnstecher, Brian},
title = {{Designer ' s Quick Guide to Power-Efficient Systems}}
}
@article{Yuventi2013,
abstract = {Data centers represent an increasingly popular construction project type, supported by the continued growth in internet-based services. These facilities can, however, consume large amounts of electricity and - especially if growth trends continue - put strain on utility grids and energy resources. Many metrics have been proposed to evaluate and communicate energy use in data centers. In many cases, the goal is that these metrics will be used to develop energy conscious behavior and perhaps data center energy rating systems or building codes to reduce average energy use. In this paper, we examine one of the more popular metrics, Power Usage Effectiveness (PUE), and discuss its shortcomings toward effectively communicating energy consumption. Our inference is that PUE is an instantaneous representation of electrical energy consumption that encourages operators to report the minimum observed values of PUE. Hence, PUE only conveys an understanding of the minimum possible energy use. Instead, we propose the use of energy-based metrics or average PUE over a significant time period - e.g., a year - to better understand the energy efficiency of a data center and to develop energy rating/ranking systems and energy codes. ?? 2013 Elsevier B.V. All rights reserved.},
author = {Yuventi, Jumie and Mehdizadeh, Roshan},
doi = {10.1016/j.enbuild.2013.04.015},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yuventi, Mehdizadeh - 2013 - A critical analysis of Power Usage Effectiveness and its use in communicating data center energy consumptio.pdf:pdf},
isbn = {0378-7788},
issn = {03787788},
journal = {Energy Build.},
keywords = {Building rating systems,Data centers,Energy efficiency metrics,Power Usage Effectiveness},
pages = {90--94},
publisher = {Elsevier B.V.},
title = {{A critical analysis of Power Usage Effectiveness and its use in communicating data center energy consumption}},
url = {http://dx.doi.org/10.1016/j.enbuild.2013.04.015},
volume = {64},
year = {2013}
}
@article{Yazdanbakhsh2017,
author = {Yazdanbakhsh, A and Mahajan, D and Esmaeilzadeh, H and Lotfi-Kamran, P},
doi = {10.1109/MDAT.2016.2630270},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yazdanbakhsh et al. - 2017 - AxBench A Multiplatform Benchmark Suite for Approximate Computing(3).pdf:pdf},
isbn = {2168-2356 VO - 34},
journal = {IEEE Des. Test},
keywords = {Algorithm design and analysis,Approximate Computing,Approximate computing,Benchmark,Benchmark testing,CPU,Density measurement,GPU,Graphics processing units,Hardware Design,Measurement,Robots,Silicon},
number = {2},
pages = {60--68},
title = {{AxBench: A Multiplatform Benchmark Suite for Approximate Computing}},
volume = {34},
year = {2017}
}
@inproceedings{yang2015adaptive,
author = {Yang, Sheng and Shafik, Rishad A and Merrett, Geoff V and Stott, Edward and Levine, Joshua M and Davis, James and Al-Hashimi, Bashir M},
booktitle = {Power Timing Model. Optim. Simul. (PATMOS), 2015 25th Int. Work.},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - 2015 - Adaptive energy minimization of embedded heterogeneous systems using regression-based learning(2).pdf:pdf},
organization = {IEEE},
pages = {103--110},
title = {{Adaptive energy minimization of embedded heterogeneous systems using regression-based learning}},
year = {2015}
}
@article{Yakovlev2018,
author = {Yakovlev, Alex},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yakovlev, Yakovlev - 2018 - Energy current and computing Subject Areas Author for correspondence.pdf:pdf},
keywords = {electrical engineering,electromagnetism,microsystems,theory of computing},
title = {{Energy current and computing Subject Areas : Author for correspondence :}},
year = {2018}
}
@article{Yakovlev2015,
annote = {Energy Modulated computing, a challenge to develop hardware that can survive energy deficiency interludes.},
author = {Yakovlev, Alex},
doi = {10.1142/9781783266975_0013},
file = {:C$\backslash$:/Users/D/Documents/Uni/Papers/10.1.1.635.1158.pdf:pdf},
isbn = {9781783266975},
journal = {Transform. Reconfigurable Syst.},
number = {May},
pages = {237--263},
title = {{Enabling Survival Instincts in Electronic Systems: An Energy Perspective}},
url = {http://www.worldscientific.com/doi/abs/10.1142/9781783266975{\_}0013},
year = {2015}
}
@article{Yakovlev2011,
author = {Yakovlev, A},
doi = {10.1109/DATE.2011.5763216},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yakovlev - 2011 - Energy-modulated computing.pdf:pdf},
isbn = {978-3-9810801-8-6},
journal = {2011 Des. Autom. Test Eur.},
pages = {1--6},
title = {{Energy-modulated computing}},
url = {http://ieeexplore.ieee.org/document/5763216/},
volume = {7},
year = {2011}
}
@article{Xu2016,
abstract = {As one of the most promising energy-efficient computing paradigms, approximate computing has gained a lot of research attention in the past few years. This paper presents a survey of state-of-the-art work in all aspects of approximate computing and highlights future research challenges in this field.},
author = {Xu, Qiang and Mytkowicz, Todd and Kim, Nam Sung},
doi = {10.1109/MDAT.2015.2505723},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu, Mytkowicz, Kim - 2016 - Approximate Computing A Survey.pdf:pdf},
issn = {21682356},
journal = {IEEE Des. Test},
keywords = {Approximate computing,Approximation methods,Computational modeling,Computer languages,Probabilistic logic,Runtime},
number = {1},
pages = {8--22},
title = {{Approximate Computing: A Survey}},
volume = {33},
year = {2016}
}
@article{Xilinx,
author = {Xilinx, Mark Gustlin},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xilinx - Unknown - 400GbE Standards Update(2).pdf:pdf},
title = {{400GbE Standards Update}}
}
@article{Xilinx2015,
author = {Xilinx, Gilles Garcia},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xilinx - 2015 - Why 25GE is the Best Choice for Data Centers Outline - update(2).pdf:pdf},
number = {April},
title = {{Why 25GE is the Best Choice for Data Centers Outline - update}},
year = {2015}
}
@article{Wu2013,
abstract = {Heat-assisted magnetic recording (HAMR) is being developed as the next-generation magnetic recording technology. Critical aspects of this technology, such as plasmonic near-field transducer (NFT) and high anisotropy granular FePt media, have been demonstrated and reported. However, progress with areal density was limited until recently. In this paper, we report a basic technology demonstration (BTD) of HAMR, at 1.007 Tbpsi with a linear density of 1975 kBPI and track density of 510 kTPI, resulting from advances in magnetic recording heads with NFT and FePtX media. This demonstration not only shows significant areal density improvement over previously reported HAMR demos, more significantly, it shows HAMR recording at a much higher linear density compared to previous reports. It is an important milestone for the development of such a new technology. Many challenges still remain to bring this technology to market, such as system reliability and further advancement of areal density.},
author = {Wu, Alexander Q. and Kubota, Yukiko and Klemmer, Timothy and Rausch, Tim and Peng, Chubing and Peng, Yingguo and Karns, Darren and Zhu, Xiaobin and Ding, Yinfeng and Chang, Eric K.C. and Zhao, Yongjun and Zhou, Hua and Gao, Kaizhong and Thiele, Jan Ulrich and Seigler, Mike and Ju, Ganping and Gage, Edward},
doi = {10.1109/TMAG.2012.2219513},
file = {:C$\backslash$:/Users/D/Documents/Uni/Papers/HAMR{\_}Areal{\_}Density{\_}Demonstration{\_}of{\_}1{\_}Tb.pdf:pdf},
isbn = {0018-9464},
issn = {00189464},
journal = {IEEE Trans. Magn.},
keywords = {Basic technology demonstration (BTD) demo,FePtX media,heat-assisted magnetic recording (HAMR),near-field transducer (NFT)},
number = {2},
pages = {779--782},
title = {{HAMR areal density demonstration of 1+ tbpsi on spinstand}},
volume = {49},
year = {2013}
}
@article{Wood2009,
abstract = {This paper proposes a new approach to magnetic recording based on shingled writing and two-dimensional readback and signal-processing. This approach continues the use of conventional granular media but proposes techniques such that a substantial fraction of one bit of information is stored on each grain. Theoretically, areal-densities of the order of 10 Terabits per square inch may be achievable. In this paper we examine the feasibility of this two-dimensional magnetic recording (TDMR) and identify the significant challenges that must be overcome to achieve this vision.},
author = {Wood, Roger and Williams, Mason and Kavcic, Aleksandar and Miles, Jim},
doi = {10.1109/TMAG.2008.2010676},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wood et al. - 2009 - The feasibility of magnetic recording at 10 terabits per square inch on conventional media(2).pdf:pdf},
isbn = {0018-9464 VO - 45},
issn = {00189464},
journal = {IEEE Trans. Magn.},
keywords = {Areal-density,Capacity,Granular media,Magnetic recording,Shingled writing,Two-dimensional},
number = {2},
pages = {917--921},
title = {{The feasibility of magnetic recording at 10 terabits per square inch on conventional media}},
volume = {45},
year = {2009}
}
@article{Wood2015,
abstract = {Two-dimensional magnetic recording (TDMR) was proposed in 2008 as a means of pushing beyond 1 Tbit/in2 areal-density yet staying with relatively conventional magnetic components. A density of approximately 1 Tbit/in2 is considered to be the limit for conventional perpendicular magnetic recording. The original concept for TDMR has evolved greatly since it was originally proposed. This evolution has occurred as more information has become available about the characteristics of shingled tracks and 2-D read-back signals and as constraints have become apparent in the implementation of recording components and electronics. With today's technology, the projected gains are smaller than originally envisioned. As a very practical first step that introduces TDMR and provides modest gains without incurring undue complexity or cost, this paper focuses on the idea of a stacked two-element reader operating on shingled magnetic recording tracks and a two-input equalizer feeding a standard data detector. The dual-reader gains are assessed based on combining waveforms captured from a single reader at many off-track positions, track-pitches, and radii.},
author = {Wood, Roger and Galbraith, Rick and Coker, Jonathan},
doi = {10.1109/TMAG.2014.2354632},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wood, Galbraith, Coker - 2015 - 2-D Magnetic Recording Progress and Evolution(3).pdf:pdf},
issn = {0018-9464},
journal = {IEEE Trans. Magn.},
keywords = {2-D magnetic recording (TDMR),2D magnetic recording,2D read-back signals,Equalizers,Head,Interference,Kernel,Magnetic heads,Magnetic recording,Noise,areal-density,data detection,data detector,dual-reader gains,equalisers,equalization,hard disk drive (HDD),inter-track interference,magnetic components,magnetic recording,off-track positions,perpendicular magnetic recording,recording electronics,shingled magnetic recording (SMR),shingled magnetic recording tracks,shingled recording,single reader,stacked two-element reader,track-pitches,two-input equalizer feeding,waveforms},
number = {4},
pages = {1--7},
title = {{2-D Magnetic Recording: Progress and Evolution}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7109980},
volume = {51},
year = {2015}
}
@article{Wood2002,
abstract = {It is believed that areal densities approaching one terabit per$\backslash$nsquare inch will ultimately be achieved with conventional magnetic$\backslash$nrecording technology. Such densities require that very sharp transitions$\backslash$nbe written on well-defined extremely narrow tracks in a very finely$\backslash$ngrained thermally stable recording medium. These requirements drive the$\backslash$nkey characteristics and geometries of the read and write heads and of$\backslash$nthe recording medium itself. This paper re-examines several aspects of a$\backslash$nproposed terabit per square inch recording system and discusses some of$\backslash$nthe critical technologies required to support such extreme areal$\backslash$ndensities. These include the necessity of very low magnetic spacing, the$\backslash$nability to fabricate very narrow-track write/read heads, and the need$\backslash$nfor exquisite track-following capability. One of the biggest challenges$\backslash$nat these extreme areal densities will be to develop a recording system$\backslash$nthat can also deliver very high data rates},
author = {Wood, Roger W. and Miles, Jim and Olson, Terry},
doi = {10.1109/TMAG.2002.1017761},
file = {:C$\backslash$:/Users/D/Documents/Uni/Papers/01017761.pdf:pdf},
isbn = {0018-9464},
issn = {00189464},
journal = {IEEE Trans. Magn.},
keywords = {Areal density,Data rate,Magnetic recording},
number = {4 I},
pages = {1711--1718},
publisher = {IEEE},
title = {{Recording technologies for terabit per square inch systems}},
volume = {38},
year = {2002}
}
@article{Wood2000,
abstract = {This paper explores the feasibility of implementing conventional magnetic recording technology at densities up to one Terabit per square inch. The key limiting physical factor is the superparamagnetic effect (thermal stability) in the recording medium. Ambient thermal energy can cause the magnetic signals to decay. The requirement for thermal stability over periods of years dictates a lower limit to the size of magnetic grains (switching units) in the recording medium. To achieve the highest areal densities, it will be necessary to use a magnetic recording configuration capable of writing and storing data on very small magnetic grains together with a signal processing system capable of recovering data reliably when each bit is recorded on very few such grains. In addition to these physical effects, there are a number of practical engineering factors that must be considered: tolerances on the head geometry, reliability of head-disk interface, track-following accuracy. In an example system, we use a perpendicular recording configuration since it appears to offer some advantage in terms of maximizing the number of stable magnetic grains per unit area. The readback signals are processed by equalization to a simple binary eye followed by soft detection of a low-rate simple parity check code. The example system approaches a density of 1 Terabit per square inch and allows 3 dB of margin against thermal decay, adjacent track interference, etc.},
author = {Wood, R.},
doi = {10.1109/20.824422},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wood - 2000 - The feasibility of magnetic recording at 1 Terabit per square inch(2).pdf:pdf},
isbn = {0018-9464},
issn = {0018-9464},
journal = {IEEE Trans. Magn.},
number = {1},
pages = {917--923},
title = {{The feasibility of magnetic recording at 1 Terabit per square inch}},
volume = {36},
year = {2000}
}
@article{RonWilson,
abstract = {System Design view of the IoT},
annote = {Altera web based system design article},
author = {Wilson, Ron},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wilson - Unknown - Rethinking the Internet of Things.pdf:pdf},
journal = {Syst. Des.},
title = {{Rethinking the Internet of Things}},
url = {http://systemdesign.altera.com/rethinking-the-internet-of-things/?utm{\_}source=altera{\&}utm{\_}medium=email{\&}utm{\_}campaign=system{\_}design{\_}journal{\&}utm{\_}content=}
}
@misc{Wei2015,
abstract = {We review possible architectures for 400 Gigabit Ethernet links based on advanced modulation formats for the first time. Their optical link power budget, digital complexity, and power dissipation are compared via simulations. The challenges of implementing the physical layer are discussed.},
author = {Wei, Jinlong and Cheng, Qixiang and Penty, Richard V. and White, Ian H. and Cunningham, David G.},
booktitle = {IEEE Commun. Mag.},
doi = {10.1109/MCOM.2015.7045407},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Wei et al. - 2015 - 400 Gigabit Ethernet using advanced modulation formats Performance, complexity, and power dissipation(2).pdf:pdf},
issn = {01636804},
title = {{400 Gigabit Ethernet using advanced modulation formats: Performance, complexity, and power dissipation}},
year = {2015}
}
@article{Wang2004a,
abstract = {Objective image and video quality measures play important roles in a variety of image and video processing applications, such as compression, communication, printing, analysis, registration, restoration, enhancement and watermarking. Most proposed quality assessment approaches in the literature are error sensitivity-based methods. In this paper, we follow a new philosophy in designing image and video quality metrics, which uses structural distortion as an estimate of perceived visual distortion. A computationally efficient approach is developed for full-reference (FR) video quality assessment. The algorithm is tested on the video quality experts group Phase I FR-TV test data set. {\textcopyright} 2003 Elsevier B.V. All rights reserved.},
author = {Wang, Zhou and Lu, Ligang and Bovik, Alan C.},
doi = {10.1016/S0923-5965(03)00076-6},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Lu, Bovik - 2004 - Video quality assessment based on structural distortion measurement(3).pdf:pdf},
isbn = {0780376226},
issn = {09235965},
journal = {Signal Process. Image Commun.},
keywords = {Error sensitivity,Human visual system,Image quality assessment,Structural distortion,Video quality assessment,Video quality experts group (VQEG)},
number = {2},
pages = {121--132},
title = {{Video quality assessment based on structural distortion measurement}},
volume = {19},
year = {2004}
}
@article{Wang2004,
abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a Structural Similarity Index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000.},
author = {Wang, Zhou and Bovik, Alan Conrad and Sheikh, Hamid Rahim and Simoncelli, Eero P. and {Z. Wang, A. C. Bovik}, H. R. Sheikh and E. P. Simoncelli and Simoncelli, Eero P.},
doi = {10.1109/TIP.2003.819861},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2004 - Image quality assessment From error visibility to structural similarity(2).pdf:pdf},
isbn = {9781439829356},
issn = {10577149},
journal = {IEEE Trans. Image Process.},
keywords = {Error sensitivity,Human visual system (HVS),Image coding,Image quality assessment,JPEG,JPEG2000,Perceptual quality,Structural information,Structural similarity (SSIM),error sensitivity,human visual system,hvs,image coding,image quality assessment,jpeg,jpeg2000,perceptual quality,structural information,structural simi-},
number = {4},
pages = {600--612},
pmid = {15376593},
title = {{Image quality assessment: From error visibility to structural similarity}},
volume = {13},
year = {2004}
}
@article{Wang2002,
abstract = {We propose a new universal objective image quality index, which is easy to calculate and applicable to various image processing applications. Instead of using traditional error summation methods, the proposed index is designed by modeling any image distortion as a combination of three factors: loss of correlation, luminance distortion, and contrast distortion. Although the new index is mathematically defined and no human visual system model is explicitly employed, our experiments on various image distortion types indicate that it performs significantly better than the widely used distortion metric mean squared error. Demonstrative images and an efficient MATLAB implementation of the algorithm are available online at http://anchovy.ece.utexas.edu//spl sim/zwang/research/quality{\_}index/demo.html.},
annote = {A better QoS result than PSNR},
author = {Wang, Zhou and Bovik, A C},
doi = {10.1109/97.995823},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Bovik - 2002 - A Universal Image Quality Index(2).pdf:pdf},
isbn = {1070-9908},
issn = {1070-9908},
journal = {IEEE Trans. Signal Process. Lett.},
keywords = {Distortion measurement,Dynamic range,Humans,Image quality,MATLAB implementation,Mathematical model,PSNR,Signal to noise ratio,Testing,Visual system,contrast distortion,image distortion,image processing,image processing applications,loss of correlation,luminance distortion,universal image quality index},
number = {4},
pages = {81--84},
title = {{A Universal Image Quality Index}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=995823},
volume = {9},
year = {2002}
}
@article{Wang2016,
abstract = {Traditional cloud service providers build large data-centers with a huge number of connected commodity computers to meet the ever-growing demand on performance. However, the growth potential of these data-centers is limited by their corresponding energy consumption and thermal issues. Energy efficiency becomes a key issue of building large-scale cloud computing centers. To solve this issue, we propose a standalone SOPC (System on a Programmable Chip) based platform for cloud applications. We improve the energy efficiency for cloud computing platforms with two techniques. First, we propose a massive-sessions optimized TCP/IP hardware stack using a macro-pipeline architecture. It enables the hardware acceleration of pipelining execution of network packet offloading and application level data processing. This achieves higher energy efficiency while maintaining peak performance. Second, we propose a online dynamic scheduling strategy. It can reconfigure or shut down FPGA nodes according to workload variance to reduce the runtime energy consumption in a standalone SOPC based reconfigurable cluster system. Two case studies including a webserver application and a cloud based ECG (electrocardiogram) classification application are developed to validate the effectiveness of the proposed platform. Evaluation results show that our SOPC based cloud computing platform can achieve up to 418X improvement in terms of energy efficiency over commercial cloud systems.},
annote = {Deals with FPGA and cloud applications used in data transfer and compute intensive applications performance. Proposes a performance model for both scenarios.},
author = {Wang, Xu and Zhu, Yongxin and Ha, Yajun and Qiu, Meikang and Huang, Tian and Si, Xueming and Wu, Jiangxing},
doi = {10.1016/j.sysarc.2016.11.009},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2016 - An energy-efficient system on a programmable chip platform for cloud applications(2).pdf:pdf},
issn = {13837621},
journal = {J. Syst. Archit.},
keywords = {Cloud computing,ECG classification,FPGA,Performanc,Performance analysis,Reconfigurable architectures,Web server},
pages = {1--16},
publisher = {Elsevier B.V.},
title = {{An energy-efficient system on a programmable chip platform for cloud applications}},
url = {http://dx.doi.org/10.1016/j.sysarc.2016.11.009},
volume = {0},
year = {2016}
}
@article{Vries2018,
author = {Vries, Alex De},
doi = {10.1016/j.joule.2018.04.016},
issn = {2542-4351},
journal = {Joule},
number = {5},
pages = {801--805},
publisher = {Elsevier Inc.},
title = {{Bitcoin ' s Growing Energy Problem}},
url = {https://doi.org/10.1016/j.joule.2018.04.016},
volume = {2},
year = {2018}
}
@article{Vries2018,
author = {Vries, Alex De},
doi = {10.1016/j.joule.2018.04.016},
file = {:C$\backslash$:/Users/D/Documents/Uni/Papers/1-s2.0-S2542435118301776-main.pdf:pdf},
issn = {2542-4351},
journal = {Joule},
number = {5},
pages = {801--805},
publisher = {Elsevier Inc.},
title = {{Bitcoin ' s Growing Energy Problem}},
url = {https://doi.org/10.1016/j.joule.2018.04.016},
volume = {2},
year = {2018}
}
@article{Volder1959,
abstract = {JACK VOLDER t T HE "COordinate Rotation DIgital Computer" computing technique can be used to solve, in one computing operation and with equal speed, the relationships involved in plane coordinate rotation; con-version from rectangular to polar coordinates; multipli-cation; division; or the conversion between a binary-and a mixed-radix system. The CORDIC computer can be described as an entire transfer computer with a special serial arithmetic unit, consisting of 3 shift registers, 3 adder-subtractors, and special interconnections. The arithmetic unit performs a sequence of simultaneous conditional additions or sub-tractions of shifted numbers to each register. This per-formance is similar to a division operation in a conven-tional computer. Only the trigonometric algorithms used in the CORDIC computing technique will be covered in this paper. These algorithms are suitable only for use with a binary code. This fact possibly accounts for their late appearance as a numerical computing technique. Matrix theory, complex-number theory, or trigonometric iden-tities can be used to prove rigorously these algorithms. However, to help give a more intuitive and pictorial understanding of the basic technique, plane trigonom-etry and analytical geometry are used in this explana-tion whenever possible.},
author = {Volder, Jack},
doi = {10.1145/1457838.1457886},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Volder - 1959 - The CORDIC Computing Technique(2).pdf:pdf},
isbn = {9783851253269},
journal = {West. Jt. Comput. Conf.},
pages = {5},
title = {{The CORDIC Computing Technique}},
url = {https://www.computer.org/csdl/proceedings/afips/1959/5054/00/50540257.pdf},
year = {1959}
}
@article{Viola2004,
abstract = {This paper describes a face detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the “Integral Image” which allows the features used by our detector to be computed very quickly. The second is a simple and efficient classifier which is built using the AdaBoost learning algo-rithm (Freund and Schapire, 1995) to select a small number of critical visual features from a very large set of potential features. The third contribution is a method for combining classifiers in a “cascade” which allows back-ground regions of the image to be quickly discarded while spending more computation on promising face-like regions. A set of experiments in the domain of face detection is presented. The system yields face detection perfor-mance comparable to the best previous systems (Sung and Poggio, 1998; Rowley et al., 1998; Schneiderman and Kanade, 2000; Roth et al., 2000). Implemented on a conventional desktop, face detection proceeds at 15 frames per second.},
author = {Viola, Paul and Jones, Mj},
doi = {10.1023/B:VISI.0000013087.49260.fb},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Viola, Jones - 2004 - Robust real-time face detection(2).pdf:pdf},
isbn = {0769511430},
issn = {0920-5691},
journal = {Int. J. Comput. Vis.},
keywords = {boosting,face detection,human sensing},
number = {2},
pages = {137--154},
title = {{Robust real-time face detection}},
url = {http://link.springer.com/article/10.1023/B:VISI.0000013087.49260.fb},
volume = {57},
year = {2004}
}
@article{Viola2001a,
abstract = {This paper describes a visual object detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the “Integral Image” which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features and yields extremely efficient classifiers [6]. The third contribution is a method for combining classifiers in a “cascade” which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. A set of experiments in the domain of face detection are presented. The system yields face detection performace comparable to the best previous systems [18, 13, 16, 12, 1]. Implemented on a conventional desktop, face detection proceeds at 15 frames per second.},
author = {Viola, Paul and Jones, Michael},
doi = {http://dx.doi.org/10.1023/B:VISI.0000013087.49260.fb},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Viola, Jones - 2001 - Robust real-time object detection(2).pdf:pdf},
isbn = {1094670599130},
issn = {09205691},
journal = {Int. J. Comput. Vis.},
number = {2},
pages = {137--154},
pmid = {7143246},
title = {{Robust real-time object detection}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Robust+Real-time+Object+Detection{\#}0},
volume = {57},
year = {2001}
}
@article{Viola2001,
abstract = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the "integral image" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a "cascade" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
author = {Viola, P and Jones, M},
doi = {10.1109/CVPR.2001.990517},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Viola, Jones - 2001 - Rapid object detection using a boosted cascade of simple features.pdf:pdf},
isbn = {0-7695-1272-0},
issn = {1063-6919},
journal = {Comput. Vis. Pattern Recognit.},
keywords = {AdaBoost,Detectors,Face detection,Filters,Focusing,Image representation,Integral Image,Machine learning,Object detection,Pixel,Robustness,Skin,background regions,boosted simple feature cascade,classifiers,face detection,feature extraction,image classification,image processing,image representation,integral image,learning (artificial intelligence),machine learning,object detection,object specific focus-of-attention mechanism,rapid object detection,real-time applications,statistical guarantees,visual object detection},
mendeley-tags = {Integral Image},
number = {C},
pages = {I----511----I----518},
pmid = {7143246},
title = {{Rapid object detection using a boosted cascade of simple features}},
volume = {1},
year = {2001}
}
@article{Venkataramani2015ml,
abstract = {Supervised machine-learning algorithms are used to solve classifi- cation problems across the entire spectrum of computing platforms, from data centers to wearable devices, and place significant de- mand on their computational capabilities. In this paper, we propose scalable-effort classifiers, a new approach to optimizing the energy efficiency of supervised machine-learning classifiers. We observe that the inherent classification difficulty varies widely across inputs in real-world datasets; only a small fraction of the inputs truly re- quire the full computational effort of the classifier, while the large majority can be classified correctly with very low effort. Yet, state- of-the-art classification algorithms expend equal effort on all inputs, irrespective of their difficulty. To address this inefficiency, we in- troduce the concept of scalable-effort classifiers, or classifiers that dynamically adjust their computational effort depending on the dif- ficulty of the input data, while maintaining the same level of accu- racy. Scalable effort classifiers are constructed by utilizing a chain of classifiers with increasing levels of complexity (and accuracy). Scalable effort execution is achieved by modulating the number of stages used for classifying a given input. Every stage in the chain contains an ensemble of biased classifiers, where each biased classi- fier is trained to detect a single class more accurately. The degree of consensus between the biased classifiers' outputs is used to decide whether classification can be terminated at the current stage or not. Our methodology thus allows us to transform any given classifica- tion algorithm into a scalable-effort chain. We build scalable-effort versions of 8 popular recognition applications using 3 different clas- sification algorithms. Our experiments demonstrate that scalable- effort classifiers yield 2.79× reduction in average operations per in- put, which translates to 2.3× and 1.5× improvement in energy for hardware and software implementations, respectively.},
annote = {The scalable idea used in this paper would be useful applied to scalable approx -{\textgreater} accurate applied to moving and static image features},
author = {Venkataramani, Swagath and Raghunathan, Anand and Liu, Jie and Shoaib, Mohammed},
doi = {10.1145/2744769.2744904},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Venkataramani et al. - 2015 - Scalable-effort classifiers for energy-efficient machine learning.pdf:pdf},
isbn = {9781450335201},
issn = {0738100X},
journal = {Proc. 52nd Annu. Des. Autom. Conf. - DAC '15},
keywords = {Approximate Computing,Energy Efficiency,Input Adaptive Systems,Machine Learning Classifiers,approximate computing,energy efficiency,input adaptive systems,machine learning classifiers},
pages = {1--6},
title = {{Scalable-effort classifiers for energy-efficient machine learning}},
url = {http://doi.acm.org/10.1145/2744769.2744904{\%}5Cnhttp://dl.acm.org/citation.cfm?doid=2744769.2744904 http://dl.acm.org/citation.cfm?doid=2744769.2744904},
year = {2015}
}
@article{Venkataramani2015,
abstract = {—Recent years have witnessed significant interest in the area of approximate computing. Much of this interest stems from the quest for new sources of computing efficiency in the face of diminishing benefits from technology scaling. We argue that trends in computing workloads will greatly increase the opportunities for approximate computing, describe the vision and key principles that have guided our work in this area, and outline a range of approximate computing techniques that we have developed at all layers of the computing stack, spanning circuits, architecture, and software.},
annote = {Assessment of the various approx techniques across the computing stack},
author = {Venkataramani, Swagath and Chakradhar, Srimat T and Roy, Kaushik and Raghunathan, Anand},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Venkataramani et al. - 2015 - Computing approximately, and efficiently.pdf:pdf},
isbn = {9783981537048},
issn = {15301591},
journal = {Des. Autom. Test Eur. Conf. Exhib. (DATE), 2015},
keywords = {Algorithm design and analysis,Approximation algorithms,Approximation methods,Computer architecture,Hardware,Resilience,Software,approximate computing techniques,computing efficiency,computing stack,computing workloads,inference mechanisms,power aware computing,spanning circuits,technology scaling},
pages = {748--751},
title = {{Computing approximately, and efficiently}},
url = {http://ieeexplore.ieee.org/xpl/freeabs{\_}all.jsp?arnumber=7092486},
year = {2015}
}
@article{Venkataramani2015ce,
abstract = {Diminishing benefits from technology scaling have pushed de-signers to look for new sources of computing efficiency. Multi-cores and heterogeneous accelerator-based architectures are a by-product of this quest to obtain improvements in the per-formance of computing platforms at similar or lower power budgets. In light of the need for new innovations to sustain these improvements, we discuss approximate computing, a field that has attracted considerable interest over the last decade. While the core principles of approximate comput-ing — computing efficiently by producing results that are good enough or of sufficient quality — are not new and are shared by many fields from algorithm design to networks and distributed systems, recent efforts have seen a percolation of these principles to all layers of the computing stack, including circuits, architecture, and software. Approximate computing techniques have also evolved from ad hoc and application-specific to more broadly applicable, supported by systematic design methodologies. Finally, the emergence of workloads such as recognition, mining, search, data analytics, inference and vision are greatly increasing the opportunities for ap-proximate computing. We describe the vision and key prin-ciples that have guided our work in this area, and outline a holistic cross-layer framework for approximate computing.},
annote = {Reviews existing approx and proposes integration methods. Computing Efficiently. Proposes a framework for judging if approximation is giving ``good enough'' results.},
author = {Venkataramani, Swagath and Chakradhar, Srimat T. and Roy, Kaushik and Raghunathan, Anand},
doi = {10.1145/2744769.2751163},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Venkataramani et al. - 2015 - Approximate computing and the quest for computing efficiency(2).pdf:pdf},
isbn = {9781450335201},
issn = {0738100X},
journal = {Proc. 52nd Annu. Des. Autom. Conf. - DAC '15},
pages = {1--6},
title = {{Approximate computing and the quest for computing efficiency}},
url = {http://dl.acm.org/citation.cfm?id=2744769.2751163{\%}5Cnhttp://dl.acm.org/citation.cfm?doid=2744769.2751163 http://dl.acm.org/citation.cfm?doid=2744769.2751163},
year = {2015}
}
@article{Venkatachalam2005,
abstract = {Power consumption is a major factor that limits the performance of computers. We survey the “state of the art” in techniques that reduce the total power consumed by a microprocessor system over time. These techniques are applied at various levels ranging from circuits to architectures, architectures to system software, and system software to applications. They also include holistic approaches that will become more important over the next decade. We conclude that power management is a multifaceted discipline that is continually expanding with new techniques being developed at every level. These techniques may eventually allow computers to break through the “power wall” and achieve unprecedented levels of performance, versatility, and reliability. Yet it remains too early to tell which techniques will ultimately solve the power problem.},
annote = {An in depth exploration of the various circuit silicon level challenges to energy consumption and the methodology to optimise such usage},
author = {Venkatachalam, Vasanth and Franz, Michael},
doi = {10.1145/1108956.1108957},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Venkatachalam, Franz - 2005 - Power reduction techniques for microprocessor systems.pdf:pdf},
isbn = {0360-0300},
issn = {03600300},
journal = {ACM Comput. Surv.},
number = {3},
pages = {195--237},
title = {{Power reduction techniques for microprocessor systems}},
url = {http://portal.acm.org/citation.cfm?doid=1108956.1108957},
volume = {37},
year = {2005}
}
@article{Vassiliadis2014,
abstract = {Reducing energy consumption is one of the key challenges in computing technology. One factor that contributes to high energy consumption is that all parts of the program are considered equally significant for the accuracy of the end-result. However, in many cases, parts of computations can be performed in an approximate way, or even dropped, without affecting the quality of the final output to a significant degree. In this paper, we introduce a task-based programming model and runtime system that exploit this observation to trade off the quality of program outputs for increased energy-efficiency. This is done in a structured and flexible way, allowing for easy exploitation of different execution points in the quality/energy space, without code modifications and without adversely affecting application performance. The programmer specifies the significance of tasks, and optionally provides approximations for them. Moreover, she provides hints to the runtime on the percentage of tasks that should be executed accurately in order to reach the target quality of results. The runtime system can apply a number of different policies to decide whether it will execute each individual less-significant task in its accurate form, or in its approximate version. Policies differ in terms of their runtime overhead but also the degree to which they manage to execute tasks according to the programmer's specification. The results from experiments performed on top of an Intel-based multicore/multiprocessor platform show that, depending on the runtime policy used, our system can achieve an energy reduction of up to 83{\%} compared with a fully accurate execution and up to 35{\%} compared with an approximate version employing loop perforation. At the same time, our approach always results in graceful quality degradation.},
archivePrefix = {arXiv},
arxivId = {1412.5150},
author = {Vassiliadis, Vassilis and Parasyris, Konstantinos and Chalios, Charalambos and Antonopoulos, Christos D. and Lalis, Spyros and Bellas, Nikolaos and Vandierendonck, Hans and Nikolopoulos, Dimitrios S.},
doi = {10.1145/2688500.2688546},
eprint = {1412.5150},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vassiliadis et al. - 2014 - A Programming Model and Runtime System for Significance-Aware Energy-Efficient Computing.pdf:pdf},
isbn = {9781450332057},
issn = {0362-1340},
keywords = {approximate computing,controlled,energy saving,programming model,quality degradation,runtime system eval-},
title = {{A Programming Model and Runtime System for Significance-Aware Energy-Efficient Computing}},
url = {http://arxiv.org/abs/1412.5150},
year = {2014}
}
@article{Van-den-Bergh2015,
abstract = {Superpixel algorithms aim to over-segment the image by grouping pixels that belong to the same object. Many state-of-the-art superpixel algorithms rely on minimizing objective functions to enforce color homogeneity. The optimization is accomplished by sophisticated methods that progressively build the superpixels, typically by adding cuts or growing superpixels. As a result, they are computationally too expensive for real-time applications. We introduce a new approach based on a simple hill-climbing optimization. Starting from an initial superpixel partitioning, it continuously refines the superpixels by modifying the boundaries. We define a robust and fast to evaluate energy function, based on enforcing color similarity between the boundaries and the superpixel color histogram. In a series of experiments, we show that we achieve an excellent compromise between accuracy and efficiency. We are able to achieve a performance comparable to the state-of-the-art, but in real-time on a single Intel i7 CPU at 2.8GHz.},
annote = {Uses a hill climbing methodology to reduce computational time while extracting image shapes. Saves power over the traditional methods.},
archivePrefix = {arXiv},
arxivId = {1309.3848},
author = {{Van den Bergh}, Michael and Boix, Xavier and Roig, Gemma and {Van Gool}, Luc},
doi = {10.1007/s11263-014-0744-2},
eprint = {1309.3848},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Van den Bergh et al. - 2015 - SEEDS Superpixels Extracted Via Energy-Driven Sampling(3).pdf:pdf},
isbn = {9783642337857},
issn = {15731405},
journal = {Int. J. Comput. Vis.},
keywords = {Segmentation,Superpixels,clustering,hill-climbing,histograms,over-segmentation},
number = {3},
pages = {298--314},
title = {{SEEDS: Superpixels Extracted Via Energy-Driven Sampling}},
volume = {111},
year = {2015}
}
@article{Turner2015,
author = {Turner, Paul and Storage, Cloudian},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Turner, Storage - 2015 - Storage for the Internet of Things(3).pdf:pdf},
number = {April},
title = {{Storage for the Internet of Things}},
year = {2015}
}
@book{Tsuchiyama2010,
abstract = {Introduction to Parallelization Why Parallell Parallel Computing (Hardware) Parallel Computing is OpenCL? Historical Background An Overview of OpenCL Why OpenCL? Applicable Platforms OpenCL Setup Available OpenCL Environments Developing Environment Setup First OpenCL Program Basic OpenCL Basic Program Flow Online/Offline Compilation Calling the Kernel Advanced OpenCL OpenCL C OpenCL Programming Practice Case Study FFT (Fast Fourier Transform) Mersenne Twister Notes},
author = {Tsuchiyama, Rjoji and Nakamura, Takashi and Iizuka, Takuro and Asahara, Akihiro and Miki, Satoshi},
booktitle = {Group},
pages = {245 pages},
title = {{The OpenCL Programming Book}},
url = {http://www.fixstars.com/en/company/books/opencl/},
year = {2010}
}
@article{Traiola2018,
abstract = {Approximate Computing (AxC) trades off between the level of accuracy required by the user and the actual precision provided by the computing system to achieve several optimizations such as performance improvement, energy and area reduction etc. Several AxC techniques have been proposed so far in the literature. They work at different abstraction levels and propose both hardware and software implementations. The common issue of all existing approaches is the lack of a methodology to estimate the impact of a given AxC technique on the application-level accuracy. In this paper we propose a probabilistic approach to predict the relation between component-level functional approximation and application-level accuracy. Experimental results on a set of benchmark applications show that the proposed approach is able to estimate the approximation error with good accuracy and very low computation time.},
author = {Traiola, Marcello and Savino, Alessandro and Barbareschi, Mario and Carlo, Stefano Di and Bosio, Alberto and Torino, Politecnico},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Traiola et al. - 2018 - Predicting the Impact of Functional Approximation from Component- to Application-Level.pdf:pdf},
isbn = {9781538659922},
journal = {2018 IEEE 24th Int. Symp. On-Line Test. Robust Syst. Des.},
keywords = {approximate computing,bayesian networks,functional approxima-,quality metrics,tion},
number = {iii},
pages = {61--64},
title = {{Predicting the Impact of Functional Approximation : from Component- to Application-Level}},
year = {2018}
}
@incollection{Traeber2017,
author = {Traeber, Andreas and Gautschi, Michael and Schiavone, Pasquale Davide},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Traeber, Gautschi, Schiavone - 2017 - RI5CY User Manual(2).pdf:pdf},
keywords = {Pulp,RISC V},
mendeley-tags = {Pulp,RISC V},
number = {November},
pages = {1--60},
title = {{RI5CY : User Manual}},
year = {2017}
}
@incollection{Traber2017,
author = {Traber, Andreas and Gautschi, Michael},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Traber, Gautschi - 2017 - PULPino Datasheet.pdf:pdf},
keywords = {Pulp,Risc V},
mendeley-tags = {Pulp,Risc V},
title = {{PULPino : Datasheet}},
year = {2017}
}
@article{Totoni2013,
author = {Totoni, Ehsan and Dikmen, Mert and Garzar{\'{a}}n, Mar{\'{i}}a Jes{\'{u}}s},
doi = {10.1145/2541228.2555302},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Totoni, Dikmen, Garzar{\'{a}}n - 2013 - Easy, fast, and energy-efficient object detection on heterogeneous on-chip architectures.pdf:pdf},
issn = {15443566},
journal = {ACM Trans. Archit. Code Optim.},
number = {4},
pages = {1--25},
title = {{Easy, fast, and energy-efficient object detection on heterogeneous on-chip architectures}},
url = {http://dl.acm.org/citation.cfm?doid=2541228.2555302},
volume = {10},
year = {2013}
}
@article{Tompson2012,
abstract = {This paper presents an overview of the OpenCL 1.1 standard [Khronos 2012]. We first motivate the need for GPGPU computing and then discuss the various concepts and technological gackground necessary to understand the programming model. We use concurent matrix multiplication as a framework for explaing various performance characteristics of compiling and running OpenCL code, and contrast this to native code on more traditional general purpose CPUs.},
author = {Tompson, Jonathan and Schlachter, Kristofer},
journal = {Digit. version available here},
keywords = {barrier synchronization,matrix multiply,opencl},
title = {{An Introduction to the OpenCL Programming Model}},
url = {http://www.cs.nyu.edu/{~}lerner/spring12/Preso07-OpenCL.pdf},
year = {2012}
}
@article{Thompson2018,
abstract = {It is a triumph of technology and of economics that our computer chips are so universal. Countless applications are only possible because of the staggering variety of calculations that modern chips can compute. But, this was not always the case. Computers used to be specialized, doing only narrow sets of calculations. Their rise as a 'general purpose technology (GPT)' only happened because of the technical breakthroughs by computer scientists like von Neumann and Turing, and the mutually-reinforcing economic cycle of general purpose technologies, where product improvement and market growth fuel each other. This paper argues that technological and economic forces are now pushing computing in the opposite direction, making computer processors less general purpose and more specialized. This process has already begun, driven by the slow down in Moore's Law and the algorithmic success of Deep Learning. This trend towards specialization threatens to fragment computing into 'fast lane' applications that get powerful customized chips and 'slow lane' applications that get stuck using general purpose chips whose progress fades. The rise of general purpose computer chips has been remarkable. So, too, could be their fall. This paper outlines the forces already starting to fragment this general purpose technology.},
annote = {Discusses the pervasive effect of the slowing of Moores law and the switch to specialised processing and the impact on overall market demands, reducing for processors and increasing for specialised computing accelerating this niche market. Defines a fragmentation cycle, technology slows, fewer users adopt the slowing technology reducing available finance for innovation.},
author = {Thompson, Neil C and Spanuth, Svenja},
file = {:C$\backslash$:/Users/D/Documents/Uni/Papers/SSRN-id3287769.pdf:pdf},
journal = {SSRN},
keywords = {CPU,Computer Chips,Deep Learning,Economics of I.T.,GPU,General Purpose Technology,Information Technology,Moore's Law,Processors},
title = {{The Decline of Computers as a General Purpose Technology : Why Deep Learning and the End of Moore ' s Law are Fragmenting Computing}},
url = {https://papers.ssrn.com/sol3/papers.cfm?abstract{\_}id=3287769},
year = {2018}
}
@article{Tatchell-Evans2015,
abstract = {A combination of laboratory experiments and a system model are used to carry out the first investigation into the potential for cold air to bypass IT equipment within data centres (DCs) employing aisle containment, and the effect of this bypass on DC electricity consumption. The laboratory experiments involved applying a differential pressure across commercially available server racks and aisle containment systems and measuring the resulting air flow. The potential to minimise bypass by sealing leakage paths and redesigning racks was investigated and quantified experimentally. A new system model is developed using a combination of manufacturer data, empirical relationships and experimental results to predict the impact of bypass on the power consumption of the various components of a DC's cooling infrastructure. The results show that, at typical cold aisle pressures, as much as 20{\%} of the supplied air may bypass servers by finding alternate paths through the server rack itself. This increases the required flow rate from air conditioning units (ACUs). The system model predicts that: (i) practical measures undertaken to reduce this bypass could reduce total power consumption by up to 8.8{\%} and (ii) excessive pressure differentials across the containment system could also increase power consumption, by up to 16{\%}.},
author = {Tatchell-Evans, Morgan and Kapur, Nik and Summers, Jonathan and Thompson, Harvey and Oldham, Dan},
doi = {10.1016/j.apenergy.2016.03.076},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tatchell-Evans et al. - 2015 - An experimental and theoretical investigation of the extent of bypass air within data centres employin(3).pdf:pdf},
issn = {03062619},
journal = {Appl. Energy},
keywords = {Aisle containment,Bypass air,Data centre,Energy efficiency,System model,Thermal management},
pages = {457--469},
publisher = {The Authors},
title = {{An experimental and theoretical investigation of the extent of bypass air within data centres employing aisle containment, and its impact on power consumption}},
url = {http://dx.doi.org/10.1016/j.apenergy.2016.03.076},
volume = {186},
year = {2015}
}
@article{Tarantola2004,
author = {Tarantola, Albert},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tarantola - 2004 - Tensors for Beginners(2).pdf:pdf},
number = {2},
pages = {1--19},
title = {{Tensors for Beginners}},
year = {2004}
}
@article{Tamboli2015,
author = {Tamboli, Alam and Analyst, Senior},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tamboli, Analyst - 2015 - IoT and the Service Provider Market(2).pdf:pdf},
title = {{IoT and the Service Provider Market}},
year = {2015}
}
@article{Tagliavini2016,
abstract = {{\textcopyright} 2016 IEEE.Pushing supply voltages in the near-threshold region is today one of the main avenues to minimize power consumption in digital integrated circuits. This works well with logic units, but memory operations on standard six-transistor static RAM (6T-SRAM) cells become unreliable at low voltages. Standard cell memory (SCM) works fully reliably at near-threshold voltages, but has much lower area density than 6T-SRAM and thus it is too costly. Hybrid memory designs based on a combination of 6T-SRAM and SCM have the potential to combine the best from both worlds, provided that appropriate software techniques for their management are used. Several embedded applications exhibit inherent tolerance to data approximation: this feature can be exploited by mapping error-tolerant data onto unreliable 6T-SRAM while keeping critical information error-free in SCM. However, one key issue is bounding error when it is input-data dependent. In this work we consider the motion detection stage of a computer vision pipeline, which is a major power bottleneck in always-on computer vision systems. We introduce an application-level metric for defining suitable tolerance thresholds and an associated runtime mechanism for their control. At each accuracy checkpoint the error on the computation is checked. If the runtime detects that an error threshold has been exceeded, the voltage settings are adjusted. Using this methodology, we achieve a significant reduction of the total energy consumption (up to 33{\%} in the best case) while maintaining a tight control on quality of results.},
annote = {An ASIC based, specialised implementation utilising Standard Cell Memory in place of SRAM. Not an off the shelf solution.},
author = {Tagliavini, Giuseppe and Marongiu, Andrea and Rossi, Davide and Benini, Luca},
doi = {10.1109/ICECS.2016.7841261},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tagliavini et al. - 2016 - Always-on motion detection with application-level error control on a near-threshold approximate computing pla.pdf:pdf},
isbn = {9781509061136},
journal = {2016 IEEE Int. Conf. Electron. Circuits Syst. ICECS 2016},
pages = {552--555},
publisher = {IEEE},
title = {{Always-on motion detection with application-level error control on a near-threshold approximate computing platform}},
year = {2016}
}
@book{Sutton,
author = {Sutton, Richard S. and Barto, Andrew G.},
title = {{Reinforcement learning : an introduction}}
}
@article{Summit2015IoTFuture,
author = {Summit, Ethernet},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Summit - 2015 - Future of Ethernet And The Internet Of Things ( IoT ).pdf:pdf},
title = {{Future of Ethernet And The Internet Of Things ( IoT )}},
year = {2015}
}
@article{Summit2015aONHighlights,
author = {Summit, Ethernet},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Summit - 2015 - ACG RESEARCH Optical Networking 4Q WW Highlights.pdf:pdf},
title = {{ACG RESEARCH Optical Networking 4Q WW Highlights}},
year = {2015}
}
@article{Stulhfath2015,
author = {Stulhfath},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stulhfath - 2015 - Internet of Things – what are the driving technologies Test {\&} Certification of IoT Devices.pdf:pdf},
title = {{Internet of Things – what are the driving technologies Test {\&} Certification of IoT Devices}},
year = {2015}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {J. Mach. Learn. Res.},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
pmid = {23285570},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@inproceedings{Sreekanth2015-100GPP,
author = {Sreekanth, Jon},
booktitle = {Ethernet Technol. summit 2015},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sreekanth - 2015 - FPGA Packet Processing at 100GE and Beyond(2).pdf:pdf},
keywords = {FPGA,NPU,x86},
mendeley-tags = {FPGA,NPU,x86},
number = {April},
pages = {1--12},
title = {{FPGA Packet Processing at 100GE and Beyond}},
year = {2015}
}
@article{Specification,
author = {Specification, Preliminary},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Specification - Unknown - OV7690.pdf:pdf},
title = {{OV7690}}
}
@article{Sjalander2014,
abstract = {CMOS scaling is near its end but new emerging devices are being developed to replace CMOS. These devices have different features than CMOS, such as the possibility for multi-value logic, which present new opportunities when designing computer systems. In this work we investigate the use of multi-value devices to design a cache that can tune the amount of resources used to store application data. We leverage work on approximate computing to store data that are not application critical in a compact quaternary format while critical data is stored in a more error resilient binary format.},
author = {Sjalander, Magnus and Nilsson, Nina Shariati and Kaxiras, Stefanos},
doi = {10.1109/NANOARCH.2014.6880480},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sjalander, Nilsson, Kaxiras - 2014 - A tunable cache for approximate computing(2).pdf:pdf},
isbn = {9781479963836},
journal = {Proc. 2014 IEEE/ACM Int. Symp. Nanoscale Archit. NANOARCH 2014Magnus Sjalander, Nina Shariati Nilsson, Stefanos Kaxiras. 2014. A tunable cache Approx. Comput. Proc. 2014 IEEE/ACM Int. Symp. Nanoscale Arch},
number = {2},
pages = {88--89},
title = {{A tunable cache for approximate computing}},
year = {2014}
}
@inproceedings{sinha2000algorithmic,
author = {Sinha, Amit and Wang, Alice and Anantha, P and Chandrakasan, Anantha P and Anantha, P and Chandrakasan, Anantha P},
booktitle = {Proc. 2000 Int. Symp. Low power Electron. Des.},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sinha et al. - 2000 - Algorithmic transforms for efficient energy scalable computation(2).pdf:pdf},
number = {i},
organization = {ACM},
pages = {31--36},
title = {{Algorithmic transforms for efficient energy scalable computation}},
year = {2000}
}
@article{Singer2015,
author = {Singer, Steve},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Singer - 2015 - Ethernet Tech Summit(2).pdf:pdf},
title = {{Ethernet Tech Summit}},
year = {2015}
}
@article{Simunic2001,
abstract = {Energy-efficient design of battery-powered systems demands optimizations in both hardware and software. We present a modular approach for enhancing instruction level simulators with cycle-accurate simulation of energy dissipation in embedded systems. Our methodology has tightly coupled component models thus making our approach more accurate. Performance and energy computed by our simulator are within a 5{\%} tolerance of hardware measurements on the SmartBadge [2]. We show how the simulation methodology can be used for hardware design exploration aimed at enhancing the SmartBadge with real-time MPEG video feature. In addition, we present a profiler that relates energy consumption to the source code. Using the profiler we can quickly and easily redesign the MP3 audio decoder software to run in real time on the SmartBadge with low energy consumption. Performance increase of 92{\%} and energy consumption decrease of 77{\%} over the original executable specification have been achieved.},
annote = {In an ideal world, it would be great if it were possible to estimate energy requirements for an embedded system, rather than having to design, build, write the software, then measure energy requirements and design an energy system to drive it.
Discusses some of the various approaches and indicates that, with a plethora of processor types and combinations available, such solutions are not a simple task or possibly cost effective solution for typical SMEs},
author = {Simunic, Tajana and Benini, Luca and Micheli, Giovanni De},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Simunic, Benini, Micheli - 2001 - Energy-Efficient Design of Battery-Powered Embedded Systems.pdf:pdf},
keywords = {Low-power design,consumption model,performance tradeoffs,power,system-level},
number = {1},
pages = {15--28},
title = {{Energy-Efficient Design of Battery-Powered Embedded Systems}},
volume = {9},
year = {2001}
}
@article{Sidler2016,
abstract = {TCP/IP is widely used both in the Internet as well as in data centers. The protocol makes very few assumptions about the underlying network and provides useful guarantees such as reliable transmission, in-order delivery, or control flow. The price for this functionality is complexity, latency, and computational overhead, which is especially pronounced in software implementations. While for Internet communication this is acceptable, the overhead is too high in data centers. In this paper, we explore how to optimize a TCP/IP stack running on an FPGA for data center applications with an emphasis on data processing (e.g., key value stores). Using a key-value store and a low-latency consensus protocol implemented on an FPGA as an example of the requirements that arise in data centers, we provide an extensive analysis of the overheads of TCP/IP and the solutions that can be adopted to minimize such an overhead. The proposed optimized TCP/IP stack minimizes tail latencies (a key metric in distributed data processing) and is efficiently implemented so as to be able to share the FPGA with application logic.},
annote = {This is the basic work as used by Intilop to generate the FPGA solution.

An interesting application of a full segmentation offload, low latency FPGA solution to the challenge of TCP and UDP frames normally handled by a software stack. Reduces latency and power.},
author = {Sidler, David and Istv{\'{a}}n, Zsolt and Alonso, Gustavo},
doi = {10.1109/FPL.2016.7577319},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sidler, Istv{\'{a}}n, Alonso - 2016 - Low-latency TCPIP stack for data center applications.pdf:pdf},
isbn = {9782839918442},
journal = {FPL 2016 - 26th Int. Conf. Field-Programmable Log. Appl.},
keywords = {Intilop,TCP IP offload},
mendeley-tags = {Intilop,TCP IP offload},
title = {{Low-latency TCP/IP stack for data center applications}},
year = {2016}
}
@article{ShreyamshaKumar2015,
abstract = {Like bilateral filter (BF), cross bilateral filter (CBF) considers both gray-level similarities and geometric closeness of the neighboring pixels without smoothing edges, but it uses one image for finding the kernel and other to filter, and vice versa. In this paper, it is proposed to fuse source images by weighted average using the weights computed from the detail images that are extracted from the source images using CBF. The performance of the proposed method has been verified on several pairs of multisensor and multifocus images and compared with the existing methods visually and quantitatively. It is found that, none of the methods have shown consistence performance for all the performance metrics. But as compared to them, the proposed method has shown good performance in most of the cases. Further, the visual quality of the fused image by the proposed method is superior to other methods.},
annote = {This paper mainly addresses the use of dual or multi sourced images to produce high quality results, while acknowledging there is an increased cost in computation.},
author = {{Shreyamsha Kumar}, B. K.},
doi = {10.1007/s11760-013-0556-9},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shreyamsha Kumar - 2015 - Image fusion based on pixel significance using cross bilateral filter.pdf:pdf},
issn = {18631711},
journal = {Signal, Image Video Process.},
keywords = {Bilateral filter,Cross bilateral filter,Detail image,Detail strength,Image fusion,Multifocus,Multisensor,Pixel significance},
number = {5},
pages = {1193--1204},
publisher = {Springer London},
title = {{Image fusion based on pixel significance using cross bilateral filter}},
url = {http://dx.doi.org/10.1007/s11760-013-0556-9},
volume = {9},
year = {2015}
}
@article{ShreyamshaKumar2013,
abstract = {The energy compaction and multiresolution properties of wavelets have made the image fusion successful in combining important features such as edges and textures from source images without introducing any artifacts for context enhancement and situational awareness. The wavelet transform is visualized as a convolution of wavelet filter coefficients with the image under consideration and is computationally intensive. The advent of lifting-based wavelets has reduced the computations but at the cost of visual quality and performance of the fused image. To retain the visual quality and performance of the fused image with reduced computations, a discrete cosine harmonic wavelet (DCHWT)-based image fusion is proposed. The performance of DCHWT is compared with both convolution and lifting-based image fusion approaches. It is found that the performance of DCHWT is similar to convolution-based wavelets and superior/similar to lifting-based wavelets. Also, the computational complexity (in terms of additions and multiplications) of the proposed method scores over convolution-based wavelets and is competitive to lifting-based wavelets.},
author = {{Shreyamsha Kumar}, B. K.},
doi = {10.1007/s11760-012-0361-x},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shreyamsha Kumar - 2013 - Multifocus and multispectral image fusion based on pixel significance using discrete cosine harmonic wavelet t.pdf:pdf},
isbn = {1863-1703},
issn = {18631703},
journal = {Signal, Image Video Process.},
keywords = {Discrete cosine harmonic wavelet transform,Discrete wavelet transform,Harmonic wavelet transform,Image fusion,Multifocus,Multisensor,Pixel significance},
number = {6},
pages = {1125--1143},
title = {{Multifocus and multispectral image fusion based on pixel significance using discrete cosine harmonic wavelet transform}},
volume = {7},
year = {2013}
}
@inproceedings{Shendar,
author = {Shendar, Noam},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shendar, Storage - Unknown - Boosting SAN Performance Cheaply {\&} Easily(5).pdf:pdf},
title = {{Boosting SAN Performance Cheaply {\&} Easily}}
}
@article{Shen2012,
abstract = {OpenCL and OpenMP are the most commonly used programming models for$\backslash$nmulti-core processors. They are also fundamentally different in their$\backslash$napproach to parallelization. In this paper, we focus on comparing$\backslash$nthe performance of OpenCL and OpenMP. We select three applications$\backslash$nfrom the Rodinia benchmark suite (which provides equivalent OpenMP$\backslash$nand OpenCL implementations), and carry out experiments with different$\backslash$ndatasets on three multi-core platforms. We see that the incorrect$\backslash$nusage of the multi-core CPUs, the inherent OpenCL fine-grained parallelism,$\backslash$nand the immature OpenCL compilers are the main reasons that lead$\backslash$nto the OpenCL poorer performance. After tuning the OpenCL versions$\backslash$nto be more CPU-friendly, we show that OpenCL either outperforms or$\backslash$nachieves similar performance in more than 80{\%} of the cases. Therefore,$\backslash$nwe believe that OpenCL is a good alternative for multi-core CPU programming.},
author = {Shen, Jie and Fang, Jianbin and Sips, Henk and Varbanescu, Ana Lucia},
doi = {10.1109/ICPPW.2012.18},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shen et al. - 2012 - Performance gaps between OpenMP and OpenCL for multi-core CPUs.pdf:pdf},
isbn = {9780769547954},
issn = {15302016},
journal = {Proc. Int. Conf. Parallel Process. Work.},
keywords = {CPU,Multi-core,OpenCL,OpenMP,Performance Comparison,shen},
mendeley-tags = {shen},
pages = {116--125},
title = {{Performance gaps between OpenMP and OpenCL for multi-core CPUs}},
year = {2012}
}
@inproceedings{ShahMaxAccelFPGA,
author = {Shah, Shreyas},
booktitle = {Ethernet Technol. summit},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shah - 2015 - Maximizing Application Acceleration with FPGAs(2).pdf:pdf},
keywords = {FPGA,Xilinx,uWall},
mendeley-tags = {FPGA,Xilinx,uWall},
title = {{Maximizing Application Acceleration with FPGAs}},
year = {2015}
}
@article{Shagrithaya2013,
abstract = {High-level FPGA synthesis tools aim to increase the productivity of FPGAs and to adopt them among software developers and domain experts. OpenCL is a specification introduced for parallel programming across heterogeneous platforms. In this paper, an automated compilation flow to generate customized application-specific hardware descriptions from OpenCL computation kernels is reported. The flow uses Xilinx AutoESL tool to obtain the design specification for OpenCL kernel cores. The provided architecture integrates generated cores with memory and OpenCL host application interfaces. The host program in the OpenCL application is compiled and executed to demonstrate a proof-of-concept implementation towards achieving an end-to-end flow that provides abstraction of hardware at the front-end.},
author = {Shagrithaya, Kavya and Kepa, Krzysztof and Athanas, Peter},
doi = {10.1109/ASAP.2013.6567546},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shagrithaya, Kepa, Athanas - 2013 - Enabling development of OpenCL applications on FPGA platforms(2).pdf:pdf},
isbn = {9781479904921},
issn = {10636862},
journal = {Proc. Int. Conf. Appl. Syst. Archit. Process.},
keywords = {AutoESL,Convey,FPGA,HPC,OpenCL,Vivado,source-to-source translation},
pages = {26--30},
title = {{Enabling development of OpenCL applications on FPGA platforms}},
year = {2013}
}
@article{Shafique2016,
abstract = {We present a survey of approximate techniques and discuss concepts for building power-/energy-efficient computing components reaching from approximate accelerators to arithmetic blocks (like adders and multipliers). We provide a systematical understanding of how to generate and explore the design space of approximate components, which enables a wide-range of power/energy, performance, area and output quality tradeoffs, and a high degree of design flexibility to facilitate their design. To enable cross-layer approximate computing, bridging the gap between the logic layer (i.e. arithmetic blocks) and the architecture layer (and even considering the software layers) is crucial. Towards this end, this paper introduces open-source libraries of low-power and high-performance approximate components. The elementary approximate arithmetic blocks (adder and multiplier) are used to develop multi-bit approximate arithmetic blocks and accelerators. An analysis of data-driven resilience and error propagation is discussed. The approximate computing components are a first steps towards a systematic approach to introduce approximate computing paradigms at all levels of abstractions.},
author = {Shafique, Muhammad and Hafiz, Rehan and Rehman, Semeen and El-Harouni, Walaa and Henkel, J{\"{o}}rg},
doi = {10.1145/2897937.2906199},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shafique et al. - 2016 - Cross-layer approximate computing from logic to architectures.pdf:pdf},
isbn = {9781450342360},
issn = {0738100X},
journal = {Proc. 53rd Annu. Des. Autom. Conf. - DAC '16},
keywords = {A,Approximate Computing,Arithmetic,Logic Blocks,a re -,accelerator,adder,approximate computing,approximate computing is,architecture,arithmetic,computing itself,configurable accuracy,cross-layer,design space,energy efficiency,logic,low power,multiplier,performance,though as old as},
pages = {1--6},
title = {{Cross-layer approximate computing: from logic to architectures}},
url = {http://dl.acm.org/citation.cfm?doid=2897937.2906199},
year = {2016}
}
@article{Shafique2017,
author = {Shafique, Muhammad and Garg, Siddharth},
doi = {10.1109/MDAT.2016.2633408},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shafique, Garg - 2017 - Computing in the Dark Silicon Era Current Trends and Research Challenges(3).pdf:pdf},
issn = {2168-2356},
journal = {IEEE Des. Test},
number = {2},
pages = {8--23},
title = {{Computing in the Dark Silicon Era: Current Trends and Research Challenges}},
url = {http://ieeexplore.ieee.org/document/7762141/},
volume = {34},
year = {2017}
}
@article{Shafik2018,
abstract = {—The traditional hallmark in embedded systems is to minimize energy consumption considering hard or soft real-time deadlines. The basic principle is to transfigure the uncertainties of task execution times in the realworld into energy saving opportunities. The energy saving is achieved by suitably controlling the reliable power supply at circuit or system-level with the aim of minimizing the slack times, while meeting the specified performance requirements. Computing paradigm for emerging ubiquitous systems, particularly for the energy-harvested ones, has clearly shifted from the traditional systems. The energy supply of these systems can vary temporally and spatially within a dynamic range, essentially making computation extremely challenging. Such a paradigm shift requires disruptive approaches to design computing systems that can provide continued functionality under unreliable supply power envelope and operate with autonomous survivability (i.e., the ability to automatically guarantee retention and/or completion of a given computation task). In this paper, we introduce Real-PowerComputing, inspired by the above trends and tenets. We show how computation systems must be designed with power-proportionality to achieve sustained computation and survivability when operating at extreme power conditions. We present extensive analysis of the need for this new computing approach using definitions, where necessary, coupled with detailed taxonomies, empirical observations, a review of relevant research works and example scenarios using three case studies representing the proposed paradigm. Index},
author = {Shafik, Rishad and Yakovlev, Alex and Das, Shidhartha},
doi = {10.1109/TC.2018.2822697},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shafik, Yakovlev, Das - 2018 - Real-Power Computing.pdf:pdf},
issn = {15579956},
journal = {IEEE Trans. Comput.},
keywords = {Embedded systems,Energy harvesting,Energy transparency,Energy-driven computing,Energy-modulated computing,Low-power design,Power-compute co-design,Power-neutral computing,Power-proportional computing,Run-time systems,Survivability,Transient computing,Worst-case power consumption},
number = {10},
pages = {1445--1461},
publisher = {IEEE},
title = {{Real-Power Computing}},
volume = {67},
year = {2018}
}
@article{Shafik2016,
abstract = {Embedded systems execute applications with varying performance requirements. These applications exercise the hardware differently depending on the computation task, generating varying workloads with time. Energy minimization with such workload and performance variations within (intra) and across (inter) applications is particularly challenging. To address this challenge we propose an online approach, capable of minimizing energy through adaptation to these variations. At the core of this approach is a reinforcement learning algorithm that suitably selects the appropriate voltage/frequency scaling (VFS) based on workload predictions to meet the applications' performance requirements. The adaptation is then facilitated and expedited through learning transfer, which uses the interaction between the application, runtime and hardware layers to adjust the VFS. The proposed approach is implemented as a power governor in Linux and extensively validated on an ARM Cortex-A8 running different benchmark applications. We show that with intra- and inter-application variations, our proposed approach can effectively minimize energy consumption by up to 33{\%} compared to the existing approaches. Scaling the approach to multi-core systems, we also demonstrate that it can minimize energy by up to 18{\%} with 2X reduction in the learning time when compared with an existing approach.},
author = {Shafik, Rishad A. and Yang, Sheng and Das, Anup and Maeda-Nunez, Luis A. and Merrett, Geoff V. and Al-Hashimi, Bashir M.},
doi = {10.1109/TCAD.2015.2481867},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shafik et al. - 2016 - Learning Transfer-Based Adaptive Energy Minimization in Embedded Systems.pdf:pdf},
isbn = {0278-0070},
issn = {02780070},
journal = {IEEE Trans. Comput. Des. Integr. Circuits Syst.},
keywords = {Energy-efficiency,dynamic voltage/frequency scaling,reinforcement learning},
number = {6},
pages = {877--890},
title = {{Learning Transfer-Based Adaptive Energy Minimization in Embedded Systems}},
volume = {35},
year = {2016}
}
@article{Shafik2009,
abstract = {In this paper, we examine the impact of application task mapping on the reliability of MPSoC in the presence of single-event upsets (SEUs). We propose a novel soft error-aware design optimization using joint power minimization with voltage scaling and reliability improvement through application task mapping. The aim is to minimize the number of SEUs experienced by the MPSoC for a suitably identified voltage scaling of the system processing cores such that the power is reduced and the specified real-time constraint is met.We evaluate the effectiveness of the proposed optimization technique using an MPEG-2 decoder and random task graphs. We show that for an MPEG-2 decoder with four processing cores, our optimization technique produces a design that experiences 38{\%} less SEUs than soft error-unaware design optimization for a soft error rate of 10-9, while consuMPEG-2 decoderming 9{\%} less power and meeting a given real-time constraint. Furthermore, we investigate the impact of architecture allocation (varying the number of MPSoC cores) on the power consumption and SEUs experienced. We show that for an MPSoC with six processing cores and a given real-time constraint, the proposed technique experiences upto 7{\%} less SEUs compared to soft error-unaware optimization, while consuming only 3{\%} more power.},
author = {Shafik, R.a. and Al-Hashimi, B.M. M and Chakrabarty, K.},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shafik, Al-Hashimi, Chakrabarty - 2010 - Soft error-aware design optimization of low power and time-constrained embedded systems(2).pdf:pdf},
isbn = {978-1-4244-7054-9},
issn = {1530-1591},
journal = {Des. Autom. {\&} Test Eur. Conf. {\&} Exhib. (DATE), 2010},
pages = {0--5},
title = {{Soft error-aware design optimization of low power and time-constrained embedded systems}},
year = {2010}
}
@article{Shafait2008,
abstract = {Adaptive binarization is an important first step in many document analysis and OCR processes. This paper describes a fast adaptive binarization algorithm! that yields the same quality of binarization as the Sauvola method,1 but runs in time close to that of global thresholding methods (like Otsu's method2), independent of the window size. The algorithm combines the statistical constraints of Sauvola's method with integral images.3 Testing on the UW-1 dataset demonstrates a 20-fold speedup compared to the original Sauvola algorithm.},
author = {Shafait, Faisal and Keysers, Daniel and Breuel, Tm},
doi = {10.1117/12.767755},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shafait, Keysers, Breuel - 2008 - Efficient implementation of local adaptive thresholding techniques using integral images.pdf:pdf},
isbn = {9780819469878},
issn = {0277786X},
journal = {SPIE Doc. Imaging Retr.},
keywords = {adaptive thresholding,document binarization,integral images,local thresholding},
pages = {1--5},
title = {{Efficient implementation of local adaptive thresholding techniques using integral images}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=812244},
year = {2008}
}
@article{Shabtai2015,
author = {Shabtai, Avi and Multiphy, C E O},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shabtai, Multiphy - 2015 - The Path to 400G Ethernet for Data Center Applications(2).pdf:pdf},
number = {April},
title = {{The Path to 400G Ethernet for Data Center Applications}},
year = {2015}
}
@article{Sengupta2017,
author = {Sengupta, Deepashree and Snigdha, Farhana Sharmin and Hu, Jiang and Sapatnekar, Sachin S},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sengupta et al. - 2017 - SABER Selection of Approximate Bits for the Design of Error Tolerant Circuits.pdf:pdf},
isbn = {9781450349277},
title = {{SABER : Selection of Approximate Bits for the Design of Error Tolerant Circuits}},
year = {2017}
}
@article{Sekanina2017,
abstract = {Low power image and video processing circuits are crucial in many applications of computer vision. Traditional techniques used to reduce power consumption in these applications have recently been accompanied by circuit approximation methods which exploit the fact that these applications are highly error resilient and, hence, the quality of image processing can be traded for power consumption. On the basis of a literature survey, we identified the components whose implementations are the most frequently approximated and the methods used for obtaining these approximations. One of the components is the median image filter. We propose, evaluate and compare two approximation strategies based on Cartesian genetic programming applied to approximate various common implementations of the median filter. For filters developed using these approximation strategies, trade-offs between the quality of filtering and power consumption are investigated. Under conditions of our experiments we conclude that better trade-offs are achieved when the image filter is evolved from scratch rather than a conventional filter is approximated.},
author = {Sekanina, Lukas and Vasicek, Zdenek and Mrazek, Vojtech},
doi = {10.13164/re.2017.0623},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sekanina, Vasicek, Mrazek - 2017 - Approximate circuits in low-power image and video processing The approximate median filter(2).pdf:pdf},
issn = {12102512},
journal = {Radioengineering},
keywords = {Approximate computing,Circuit design,Evolutionary computation,Image filter},
number = {3},
pages = {623--632},
title = {{Approximate circuits in low-power image and video processing: The approximate median filter}},
volume = {26},
year = {2017}
}
@inproceedings{Sekanina2013,
abstract = {This paper deals with evolutionary design of approximate circuits. This class of circuits is characterized by relaxing the requirement on functional equivalence between the specification and implementation in order to reduce the area on a chip or minimize energy consumption. We proposed a CGP-based automated design method which enables to find a good trade off between key circuit parameters (functionality, area and power consumption). In particular, the digital approximate circuits consisting of elementary gates are addressed in this paper. Experimental results are provided for combinational single-output circuits and adders where two different metrics are compared for the error assessment.},
author = {Sekanina, Lukas and Vasicek, Zdenek},
booktitle = {Proc. 2013 IEEE Int. Conf. Evolvable Syst. ICES 2013 - 2013 IEEE Symp. Ser. Comput. Intell. SSCI 2013},
doi = {10.1109/ICES.2013.6613278},
file = {:C$\backslash$:/Users/D/Documents/Uni/Papers/06613278.pdf:pdf},
isbn = {9781467358699},
title = {{Approximate circuit design by means of evolvable hardware}},
year = {2013}
}
@article{Seiter2013,
abstract = {In this paper we present an FPGA based characterization system for our 3D TOF distance sensors supporting up to 128 × 128 pixels. The system is capable of flexibly generating all control signals required for a typical TOF measurement. Their properties can be changed within a very broad range. The cycle-to-cycle jitter of those signals was reduced to 1 ps by ECL circuitry. This is equivalent to a standard deviation of the measured distance of 0.15 mm. Furthermore, the system is able to preprocess the distance information before transferring the data to a terminal PC, which reduces the data load on the USB interface. The system includes an averaging function with a maximum of 256 elements to reduce the standard deviation of precision distance measurement sensors. A novel fiber based setup is introduced to systemize the characterization process. By means of averaging a standard deviation of 2 mm could be achieved with one of our 3D TOF distance sensors. {\textcopyright} 2013 IEEE.},
author = {Seiter, Johannes and Hofbauer, Michael and Davidovic, Milos and Zimmermann, Horst},
doi = {10.1109/DDECS.2013.6549825},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Seiter et al. - 2013 - FPGA based time-of-flight 3D camera characterization system.pdf:pdf},
isbn = {9781467361361},
journal = {Proc. 2013 IEEE 16th Int. Symp. Des. Diagnostics Electron. Circuits Syst. DDECS 2013},
keywords = {3D camera,FPGA,TOF sensor,characterization system,optical distance sensor,time-of-flight},
pages = {240--245},
title = {{FPGA based time-of-flight 3D camera characterization system}},
year = {2013}
}
@article{Schaffner2014,
abstract = {Many video processing algorithms are formulated as least- squares problems that result in large, sparse linear systems. Solving such systems in real time is very demanding. This paper focuses on reducing the computational complexity of a direct Cholesky-decomposition-based solver. Our ap- proximation scheme builds on the observation that, in well- conditioned problems, many elements in the decomposition nearly vanish. Such elements may be pruned from the de- pendency graph with mild accuracy degradation. Using an example from image-domain warping, we show that pruning reduces the amount of operations per solve by over 75 {\%}, resulting in significant savings in computing time, area or energy},
author = {Schaffner, Michael and G{\"{u}}rkaynak, Frank K and Smolic, Aljosa and Kaeslin, Hubert and Benini, Luca},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Schaffner et al. - 2014 - An Approximate Computing Technique for Reducing the Complexity of a Direct-Solver for Sparse Linear Systems(2).pdf:pdf},
isbn = {9781450327305},
journal = {DAC},
keywords = {approxi-,cholesky decomposition,hardware accelerator,mate computing,video processing},
pages = {1--6},
title = {{An Approximate Computing Technique for Reducing the Complexity of a Direct-Solver for Sparse Linear Systems in Real-Time Video Processing Sparse Linear Systems in Image and Video}},
year = {2014}
}
@article{Sayre2015,
author = {Sayre, Edward P and Macomson, Wis},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sayre, Macomson - 2015 - Practical Design of a 28 Gbps Backplane Presented by(2).pdf:pdf},
keywords = {Sayre201528GbBP},
mendeley-tags = {Sayre201528GbBP},
number = {April},
pages = {1--30},
title = {{Practical Design of a 28 Gbps Backplane Presented by :}},
year = {2015}
}
@article{Sargologos2015IoTGateway,
annote = {check freescale's1021a},
author = {Sargologos, Nicholas and Networking, Digital},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sargologos, Networking - 2015 - Enabling Trust in IoT Gateways Infrastructure of the(2).pdf:pdf},
title = {{Enabling Trust in IoT Gateways Infrastructure of the}},
year = {2015}
}
@inproceedings{sampson2011enerj,
author = {Sampson, Adrian and Dietl, Werner and Fortuna, Emily and Gnanapragasam, Danushen and Ceze, Luis and Grossman, Dan},
booktitle = {ACM SIGPLAN Not.},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sampson et al. - 2011 - EnerJ Approximate data types for safe and general low-power computation.pdf:pdf},
number = {6},
organization = {ACM},
pages = {164--174},
title = {{EnerJ: Approximate data types for safe and general low-power computation}},
volume = {46},
year = {2011}
}
@article{Samadi2013,
abstract = {Approximate computing, where computation accuracy is traded off for better performance or higher data throughput, is one solution that can help data processing keep pace with the current and grow-ing overabundance of information. For particular domains such as multimedia and learning algorithms, approximation is commonly used today. We consider automation to be essential to provide transparent approximation and we show that larger benefits can be achieved by constructing the approximation techniques to fit the underlying hardware. Our target platform is the GPU because of its high performance capabilities and difficult programming chal-lenges that can be alleviated with proper automation. Our approach, SAGE, combines a static compiler that automatically generates a set of CUDA kernels with varying levels of approximation with a run-time system that iteratively selects among the available ker-nels to achieve speedup while adhering to a target output quality set by the user. The SAGE compiler employs three optimization techniques to generate approximate kernels that exploit the GPU microarchitecture: selective discarding of atomic operations, data packing, and thread fusion. Across a set of machine learning and image processing kernels, SAGE's approximation yields an aver-age of 2.5x speedup with less than 10{\%} quality loss compared to the accurate execution on a NVIDIA GTX 560 GPU.},
author = {Samadi, Mehrzad and Lee, Janghaeng and Jamshidi, D Anoushe and Hormati, Amir and Mahlke, Scott},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Samadi et al. - 2013 - SAGE Self-Tuning Approximation for Graphics Engines.pdf:pdf},
isbn = {9781450326384},
journal = {Micro},
keywords = {all or part of,approximation,compiler,gpu,is granted without fee,not,optimization,or hard copies of,permission to make digital,provided that copies are,sonal or classroom use,this work for per-},
title = {{SAGE: Self-Tuning Approximation for Graphics Engines}},
url = {http://cccp.eecs.umich.edu/papers/samadi-micro13.pdf},
year = {2013}
}
@article{Samadi2014,
abstract = {Approximate computing is an approach where reduced accuracy of results is traded off for increased speed, throughput, or both. Loss of accuracy is not permissible in all computing domains, but there are a growing number of data-intensive domains where the output of programs need not be perfectly correct to provide useful results or even noticeable differences to the end user. These soft domains include multimedia processing, machine learning, and data mining/analysis. An important challenge with approximate computing is transparency to insulate both software and hardware developers from the time, cost, and difficulty of using approximation. This paper proposes a software-only system, Paraprox, for realizing transparent approximation of data-parallel programs that operates on commodity hardware systems. Paraprox starts with a data-parallel kernel implemented using OpenCL or CUDA and creates a parameterized approximate kernel that is tuned at runtime to maximize performance subject to a target output quality (TOQ) that is supplied by the user. Approximate kernels are created by recognizing common computation idioms found in data-parallel programs (e.g., Map, Scatter/Gather, Reduction, Scan, Stencil, and Partition) and substituting approximate implementations in their place. Across a set of 13 soft data-parallel applications with at most 10{\%} quality degradation, Paraprox yields an average performance gain of 2.7x on a NVIDIA GTX 560 GPU and 2.5x on an Intel Core i7 quad-core processor compared to accurate execution on each platform.},
author = {Samadi, Mehrzad and Jamshidi, Davoud Anoushe and Lee, Janghaeng and Mahlke, Scott},
doi = {10.1145/2541940.2541948},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Samadi et al. - 2014 - 03 Paraprox Pattern-Based Approximation for Data Parallel Applications.pdf:pdf},
isbn = {9781450323055},
issn = {0362-1340},
journal = {Int. Conf. Archit. Support Program. Lang. Oper. Syst.},
keywords = {accuracy-aware computing,approximation},
pages = {35--50},
title = {{Paraprox: Pattern-Based Approximation for Data Parallel Applications}},
url = {http://dl.acm.org/citation.cfm?doid=2541940.2541948},
year = {2014}
}
@book{Russ2008ImProc,
abstract = {1. Adjusting Pixel Values -- 2. Neighborhood Operations -- 3. Image Processing in the Fourier Domain -- 4. Binary Images -- 5. Measurements -- App. Software.},
author = {Russ, John Christian. and Russ, John Christian.},
booktitle = {Book, CRC Press},
isbn = {9780849370731},
pages = {355},
publisher = {CRC Press},
title = {{Introduction to image processing and analysis}},
year = {2008}
}
@article{Rusci2016,
abstract = {In this paper, we present an ultra-low-power smart visual sensor architecture. A 10.6-{\textless}inline-formula{\textgreater} {\textless}img src="/images/tex/27560.gif" alt="$\backslash$mu text{\{}W{\}}"{\textgreater} {\textless}/inline-formula{\textgreater} low-resolution contrast-based imager featuring internal analog preprocessing is coupled with an energy-efficient quad-core cluster processor that exploits near-threshold computing within a few milliwatt power envelope. We demonstrate the capability of the smart camera on a moving object detection framework. The computational load is distributed among mixed-signal pixel and digital parallel processing. Such local processing reduces the amount of digital data to be sent out of the node by 91{\&}{\#}x0025;. Exploiting context aware analog circuits, the imager only dispatches meaningful postprocessed data to the processing unit, lowering the sensor-to-processor bandwidth by {\textless}inline-formula{\textgreater} {\textless}img src="/images/tex/39903.gif" alt="31{\{}$\backslash$times {\}}"{\textgreater} {\textless}/inline-formula{\textgreater} with respect to transmitting a full pixel frame. To extract high-level features, an event-driven approach is applied to the sensor data and optimized for parallel runtime execution. A {\textless}inline-formula{\textgreater} {\textless}img src="/images/tex/39904.gif" alt="57.7{\{}$\backslash$times {\}}"{\textgreater} {\textless}/inline-formula{\textgreater} system energy saving is reached through the event-driven approach with respect to frame-based processing, on a low-power MCU node. The near-threshold parallel processor further reduces the processing energy cost by {\textless}inline-formula{\textgreater} {\textless}img src="/images/tex/39905.gif" alt="6.64{\{}$\backslash$times {\}}"{\textgreater} {\textless}/inline-formula{\textgreater}, achieving an overall system energy cost of {\textless}inline-formula{\textgreater} {\textless}img src="/images/tex/39906.gif" alt="1.79{\~{}}$\backslash$mu math$\backslash$rm {\{}J{\}}"{\textgreater} {\textless}/inline-formula{\textgreater} per frame, which results to be {\textless}inline-formula{\textgreater} {\textless}img src="/images/tex/39907.gif" alt="21.8{\{}$\backslash$times {\}}"{\textgreater} {\textless}/inline-formula{\textgreater} and up to {\textless}inline-formula{\textgreater} {\textless}img src="/images/tex/39908.gif" alt="383{\{}$\backslash$times {\}}"{\textgreater} {\textless}/inline-formula{\textgreater} lower than, respectively, an event-based imaging system based on an asynchronous visual sensor and a traditional frame-based smart- visual sensor.},
annote = {Process image at the sensor and send reduced information data. Image sensor is only 128x64. PULP},
author = {Rusci, Manuele and Rossi, Davide and Lecca, Michela and Gottardi, Massimo and Farella, Elisabetta and Benini, Luca},
doi = {10.1109/JSEN.2016.2556421},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rusci et al. - 2016 - An Event-Driven Ultra-Low-Power Smart Visual Sensor.pdf:pdf},
isbn = {1530-437X VO - 16},
issn = {1530437X},
journal = {IEEE Sens. J.},
keywords = {Embedded System,Event-Driven,PULP,Smart Visual Sensor},
mendeley-tags = {PULP},
number = {13},
pages = {5344--5353},
publisher = {IEEE},
title = {{An Event-Driven Ultra-Low-Power Smart Visual Sensor}},
volume = {16},
year = {2016}
}
@misc{RupertGoodwins2015,
abstract = {The competition between flash and hard disk-based storage systems will continue to drive developments in both. Flash has the upper hand in performance and benefits from Moore's Law improvements in cost per bit, but has increasing limitations in lifecycle and reliability. Finding well-engineered solutions to these will define its progress. Hard disk storage, on the other hand, has cost and capacity on its side. Maintaining those advantages is the primary driver in its roadmap.},
author = {{Rupert Goodwins}},
booktitle = {ZDNet},
title = {{The future of storage: 2015 and beyond}},
url = {http://www.zdnet.com/article/the-future-of-storage-2015-and-beyond/},
year = {2015}
}
@book{Rossant2015,
author = {Rossant, Cyrille},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rossant - 2015 - Learning IPython for Interactive Computing and Data Visualization.pdf:pdf},
isbn = {978-1-78398-698-9},
title = {{Learning IPython for Interactive Computing and Data Visualization}},
year = {2015}
}
@article{Rosenberg2011,
author = {Rosenberg, Ofer and Gaster, Benedict R and Zheng, Bixia and Lipov, Irina},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rosenberg et al. - 2011 - OpenCL Static C Kernel Language Extension.pdf:pdf},
pages = {1--12},
title = {{OpenCL Static C ++ Kernel Language Extension}},
url = {http://developer.amd.com/wordpress/media/2012/10/CPP{\_}kernel{\_}language.pdf},
year = {2011}
}
@article{Rohou2012,
abstract = {Hardware performance monitoring counters have recently received a lot of attention. They have been used by diverse communities to understand and improve the quality of computing systems: for example, architects use them to extract application characteristics and propose new hardware mechanisms, compiler writers study how generated code behaves on particular hardware, software developers identify critical regions of their applications and evaluate design choices to select the best performing implementation. In this paper, we propose that counters be used by all categories of users, in particular non-experts, and we advocate that a few simple metrics derived from these counters are relevant and useful. For example, a low IPC (number of executed instructions per cycle) indicates that the hardware is not performing at its best, a high cache miss ratio can suggest several causes, such as conflicts between processes in a multicore environment. We also introduce a new simple and flexible user-level tool that collects these data on Linux platforms, and we illustrate its practical benefits through several use cases.},
author = {Rohou, Erven},
doi = {10.1109/ICPPW.2012.58},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rohou - 2012 - Tiptop Hardware performance counters for the masses.pdf:pdf},
isbn = {9780769547954},
issn = {15302016},
journal = {Proc. Int. Conf. Parallel Process. Work.},
keywords = {PMU,hardware counters,performance,tool},
pages = {404--413},
publisher = {IEEE},
title = {{Tiptop: Hardware performance counters for the masses}},
year = {2012}
}
@article{RodriguezArreola2015,
abstract = {{\textcopyright} 2015 ACM.Systems operating from harvested sources typically integrate batteries or supercapacitors to smooth out rapid changes in harvester output. However, such energy storage devices require time for charging and increase the size, mass and cost of the system. A recent approach to address this is to power systems directly from the harvester output, termed transient computing. To solve the problem of having to restart computation from the start due to power-cycles, a number of techniques have been proposed to deal with transient power sources. In this paper, we quantitatively evaluate three state-of-the-art approaches on a Texas Instruments MSP430 microcontroller characterizing the application scenarios where each performs best. Finally, recommendations are provided to system designers for selecting the most suitable approach.},
annote = {A methodology for survival in Energy harvester driven systems. Utilises checkpointing of variables that can be utilised to recover computation after a "brown out"},
author = {{Rodriguez Arreola}, Alberto and Balsamo, Domenico and Das, Anup K. and Weddell, Alex S. and Brunelli, Davide and Al-Hashimi, Bashir M. and Merrett, Geoff V.},
doi = {10.1145/2820645.2820652},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rodriguez Arreola et al. - 2015 - Approaches to Transient Computing for Energy Harvesting Systems.pdf:pdf},
isbn = {9781450338370},
journal = {Proc. 3rd Int. Work. Energy Harvest. Energy Neutral Sens. Syst. - ENSsys '15},
keywords = {checkpoint,hibernus,iot,mementos,quickrecall},
pages = {3--8},
title = {{Approaches to Transient Computing for Energy Harvesting Systems}},
url = {http://dl.acm.org/citation.cfm?doid=2820645.2820652},
year = {2015}
}
@article{Rodrigues2011,
abstract = {As supercomputers grow, understanding their behavior and performance has become increasingly challenging. New hurdles in scalability, programmability, power consumption, reliability, cost, and cooling are emerging, along with new technologies such as 3D integration, GP-GPUs, silicon-photonics, and other "game changers". Currently, they HPC community lacks a unified toolset to evaluate these technologies and design for these challenges. To address this problem, a number of institutions have joined together to create the Structural Simulation Toolkit (SST), an open, modular, parallel, multi-criteria, multi-scale simulation framework. The SST includes a number of processor, memory, and network models. The SST has been used in a variety of network, memory, and application studies and aims to become the standard simulation framework for designing and procuring HPC systems.},
author = {Rodrigues, a. F. and CooperBalls, E. and Jacob, B. and Hemmert, K. S. and Barrett, B. W. and Kersey, C. and Oldfield, R. and Weston, M. and Risen, R. and Cook, J. and Rosenfeld, P.},
doi = {10.1145/1964218.1964225},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rodrigues et al. - 2011 - The structural simulation toolkit.pdf:pdf},
issn = {01635999},
journal = {ACM SIGMETRICS Perform. Eval. Rev.},
keywords = {architecture,simulation},
number = {4},
pages = {37},
title = {{The structural simulation toolkit}},
volume = {38},
year = {2011}
}
@incollection{Rockchip2017,
author = {Rockchip, Fuzhou and Co, Electronics},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rockchip, Co - 2017 - Rockchip TRM RK3399 Data sheet.pdf:pdf},
title = {{Rockchip TRM RK3399 Data sheet}},
year = {2017}
}
@article{Roberts1996,
abstract = {The compact disk read-only memory (CD-ROM) is a mature storage medium with complex error control. It comprises four levels of Reed Solomon codes allied to a sequence of sophisticated interleaving strategies and 8:14 modulation coding. New storage media are being developed and introduced that place still further demands on signal processing for error correction. It is therefore appropriate to explore thoroughly the limit of existing strategies to assess future requirements. We describe a simulation of all stages of the CD-ROM coding, modulation, and decoding. The results of decoding the burst error of a prescribed number of modulation bits are discussed in detail. Measures of residual uncorrected error within a sector are displayed by C1, C2, P, and Q error counts and by the status of the final cyclic redundancy check (CRC). Where each data sector is encoded separately, it is shown that error-correction performance against burst errors depends critically on the position of the burst within a sector. The C1 error measures the burst length, whereas C2 errors reflect the burst position. The performance of Reed Solomon product codes is shown by the P and Q statistics. It is shown that synchronization loss is critical near the limits of error correction. An example is given of miscorrection that is identified by the CRC check.},
author = {Roberts, J D and Ryley, A and Jones, D M and Burke, D},
doi = {10.1364/AO.35.003915},
issn = {0003-6935 (Print)},
journal = {Appl. Opt.},
number = {20},
pages = {3915--3924},
pmid = {21102793},
title = {{Analysis of error-correction constraints in an optical disk.}},
volume = {35},
year = {1996}
}
@article{Ringenburg2015,
abstract = {Energy efficiency is a key concern in the design of modern computer systems. One promising approach to energy-efficient computation, approximate computing, trades off output accuracy for significant gains in energy efficiency. However, debugging the actual cause of output quality problems in approximate programs is challenging. This paper presents dynamic techniques to debug and monitor the quality of approximate computations. We propose both offline debugging tools that instrument code to determine the key sources of output degradation and online approaches that monitor the quality of deployed applications. We present two offline debugging techniques and three online monitoring mechanisms. The first offline tool identifies correlations between output quality and the execution of individual approximate operations. The second tracks approximate operations that flow into a particular value. Our online monitoring mechanisms are complementary approaches designed for detecting quality problems in deployed applications, while still maintaining the energy savings from approximation. We present implementations of our techniques and describe their usage with seven applications. Our online monitors control output quality while still maintaining significant energy efficiency gains, and our offline tools provide new insights into the effects of approximation on output quality.},
author = {Ringenburg, Michael and Sampson, Adrian and Ackerman, Isaac and Ceze, Luis and Grossman, Dan},
doi = {10.1145/2786763.2694365},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ringenburg et al. - 2015 - Monitoring and Debugging the Quality of Results in Approximate Programs.pdf:pdf},
isbn = {9781450328357},
issn = {01635964},
journal = {ACM SIGARCH Comput. Archit. News},
number = {1},
title = {{Monitoring and Debugging the Quality of Results in Approximate Programs}},
volume = {43},
year = {2015}
}
@inproceedings{Summit,
author = {Raman, Renu},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Raman - Unknown - 40GE Petabyte scale enterprise grade for demanding cloud applications(2).pdf:pdf},
keywords = {Raman2015PbScale},
mendeley-tags = {Raman2015PbScale},
title = {{40GE : Petabyte scale enterprise grade for demanding cloud applications}}
}
@article{Rahimi2005,
abstract = {Despite their increasing sophistication, wireless sensor net- works still do not exploit the most powerful of the human senses: vision. Indeed, vision provides humans with un- matched capabilities to distinguish objects and identify their importance. Our work seeks to provide sensor networks with similar capabilities by exploiting emerging, cheap, low- power and small form factor CMOS imaging technology. In fact, we can go beyond the stereo capabilities of human vi- sion, and exploit the large scale of sensor networks to pro- vide multiple, widely different perspectives of the physical phenomena. To this end, we have developed a small camera device called Cyclops that bridges the gap between the compu- tationally constrained wireless sensor nodes such as Motes, and CMOS imagers which, while low power and inexpensive, are nevertheless designed to mate with resource-rich hosts. Cyclops enables development of new class of vision applica- tions that span across wireless sensor network. We describe our hardware and software architecture, its temporal and power characteristics and present some representative ap- plications.},
annote = {A low power and resolution 352x288 image sensor to replace multiple environmental sensors. Ideal for large scale wireless sensor network deployment with low power. Tightly constrained and architected for low power. Emphasises that high resolution images coupled with high speed processing requires a different platform.},
author = {Rahimi, Mohammad and Baer, Rick and Iroezi, Obimdinachi I and Garcia, Juan C and Warrior, Jay and Estrin, Deborah and Srivastava, Mani},
doi = {10.1590/S1518-70122011000200005},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rahimi et al. - 2005 - Cyclops In Situ Image Sensing and Interpretation in Wireless Sensor Networks.pdf:pdf},
isbn = {159593054X},
issn = {1518-7012},
journal = {Proc. 3rd Int. Conf. Embed. Networked Sens. Syst.},
keywords = {a low power micro-controller,cmos imag-,cyclops is a smart,figure 1,imaging,ing,power efficiency,sensor network,vision,vision sensor that,will be controlled by},
pages = {192--204},
pmid = {8811101},
title = {{Cyclops: In Situ Image Sensing and Interpretation in Wireless Sensor Networks}},
year = {2005}
}
@article{Rahimi2015,
abstract = {Multimedia applications running on thousands of deep and wide pipelines working concurrently in GPUs have been an important target for power minimization both at the architectural and algorithmic levels. At the hardware level, energy-efficiency techniques that employ voltage overscaling face a barrier so-called “path walls”: reducing operating voltage beyond a certain point generates massive number of timing errors that are impractical to tolerate. We propose an architec- tural innovation, called A2M2 module (approximate associative memristive memory) that exhibits few tolerable timing errors suitable for GPU applications under voltage overscaling. A2M2 is integrated with every floating point unit (FPU), and performs partial functionality of the associated FPU by pre-storing high frequency patterns for computational reuse that avoids overhead due to re-execution. Voltage overscaled A2M2 is designed to match an input search pattern with any of the stored patterns within a Hamming distance range of 0–2. This matching behavior under voltage overscaling leads to a controllable approximate computing for multimedia applications. Our experimental results for the AMD Southern Islands GPU show that four image processing kernels tolerate the mismatches during pattern matching resulting in a PSNR ≥ 30dB. The A2M2 module with 8-row enables 28{\%} voltage overscaling in 45nm technology resulting in 32{\%} average energy saving for the kernels, while delivering an acceptable quality of service.},
annote = {Original papers that quotes 30db PSNR is acceptable},
author = {Rahimi, Abbas and Ghofrani, Amirali and Cheng, Kwang-ting and Benini, Luca and Gupta, Rajesh K},
doi = {10.7873/DATE.2015.0579},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rahimi et al. - 2015 - Approximate Associative Memristive Memory for Energy-Efficient GPUs.pdf:pdf},
isbn = {9783981537048},
issn = {15301591},
journal = {Des. Autom. Test Eur. Conf. Exhib.},
pages = {1497--1502},
title = {{Approximate Associative Memristive Memory for Energy-Efficient GPUs}},
year = {2015}
}
@article{Raha2015,
abstract = {—Approximate computing is an emerging design paradigm that exploits the intrinsic ability of applications to produce acceptable outputs even when their computations are executed approximately. In this work, we explore approximate computing for a key computation pattern, Reduce-and-Rank (RnR), which is prevalent in a wide range of workloads including video processing, recognition, search and data mining. An RnR kernel performs a reduction operation (e.g., distance computation, dot product, L1-norm) between an input vector and each of a set of reference vectors, and ranks the reduction outputs to select the top reference vectors for the current input. We propose two complementary approximation strategies for the RnR computation pattern. The first is interleaved reduction-and-ranking, wherein the vector reductions are decomposed into multiple partial reductions and interleaved with the rank computation. Leveraging this trans-formation, we propose the use of intermediate reduction results and ranks to identify future computations that are likely to have low impact on the output, and can hence be approximated. The second strategy, input similarity based approximation, exploits the spatial or temporal correlation of inputs (e.g., pixels of an image or frames of a video) to identify computations that are amenable to approximation. These strategies address a key challenge in approximate computing – identification of which computations to approximate – and may be used to drive any approximation mechanism such as computation skipping and precision scaling to realize performance or energy improvements. A second key challenge in approximate computing is that the extent to which computations can be approximated varies significantly from application to application, and across inputs for even a single application. Hence, quality configurability, or the ability to automatically modulate the degree of approximation at runtime is essential. To enable quality configurability in RnR kernels, we propose a kernel-level quality metric that correlates well to application-level quality, and identify key parameters that can be used to tune the proposed approximation strategies dynamically. We develop a runtime framework that modulates the identified parameters during execution of RnR kernels to minimize their energy while meeting a given target quality. To evaluate the proposed concepts, we designed quality-configurable hardware implementations of 6 RnR-based applications from the recognition, mining, search and video processing application domains in 45nm technology. Our experiments demonstrate 1.06X-2.18X reduction in energy consumption with virtually no loss in output quality ({\textless}0.5{\%}) at the application-level. The energy benefits further improve up to 2.38X and 2.5X when the quality constraints are relaxed to 2.5{\%} and 5{\%} respectively.},
annote = {Don't quite understand where this paper is going. Uses Reduce and Rank techniquesbased on 1) Interleaved reduce and rank or 2) input similarity based approximation},
author = {Raha, Arnab and Venkataramani, Swagath and Raghunathan, Vijay and Raghunathan, Anand},
doi = {10.7873/DATE.2015.0569},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Raha et al. - 2015 - Quality configurable reduce-and-rank for energy efficient approximate computing.pdf:pdf},
isbn = {9783981537048},
issn = {15301591},
journal = {Des. Autom. Test Eur. Conf. Exhib. (DATE), 2015},
keywords = {Approximation algorithms,Approximation methods,Calibration,Correlation,Kernel,RnR computation pattern,Training,application-level quality,approximation theory,computation skipping,data mining,energy efficient approximate computing,interleaved reduction-and-ranking,kernel-level quality metric,pattern recognition,precision scaling,quality configurable reduce-and-rank,search and data mining,search problems,video processing,video recognition},
pages = {665--670},
title = {{Quality configurable reduce-and-rank for energy efficient approximate computing}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7309615},
year = {2015}
}
@article{Raha2017,
author = {Raha, Arnab and Raghunathan, Vijay},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Raha, Raghunathan - 2017 - Towards Full-System Energy-Accuracy Tradeoffs A Case Study of An Approximate Smart Camera System.pdf:pdf},
isbn = {9781450349277},
title = {{Towards Full-System Energy-Accuracy Tradeoffs : A Case Study of An Approximate Smart Camera System}},
year = {2017}
}
@article{Raghuwanshi2018,
abstract = {This paper proposes a new approach for content based image retrieval based on feed-forward architecture and Tetrolet transforms. The proposed method addresses the prob- lems of accuracy and retrieval time of the retrieval system. The proposed retrieval system works in two phases: feature extraction and retrieval. The feature extraction phase extracts the texture, edge and color features in a sequence. The texture features are extracted using Tetrolet transform. This transform provides better texture analysis by considering the local geometry of the image. Edge orientation histogram is used for retrieving the edge feature while color histogram is used for extracting the color features. Further retrieval phase retrieves the images in the feed-forward manner. At each stage, the number of images for next stage is reduced by filtering out irrelevant images. The Euclidean distance is used to measure the distance between the query and database images at each stage. The experimental results on COREL- 1 K and CIFAR - 10 benchmark databases show that the proposed system performs better in terms of the accuracy and retrieval time in comparison to the state-of-the-art methods.},
annote = {Enhances earlier work to reduce processing time and adds a feed forward system to calculate image indexes on texture, edge and colour.},
author = {Raghuwanshi, Ghanshyam and Tyagi, Vipin},
doi = {10.1007/s11042-018-5628-y},
file = {:C$\backslash$:/Users/D/Documents/Uni/Papers/Raghuwanshi-Tyagi2018{\_}Article{\_}Feed-forwardContentBasedImageR.pdf:pdf},
issn = {15737721},
journal = {Multimed. Tools Appl.},
keywords = {CBIR,Edge orientation histogram,Feed-forward,Image retrieval,Tetrolet transform},
number = {18},
pages = {23389--23410},
publisher = {Multimedia Tools and Applications},
title = {{Feed-forward content based image retrieval using adaptive tetrolet transforms}},
volume = {77},
year = {2018}
}
@article{Raghuwanshi2016,
abstract = {This paper proposes a novel technique for texture image retrieval based on tetrolet transforms. Tetrolets provide fine texture information due to its different way of analysis. Tetrominoes are applied at each decomposition level of an image and best combination of tetrominoes is selected, which better shows the geometry of an image at each level. All three high pass components of the decomposed image at each level are used as input values for feature extraction. A feature vector is created by taking standard deviation in combination with energy at each subband. Retrieval performance in terms of accuracy is tested on group of texture images taken from benchmark databases: Brodatz and VisTex. Experimental results indicate that the proposed method achieves 78.80{\%} retrieval accuracy on group of texture images D1 (taken from Brodatz), 84.41{\%} on group D2 (taken from VisTex) and 77.41{\%} on rotated texture image group D3 (rotated images from Brodatz).},
annote = {Proposes a methodology of Image identification signature for Database retrieval. Divides the image into sub areas before low pass filtering, tiling with terolets, high pass filtering, then energy and standard deviation characterisation for feature extraction to create a signature. Appears to be Computationally expensive which can be expected in database applications.},
author = {Raghuwanshi, Ghanshyam and Tyagi, Vipin},
doi = {10.1016/j.dsp.2015.09.003},
file = {:C$\backslash$:/Users/D/Documents/Uni/Papers/1-s2.0-S1051200415002717-main.pdf:pdf},
issn = {10512004},
journal = {Digit. Signal Process. A Rev. J.},
keywords = {Content-based image retrieval,Image search,Tetrolet transform,Texture image retrieval},
pages = {50--57},
publisher = {Elsevier Inc.},
title = {{Texture image retrieval using adaptive tetrolet transforms}},
url = {http://dx.doi.org/10.1016/j.dsp.2015.09.003},
volume = {48},
year = {2016}
}
@article{Raghunathan2005,
abstract = {Sustainable operation of battery powered wireless embedded systems (such as sensor nodes) is a key challenge, and considerable research effort has been devoted to energy optimization of such systems. Environmental energy harvesting, in particular solar based, has emerged as a viable technique to supplement battery supplies. However, designing an efficient solar harvesting system to realize the potential benefits of energy harvesting requires an in-depth understanding of several factors. For example, solar energy supply is highly time varying and may not always be sufficient to power the embedded system. Harvesting components , such as solar panels, and energy storage elements, such as batteries or ultracapacitors, have different voltage-current characteristics, which must be matched to each other as well as the energy requirements of the system to maximize harvesting efficiency. Further, battery non-idealities, such as self-discharge and round trip efficiency, directly affect energy usage and storage decisions. The ability of the system to modulate its power consumption by selectively deactivating its sub-components also impacts the overall power management architecture. This paper describes key issues and tradeoffs which arise in the design of solar energy harvesting, wireless embedded systems and presents the design, implementation, and performance evaluation of Heliomote, our prototype that addresses several of these issues. Experimental results demonstrate that Heliomote, which behaves as a plug-in to the Berkeley/Crossbow motes and autonomously manages energy harvesting and storage, enables near-perpetual, harvesting aware operation of the sensor node.},
annote = {Gives an insight to the challenges of environmental energy harvesting, specifically Photo voltaic generation, conversion, storage, networking and power balancing/management.},
author = {Raghunathan, Vijay and Kansal, Aman and Hsu, Jason and Org, Escholarship and Friedman, Jonathan and Srivastava, Mani},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Raghunathan et al. - 2005 - Design Considerations for Solar Energy Harvesting Wireless Embedded Systems.pdf:pdf},
journal = {IEEE IPSN},
title = {{Design Considerations for Solar Energy Harvesting Wireless Embedded Systems}},
url = {https://cloudfront.escholarship.org/dist/prd/content/qt76x92441/qt76x92441.pdf},
year = {2005}
}
@article{Quek2018,
abstract = {Renewable and sustainable are often used synonymously. This study examines the sustainability of switching from fossil-based electricity generation to renewable energy systems. Life cycle assessment (LCA) of both fossil- and renewable-based electricity generation were modeled for four environmental impact categories, in the context of Singapore city. The LCA method followed the CML methodology. Four major environmental impacts were calculated: global warming potential (GWP), acidification potential (AP), eutrophication potential (EP) and human toxicity potential (HTP). The cradle-to-gate analyses were done for various types of electricity supply sources: Solar photovoltaics (PV), biogas, municipal waste incineration, natural gas, coal and oil. The current mix of 95.2{\%} natural gas, 3{\%} incineration, 1{\%} petroleum products and 0.8{\%} coal was used as baseline for comparison with other mixes. The results showed that having a high proportion of electricity generated from solar PVs can produce ten times higher HTP than baseline scenario. Similarly, electricity from biogas can produce eight times higher AP and EP than baseline. Further analysis showed that HTP from solar PVs impacts locally, but mostly external to Singapore. AP from biogas impacts regionally, within and without Singapore. Policies about renewable energy in the overall electricity mix would thus be value- based, rather than purely economic or environmental.},
author = {Quek, Augustine and Ee, Alvin and Ng, Adam and Wah, Tong Yen},
doi = {10.1016/j.enpol.2018.07.055},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Quek et al. - 2018 - Challenges in Environmental Sustainability of renewable energy options in Singapore.pdf:pdf},
issn = {03014215},
journal = {Energy Policy},
number = {April},
pages = {388--394},
publisher = {Elsevier Ltd},
title = {{Challenges in Environmental Sustainability of renewable energy options in Singapore}},
url = {https://doi.org/10.1016/j.enpol.2018.07.055},
volume = {122},
year = {2018}
}
@article{Quast2016,
author = {Quast, Kathleen B and Ung, Kevin and Froudarakis, Emmanouil and Huang, Longwen and Herman, Isabella and Addison, Angela P and Ortiz-Guzman, Joshua and Cordiner, Keith and Saggau, Peter and Tolias, Andreas S and Arenkiel, Benjamin R},
doi = {10.1038/nn.4467},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Quast et al. - 2016 - Developmental broadening of inhibitory sensory maps.pdf:pdf},
issn = {1097-6256},
journal = {Nat. Neurosci.},
number = {December},
publisher = {Nature Publishing Group},
title = {{Developmental broadening of inhibitory sensory maps}},
url = {http://www.nature.com/doifindQuast, K. B., Ung, K., Froudarakis, E., Huang, L., Herman, I., Addison, A. P., {\ldots}Arenkiel, B. R. (2016). Developmental broadening of inhibitory sensory maps. Nature Neuroscience, (December). http://doi.org/10.1038/nn.4467er/10},
year = {2016}
}
@article{Issa2017,
abstract = {Approximate arithmetic has recently emerged as a promising paradigm for many imprecision-tolerant applications. It can offer substantial reductions in circuit complexity, delay and energy consumption by relaxing accuracy requirements. In this paper, we propose a novel energy-efficient approximate multiplier design using a significance-driven logic compression (SDLC) approach. Fundamental to this approach is an algorithmic and configurable lossy compression of the partial product rows based on their progressive bit significance. This is followed by the commutative remapping of the resulting product terms to reduce the number of product rows. As such, the complexity of the multiplier in terms of logic cell counts and lengths of critical paths is drastically reduced. A number of multipliers with dif- ferent bit-widths (4-bit to 128-bit) are designed in SystemVerilog and synthesized using Synopsys Design Compiler. Post-synthesis experiments showed that up to an order of magnitude energy savings, and reductions of 65{\%} in critical delay and almost 45{\%} in silicon area can be achieved for a 128-bit multiplier compared to an accurate equivalent. These gains are achieved with low accuracy losses estimated at less than 0.00071 mean relative error. Additionally, we demonstrate the energy-accuracy trade-offs for different degrees of compression, achieved through configurable logic clustering. In evaluating the effectiveness of our approach, a case study image processing application showed up to 68.3{\%} energy reduction with negligible losses in image quality expressed as peak signal-to-noise ratio (PSNR). I.},
author = {Qiqieh, Issa and Shafik, Rishad and Tarawneh, Ghaith and Sokolov, Danil and Yakovlev, Alex},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Qiqieh et al. - 2017 - Energy-Efficient Approximate Multiplier Design using Bit Significance-Driven Logic Compression(2).pdf:pdf},
institution = {IEEE},
journal = {Des. Autom. Test Eur.},
keywords = {Approximate Multiplier,SDLC},
mendeley-tags = {Approximate Multiplier,SDLC},
pages = {(in press)},
title = {{Energy-Efficient Approximate Multiplier Design using Bit Significance-Driven Logic Compression}},
year = {2017}
}
@article{PuDong-Mei;Yuan2013,
abstract = {In this paper, concepts of the first- and second-order differen? tials of images are presented to deal with the changes of pixels. They are the basic ideas in mathematics. We propose and refor? mulate them with a uniform definition framework. Based on our observation and analysis of the difference, we propose an algo? rithm to detect the edge from an image. Experiments on Corel 5K and PASCAL VOC2007 are carried out to show the difference between the first- and the second-order differentials. After a com? parison with Canny operator and the proposed first-order differ? ential, the main result shows that the second-order differential has a better performance on analyzing context changes of an image. Keywords:},
author = {{Pu, Dong-Mei ; Yuan}, Yu-bo},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pu, Dong-Mei Yuan - 2013 - FIRST AND SECOND ORDER FULL-DIFFERENTIAL IN THE EDGE DETECTION OF IMAGES.pdf:pdf},
isbn = {9781479902606},
journal = {Proc. 2013 Int. Conf. Mach. Learn. Cybern.},
keywords = {Differential Operator,Edge Detection,Feature Selection,Image Processing},
pages = {14--17},
title = {{FIRST AND SECOND ORDER FULL-DIFFERENTIAL IN THE EDGE DETECTION OF IMAGES}},
year = {2013}
}
@article{preston1988need,
author = {Preston, Kendall},
doi = {10.1038/333611a0},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Preston - 1988 - The need for standards in image processing(2).pdf:pdf},
issn = {0028-0836},
journal = {Nature},
number = {6174},
pages = {611--612},
publisher = {Nature Publishing Group},
title = {{The need for standards in image processing}},
url = {http://www.nature.com/doifinder/10.1038/333611a0},
volume = {333},
year = {1988}
}
@article{Preissl2012,
abstract = {—Inspired by the function, power, and volume of the organic brain, we are developing TrueNorth, a novel modu-lar, non-von Neumann, ultra-low power, compact architecture. TrueNorth consists of a scalable network of neurosynaptic cores, with each core containing neurons, dendrites, synapses, and axons. To set sail for TrueNorth, we developed Compass, a multi-threaded, massively parallel functional simulator and a parallel compiler that maps a network of long-distance pathways in the macaque monkey brain to TrueNorth. We demonstrate near-perfect weak scaling on a 16 rack IBM{\textregistered} Blue Gene{\textregistered}/Q (262144 CPUs, 256 TB memory), achieving an unprecedented scale of 256 million neurosynaptic cores containing 65 billion neurons and 16 trillion synapses running only 388× slower than real time with an average spiking rate of 8.1 Hz. By using emerging PGAS communication primitives, we also demonstrate 2× better real-time performance over MPI primitives on a 4 rack Blue Gene/P (16384 CPUs, 16 TB memory). I. INTRODUCTION The brain and modern computers have radically different architectures [1] suited for complementary applications. Mod-ern computing posits a stored program model, traditionally implemented in digital, synchronous, serial, centralized, fast, hardwired, general-purpose, brittle circuits, with explicit mem-ory addressing imposing a dichotomy between computation and data. In stark contrast, the brain uses replicated computa-tional units of neurons and synapses implemented in mixed-mode analog-digital, asynchronous, parallel, distributed, slow, reconfigurable, specialized, and fault-tolerant biological sub-strates, with implicit memory addressing blurring the boundary between computation and data [2]. It is therefore no surprise that one cannot emulate the function, power, volume, and real-time performance of the brain within the modern computer architecture. This task requires a radically novel architecture. Today, one must still build novel architectures in CMOS technology, which has evolved over the past half-century to serve modern computers and which is not optimized for delivering brain-like functionality in a compact, ultra-low-power package. For example, biophysical richness of neurons and 3D physical wiring are out of the question at the very outset. We need to shift attention from neuroscientific richness that is sufficient to mathematical primitives that are necessary. A question of profound relevance to science, technology, business, government, and society is how closely can one approximate the function, power, volume, and real-time per-formance of the brain within the limits of modern technology.},
author = {Preissl, Robert and Wong, Theodore M. and Datta, Pallab and Flickner, Myron and Singh, Raghavendra and Esser, Steven K. and Risk, William P. and Simon, Horst D. and Modha, Dharmendra S.},
doi = {10.1109/SC.2012.34},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Preissl et al. - 2012 - Compass A scalable simulator for an architecture for cognitive computing.pdf:pdf},
isbn = {9781467308069},
issn = {21674329},
journal = {Int. Conf. High Perform. Comput. Networking, Storage Anal. SC},
title = {{Compass: A scalable simulator for an architecture for cognitive computing}},
year = {2012}
}
@inproceedings{prasanthi2005multiplier,
author = {Prasanthi, R and Anuradha, V and Sahoo, S K and Shekhar, C},
booktitle = {Intell. Sens. Inf. Process. 2005. Proc. 2005 Int. Conf.},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Prasanthi et al. - 2005 - Multiplier less FFT processor architecture for signal and image processing.pdf:pdf},
organization = {IEEE},
pages = {326--330},
title = {{Multiplier less FFT processor architecture for signal and image processing}},
year = {2005}
}
@article{Piguet2004,
abstract = {(not listed)},
author = {Piguet, Christian and Schuster, Christian and Nagel, Jean-Luc},
doi = {10.1109/NEWCAS.2004.1359011},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Piguet, Schuster, Nagel - 2002 - Optimizing Architecture Activity and Logic Depth for Static and Dynamic Power Reduction.pdf:pdf},
isbn = {0780383222},
journal = {2nd Annu. IEEE Northeast Work. Circuits Syst. 2004. NEWCAS 2004.},
pages = {41--44},
title = {{Optimizing Architecture Activity and Logic Depth for Static and Dynamic Power Reduction}},
year = {2002}
}
@article{Paul2015,
author = {Paul, Devashish and Marketing, Strategic and Solutions, Systems},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Paul, Marketing, Solutions - 2015 - Supercomputing Clusters with RapidIO Interconnect Fabric.pdf:pdf},
title = {{Supercomputing Clusters with RapidIO Interconnect Fabric}},
year = {2015}
}
@article{Palem2018,
author = {Palem, Krishna V and A, Phil Trans R Soc and Palem, Krishna V},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Palem, A, Palem - 2018 - Inexactness and a future of computing Inexactness and a future of computing.pdf:pdf},
keywords = {computer-aided design,hybrid computing,pattern recognition,software,systems theory,theory of computing},
number = {May 2014},
title = {{Inexactness and a future of computing Inexactness and a future of computing}},
year = {2018}
}
@article{Paine2015,
author = {Paine, Jeff},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Paine - 2015 - How Network Performance Enforcement Can Double Ethernet Capacity An Inconvenient Fact(2).pdf:pdf},
number = {April},
title = {{How Network Performance Enforcement Can Double Ethernet Capacity An Inconvenient Fact}},
year = {2015}
}
@article{Pacheco2011,
abstract = {With the coming of multicore processors and the cloud, parallel computing is most cer-tainly not a niche area off in a corner of the computing world. Parallelism has become central to the efficient use of resources, and this new textbook by Peter Pacheco will go a long way toward introducing students early in their academic careers to both the art and practice of parallel computing. An Introduction to Parallel Programming illustrates fundamental programming principles in the increasingly important area of shared-memory programming using Pthreads and OpenMP and distributed-memory programming using MPI. More important, it empha-sizes good programming practices by indicating potential performance pitfalls. These topics are presented in the context of a variety of disciplines, including computer science, physics, and mathematics. The chapters include numerous programming exercises that range from easy to very challenging. This is an ideal book for students or professionals looking to learn parallel programming skills or to refresh their knowledge.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Pacheco, Peter S.},
doi = {10.1007/978-1-4471-2736-9},
eprint = {arXiv:1011.1669v3},
isbn = {9780123814722},
issn = {01928651},
journal = {Elsevier Inc.},
keywords = {and this new textbook,area off in a,by peter pacheco will,central to the efficient,cer-,cloud,corner of the computing,go a,multicore processors and the,parallel computing is most,parallelism has become,praise of an introduction,tainly not a niche,to parallel programming,use of resources,with the coming of,world},
pages = {391},
pmid = {17565499},
title = {{Introduction to Parallel Programming}},
url = {http://aims.me.cycu.edu.tw/courses/101-2/IPMC/lecture{\_}material/Introduction to Parallel Programming - Peter Pacheco (2010).pdf},
year = {2011}
}
@book{Paar2010,
abstract = {Cryptography is now ubiquitous – moving beyond the traditional environments, such as government communications and banking systems, we see cryptographic techniques realized in Web browsers, e-mail programs, cell phones, manufacturing systems, embedded software, smart buildings, cars, and even medical implants. Today's designers need a comprehensive understanding of applied cryptography. After an introduction to cryptography and data security, the authors explain the main techniques in modern cryptography, with chapters addressing stream ciphers, the Data Encryption Standard (DES) and 3DES, the Advanced Encryption Standard (AES), block ciphers, the RSA cryptosystem, public-key cryptosystems based on the discrete logarithm problem, elliptic-curve cryptography (ECC), digital signatures, hash functions, Message Authentication Codes (MACs), and methods for key establishment, including certificates and public-key infrastructure (PKI). Throughout the book, the authors focus on communicating the essentials and keeping the mathematics to a minimum, and they move quickly from explaining the foundations to describing practical implementations, including recent topics such as lightweight ciphers for RFIDs and mobile devices, and current key-length recommendations. The authors have considerable experience teaching applied cryptography to engineering and computer science students and to professionals, and they make extensive use of examples, problems, and chapter reviews, while the book's website offers slides, projects and links to further resources. This is a suitable textbook for graduate and advanced undergraduate courses and also for self-study by engineers.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Paar, Christof and Pelzl, Jan},
doi = {10.1007/978-3-642-04101-3},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Paar, Pelzl - 2010 - Understanding Cryptography.pdf:pdf},
isbn = {978-3-642-44649-8},
issn = {0717-6163},
keywords = {AES Applied cryptography Crib Cryptography Cryptol},
pmid = {15003161},
publisher = {Springer},
title = {{Understanding Cryptography}},
url = {http://link.springer.com/10.1007/978-3-642-04101-3},
year = {2010}
}
@article{Ousterhout2015Storage,
abstract = {WHEN IT COMES TO COMPUTER STORAGE, the magnetic disk has been top dog for almost half a century. The first com- mercial disks appeared in 1956, and by the early 1970s their cost and capacity had improved to the point where they began to replace magnetic tape as the primary storage medium for computers. By the end of that decade, tapes had been relegated mostly to a backup role. Since then, disk technology has improved at an exponential rate, just like integrated circuits. Nowadays, a typical drive holds 20,000 times as much data as it did in 1985, and on a per- byte basis, disks cost one-millionth of what they did then.No wonder hard disks are so pervasive. This is also why today's popular forms of computer storage, such as file systems and relational databases, were designed with disks in mind. Indeed, until recently any information kept on a computer for more than a few seconds proba- bly ended up on disk. • But the hard disk's reign may be coming to an end. The most obvious challenger is flash memory, which is faster, more compact, and more resis- tant to shock. Virtually all mobile devices, such as tablets, smartphones, and watches, already use flash instead of disk. Flash memory is also displacing hard drives in lap- tops and, increasingly, in large-scale applications running in data centers, where its speed is a significant advantage.},
author = {Ousterhout, John},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ousterhout - Unknown - The Volatile Future of Storage(2).pdf:pdf},
journal = {IEEE spectrum.},
keywords = {Storage future},
mendeley-tags = {Storage future},
number = {Issue: 11, 2015},
pages = {34 -- 40},
title = {{The Volatile Future of Storage}},
volume = {Volume: 52}
}
@article{Operation2011,
author = {Operation, Device},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Operation - 2011 - Ddr3L Sdram.pdf:pdf},
journal = {Micron},
pages = {1--151},
title = {{Ddr3L Sdram}},
volume = {11},
year = {2011}
}
@misc{getopt,
author = {Opengroup},
title = {{\textless}unistd.h{\textgreater} getopt()},
url = {http://pubs.opengroup.org/onlinepubs/7908799/xsh/getopt.html}
}
@article{Nuss2015,
author = {Nuss, Martin and Semiconductor, Vitesse},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nuss, Semiconductor - 2015 - Ethernet {\&} The Internet of Things ( IoT ) IoT – Tremendous Expectations(2).pdf:pdf},
keywords = {Nuss2015EnIoT},
mendeley-tags = {Nuss2015EnIoT},
number = {April},
pages = {1--16},
title = {{Ethernet {\&} The Internet of Things ( IoT ) IoT – Tremendous Expectations}},
year = {2015}
}
@article{Nugteren2017,
abstract = {This work demonstrates how to accelerate dense linear algebra computations using CLBlast, an open-source OpenCL BLAS library providing optimized routines for a wide variety of devices. It is targeted at machine learning and HPC applications and thus provides a fast matrix-multiplication routine (GEMM) to accelerate the core of many applications (e.g. deep learning, iterative solvers, astrophysics, computational fluid dynamics, quantum chemistry). CLBlast has four main advantages over other BLAS libraries: 1) it is optimized for and tested on a large variety of OpenCL devices including less commonly used devices such as embedded and low-power GPUs, 2) it can be explicitly tuned for specific problem-sizes on specific hardware platforms, 3) it can perform operations in half-precision floating-point FP16 saving precious bandwidth, time and energy, 4) and it can combine multiple operations in a single batched routine, accelerating smaller problems significantly. This paper describes the library and demonstrates the advantages of CLBlast experimentally for different use-cases on a wide variety of OpenCL hardware.},
archivePrefix = {arXiv},
arxivId = {1705.05249},
author = {Nugteren, Cedric},
eprint = {1705.05249},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nugteren - 2017 - CLBlast A Tuned OpenCL BLAS Library.pdf:pdf},
number = {C},
title = {{CLBlast: A Tuned OpenCL BLAS Library}},
url = {http://arxiv.org/abs/1705.05249},
year = {2017}
}
@article{Niklaus2017,
abstract = {Standard video frame interpolation methods first estimate optical flow between input frames and then synthesize an intermediate frame guided by motion. Recent approaches merge these two steps into a single convolution process by convolving input frames with spatially adaptive kernels that account for motion and re-sampling simultaneously. These methods require large kernels to handle large motion, which limits the number of pixels whose kernels can be estimated at once due to the large memory demand. To address this problem, this paper formulates frame interpolation as local separable convolution over input frames using pairs of 1D kernels. Compared to regular 2D kernels, the 1D kernels require significantly fewer parameters to be estimated. Our method develops a deep fully convolutional neural network that takes two input frames and estimates pairs of 1D kernels for all pixels simultaneously. Since our method is able to estimate kernels and synthesizes the whole video frame at once, it allows for the incorporation of perceptual loss to train the neural network to produce visually pleasing frames. This deep neural network is trained end-to-end using widely available video data without any human annotation. Both qualitative and quantitative experiments show that our method provides a practical solution to high-quality video frame interpolation.},
archivePrefix = {arXiv},
arxivId = {1708.01692},
author = {Niklaus, Simon and Mai, Long and Liu, Feng},
doi = {10.1109/CVPR.2017.244},
eprint = {1708.01692},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Niklaus, Mai, Liu - 2017 - Video frame interpolation via adaptive convolution.pdf:pdf},
isbn = {9781538604571},
issn = {15505499},
journal = {Proc. - 30th IEEE Conf. Comput. Vis. Pattern Recognition, CVPR 2017},
pages = {2270--2279},
title = {{Video frame interpolation via adaptive convolution}},
volume = {2017-Janua},
year = {2017}
}
@article{Nguyen2014,
abstract = {For a standalone Fall Detection system based on computer vision we want to obtain a low power architecture to meet the real time processing, power consumption, energy constraints which also satisfy the high performance in recognition, and accuracy. In this paper, we present the different architecture explorations for Fall Detection system implemented on heterogeneous platform as Zynq-7000 AP SoC platform. We extract the power models based on measurement to have more accuracy for Fall Detection system. The estimation of execution time was taking on Pcore processor like ARM Cortex A9 to find out the candidate for accelerating on Hardware (FPGAs) implementation. Then we analyze the features of power consumption, frame rate, and energy to get the best compromise architecture for standalone Fall Detection system.},
author = {Nguyen, Hong Thi Khanh and Fahama, Hassoon and Belleudy, Cecile and Pham, Tuan Van},
doi = {10.1109/EMS.2014.100},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nguyen et al. - 2014 - Low power architecture exploration for standalone fall detection system based on computer vision.pdf:pdf},
isbn = {9781479974115},
journal = {Proc. - UKSim-AMSS 8th Eur. Model. Symp. Comput. Model. Simulation, EMS 2014},
keywords = {Architecture exploration,Fall Detection system,Power consumption,Power model},
pages = {169--173},
title = {{Low power architecture exploration for standalone fall detection system based on computer vision}},
year = {2014}
}
@article{Nane2016,
abstract = {High-level synthesis (HLS) is increasingly popular for the design of high-performance and energy-efficient heteroge- neous systems, shortening time-to-market and addressing today's system complexity. HLS allows designers to work at a higher- level of abstraction by using a software program to specify the hardware functionality. Additionally,HLS is particularly interest- ing for designing field-programmable gate array circuits, where hardware implementations can be easily refined and replaced in the target device. Recent years have seen much activity in the HLS research community, with a plethora of HLS tool offer- ings, from both industry and academia. All these tools may have different input languages, perform different internal optimiza- tions, and produce results of different quality, even for the very same input description. Hence, it is challenging to compare their performance and understand which is the best for the hard- ware to be implemented. We present a comprehensive analysis of recent HLS tools, as well as overview the areas of active interest in the HLS research community. We also present a first- published methodology to evaluate different HLS tools. We use our methodology to compare one commercial and three academic tools on a common set of C benchmarks, aiming at perform- ing an in-depth evaluation in terms of performance and the use of resources. Index},
author = {Nane, Razvan and Sima, Vlad Mihai and Pilato, Christian and Choi, Jongsok and Fort, Blair and Canis, Andrew and Chen, Yu Ting and Hsiao, Hsuan and Brown, Stephen and Ferrandi, Fabrizio and Anderson, Jason and Bertels, Koen},
doi = {10.1109/TCAD.2015.2513673},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nane et al. - 2016 - A Survey and Evaluation of FPGA High-Level Synthesis Tools(2).pdf:pdf},
isbn = {0278-0070 VO - PP},
issn = {02780070},
journal = {IEEE Trans. Comput. Des. Integr. Circuits Syst.},
keywords = {Bambu,Dwarv,LegUp,Survey,comparison,evaluation,field-programmable gate array (FPGA),high-level synthesis (HLS)},
title = {{A Survey and Evaluation of FPGA High-Level Synthesis Tools}},
year = {2016}
}
@article{Nair2017,
abstract = {{\textcopyright} 2017 Springer Science+Business Media, LLC The design and development of image processing units (IPUs) has traditionally involved trade-offs between cost, real-time properties, portability, and ease of programming. A standard PC can be turned into an IPU relatively easily with the help of readily available computer vision libraries, but the end result will not be portable, and may be costly. Similarly, one can use field programmable gate arrays (FPGAs) as the base for an IPU, but they are expensive and require hardware-level programming. Finally, general purpose embedded hardware tends to be under-powered and difficult to develop for due to poor support for running advanced software. In recent years a new option has surfaced: single-board computers (SBCs). These generally inexpensive embedded devices would be attractive as a platform on which to develop IPUs due to their inherent portability and good compatibility with existing computer vision (CV) software. However, whether their performance is sufficient for real-time image processing has thus far remained an open question. Most SBCs (especially the ultra-low-cost ones which we target) do not offer CUDA/OpenCL support which makes it difficult to port GPU-based CV applications. In order to utilize the full power of the SBCs, their GPUs need to be used. In our attempts at doing this, we have observed that the CV algorithms which an IPU uses have to be re-designed according to the OpenGL support available on these devices. This work presents a framework where a selection of CV algorithms have been designed in a way that they optimize performance on SBCs while still maintaining portability across devices which offer OpenGL ES 2.0 support. Furthermore, this paper demonstrates an IPU based on a representative SBC (namely the Raspberry Pi) along with two CV applications backed by it. The robustness of the applications as well as the performance of the IPU are evaluated to show that SPCs can be used to build IPUs capable of producing accurate data in real time. This opens the possibilities of large scale economically deployment of vision system especially in remote and barren lands. Finally, the software developed as a part of this work has been released open source.},
author = {Nair, Suraj and Somani, Nikhil and Grunau, Artur and Dean-Leon, Emmanuel and Knoll, Alois},
doi = {10.1007/s11265-017-1267-1},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nair et al. - 2017 - Image Processing Units on Ultra-low-cost Embedded Hardware Algorithmic Optimizations for Real-time Performance(2).pdf:pdf},
issn = {19398115},
journal = {J. Signal Process. Syst.},
keywords = {Embedded vision,Human tracking,Image processing units},
pages = {1--17},
publisher = {Journal of Signal Processing Systems},
title = {{Image Processing Units on Ultra-low-cost Embedded Hardware: Algorithmic Optimizations for Real-time Performance}},
year = {2017}
}
@article{Lucas2017,
abstract = {—Approximate circuits and approximate circuit design methodologies attracted a significant attention of researchers as well as industry in recent years. In order to accelerate the approximate circuit and system design process and to support a fair benchmarking of circuit approximation meth-ods, we propose a library of approximate adders and mul-tipliers called EvoApprox8b. This library contains 430 non-dominated 8-bit approximate adders created from 13 conven-tional adders and 471 non-dominated 8-bit approximate mul-tipliers created from 6 conventional multipliers. These imple-mentations were evolved by a multi-objective Cartesian ge-netic programming. The EvoApprox8b library provides Verilog, Matlab and C models of all approximate circuits. In addition to standard circuit parameters, the error is given for seven different error metrics. The EvoApprox8b library is available at:},
author = {Mrazek, Vojtech and Hrbacek, Radek and Vasicek, Zdenek and Sekanina, Lukas and Mrazeka, Vojtech and Hrbacekb, Radek and Vasicekc, Zdenek and Sekaninad, Lukas},
doi = {10.23919/DATE.2017.7926993},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mrazek et al. - 2017 - EvoApprox8b Library of Approximate Adders and Multipliers for Circuit Design and Benchmarking of Approximation(2).pdf:pdf},
institution = {IEEE},
isbn = {9783981537086},
journal = {Des. Autom. Test Eur.},
pages = {258--261},
title = {{EvoApprox8b: Library of Approximate Adders and Multipliers for Circuit Design and Benchmarking of Approximation Methods}},
url = {www.fit.vutbr.cz/research/groups/ehw/approxlib},
year = {2017}
}
@article{Moreau2018,
abstract = {—Approximate computing is the idea that systems can gain performance and energy efficiency if they expend less effort on producing a " perfect " answer. Approximate computing tech-niques propose various ways of exposing and exploiting accuracy– efficiency trade-offs. We present a taxonomy that classifies ap-proximate computing techniques according to their most salient features: compute vs. data, deterministic vs. nondeterministic and coarse-vs. fine-grained. These axes allow us to address questions about the visibility, testability and flexibility of different techniques. We use this taxonomy to inform future research in approximate architectures, compilers and applications that will catalyze mainstream adoption of approximate computing.},
author = {Moreau, Thierry and {San Miguel}, Joshua and Wyse, Mark and Bornholt, James and Alaghi, Armin and Ceze, Luis and {Enright Jerger}, Natalie and Sampson, Adrian},
doi = {10.1109/LES.2017.2758679},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Moreau et al. - 2018 - A Taxonomy of General Purpose Approximate Computing Techniques.pdf:pdf},
issn = {19430663},
journal = {IEEE Embed. Syst. Lett.},
number = {1},
title = {{A Taxonomy of General Purpose Approximate Computing Techniques}},
volume = {10},
year = {2018}
}
@article{Moreau2015,
abstract = {Approximate systems can reclaim energy that's currently lost to the "correctness tax" imposed by traditional safety margins designed to prevent worst-case scenarios. Researchers at the University of Washington have co-designed programming language extensions, a compiler, and a hardware co-processor to support approximate acceleration. Their end-to-end system includes two building blocks. First, a new programmer-guided compiler framework transforms programs to use approximation in a controlled way. An Approximate C Compiler for Energy and Performance Tradeoffs (Accept) uses programmer annotations, static analysis, and dynamic profiling to find parts of a program that are amenable to approximation. Second, the compiler targets a system on a chip (SoC) augmented with a co-processor that can efficiently evaluate coarse regions of approximate code. A Systolic Neural Network Accelerator in Programmable logic (Snnap) is a hardware accelerator prototype that can efficiently evaluate approximate regions of code in a general-purpose program.},
author = {Moreau, Thierry and Sampson, Adrian and Ceze, Luis},
doi = {10.1109/MPRV.2015.25},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Moreau, Sampson, Ceze - 2015 - Approximate Computing Making mobile systems more efficient(2).pdf:pdf},
issn = {15361268},
journal = {IEEE Pervasive Comput.},
keywords = {ACCEPT,SNNAP,approximate computing,energy efficiency,green computing,mobile,pervasive computing},
number = {2},
pages = {9--13},
title = {{Approximate Computing: Making mobile systems more efficient}},
volume = {14},
year = {2015}
}
@article{Mooney,
author = {Mooney, Paul and Garcia, Gilles},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mooney, Garcia - Unknown - 400GbE Core Routing Implementation.pdf:pdf},
title = {{400GbE Core Routing Implementation}}
}
@article{Monson2015-100-25GLowPowerOptical,
author = {Monson, John},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Monson - Unknown - Low Power PHYs for 100G 25G Copper and Optical Module Solutions(2).pdf:pdf},
title = {{Low Power PHYs for 100G / 25G Copper and Optical Module Solutions}}
}
@article{Mohapatra2009,
abstract = {Abstract In this paper we present a design methodology for algorithm/architecture co-design of a voltage - scalable , process variation aware motion estimator based on significance driven computation . The fundamental premise of our approach lies in the fact that all ... $\backslash$n},
annote = {significance driven computation based on motion estimation of components in the frame},
author = {Mohapatra, Debabrata and Karakonstantis, Georgios and Roy, Kaushik},
doi = {10.1145/1594233.1594282},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mohapatra, Karakonstantis, Roy - 2009 - Significance Driven Computation A Voltage-Scalable , Variation-Aware , Quality-Tuning Motion (2).pdf:pdf},
isbn = {9781605586847},
issn = {15334678},
journal = {Proc. 14th ACM/IEEE Int. Symp. Low power Electron. Des. - ISLPED '09},
keywords = {low power,motion estimation,significance driven computation,variation aware,voltage over-scaling},
pages = {195},
title = {{Significance Driven Computation : A Voltage-Scalable , Variation-Aware , Quality-Tuning Motion Estimator}},
url = {http://portal.acm.org/citation.cfm?doid=1594233.1594282},
year = {2009}
}
@article{Mittal2017,
abstract = {Graphics processing unit (GPU), although a powerful performance-booster, also has many security vulnerabili-ties. Due to these, the GPU can act as a safe-haven for stealthy malware and the weakest 'link' in the security 'chain'. In this paper, we present a survey of techniques for analyzing and improving GPU security. We classify the works on key attributes to highlight their similarities and differences. More than informing users and researchers about GPU security techniques, this survey aims to increase their awareness about GPU security vulnerabilities and potential countermeasures.},
archivePrefix = {arXiv},
arxivId = {1804.00114},
author = {Mittal, Sparsh and Abhinaya, S B and Reddy, Manish and Ali, Irfan},
eprint = {1804.00114},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mittal et al. - 2017 - A Survey of Techniques for Improving Security of GPUs.pdf:pdf},
keywords = {Index Terms GPU,buffer overflow,covert channel,encryption,information leakage,malware,review,security,side-channel},
pages = {1--19},
title = {{A Survey of Techniques for Improving Security of GPUs}},
url = {https://arxiv.org/pdf/1804.00114.pdf},
year = {2017}
}
@article{MITTAL2013,
abstract = {Approximate computing trades off computation quality with the effort expended and as rising performance demands confront with plateauing resource budgets, approximate computing has become, not merely attractive, but even imperative. In this paper, we present a survey of techniques for approximate computing (AC). We discuss strategies for finding approximable program portions and monitoring output quality, techniques for using AC in different processing units (e.g., CPU, GPU and FPGA), processor components, memory technologies etc., and programming frameworks for AC. We classify these techniques based on several key characteristics to emphasize their similarities and differences. The aim of this paper is to provide insights to researchers into working of AC techniques and inspire more efforts in this area to make AC the mainstream computing approach in future systems. A Survey Of Techniques for Approximate Computing (PDF Download Available). Available from: https://www.researchgate.net/publication/290194892{\_}A{\_}Survey{\_}Of{\_}Techniques{\_}for{\_}Approximate{\_}Computing [accessed May 30, 2016].},
annote = {techniques, strategies and the various processes to be involved, eg Quality checking, randomisation of data where lower order bits are ignored, etc


reinforces data centre power consumption.
Surveys various AC techniques},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Mittal, SPARSH},
doi = {10.1145/2893356},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mittal - 2016 - A Survey of Techniques for Approximate Computing(3).pdf:pdf},
isbn = {9788578110796},
issn = {03600300},
journal = {ACM Comput. Surv.},
keywords = {CPU,FPGA,GPU,Review,Survey,Techniques,approx- imate storage,approximate computing technique (ACT),classification,icle,neural networks,quality configurability},
mendeley-tags = {Survey,Techniques},
number = {4},
pages = {1--33},
pmid = {25246403},
title = {{A Survey of Techniques for Approximate Computing}},
url = {http://dl.acm.org/citation.cfm?doid=2891449.2893356},
volume = {48},
year = {2016}
}
@article{Mittal2016,
abstract = {Recent trends of increasing core-count and bandwidth/memory wall have motivated researchers to explore novel memory technologies for designing processor components such as cache, register file, shared memory, and so on. Domain-wall memory (DWM), also known as racetrack memory, is a promising emerging technology due to its non-volatility and very high density. However, use of DWM presents challenges due to characteristics of both DWM itself (e.g., requirement of shift operations, variable latency) and processor components. Recently, several techniques have been proposed to address these challenges. This article presents a survey of architectural techniques for using DWM for designing components in both CPU and GPU. We discuss techniques related to performance, energy, and reliability and also discuss works that compare DWM with other memory technologies. We also highlight the opportunities and obstacles in using DWM for designing processor components. This survey is expected to spark further research in this area and be useful for researchers, chip designers, and computer architects.},
author = {Mittal, Sparsh},
doi = {10.1145/2994550},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mittal - 2016 - A Survey of Techniques for Architecting Processor Components Using Domain-Wall Memory(3).pdf:pdf},
issn = {15504832},
journal = {ACM J. Emerg. Technol. Comput. Syst.},
number = {2},
pages = {1--25},
title = {{A Survey of Techniques for Architecting Processor Components Using Domain-Wall Memory}},
url = {http://dl.acm.org/citation.cfm?doid=3014160.2994550},
volume = {13},
year = {2016}
}
@book{Mitchell2009,
abstract = {The emerging field of Ecosystem Informatics applies methods from computer science and mathematics to address fundamental and applied problems in the ecosystem sciences. The ecosystem sciences are in the midst of a revolution driven by a combination of emerging technologies for improved sensing and the critical need for better science to help manage global climate change. This paper describes several initiatives at Oregon State University in ecosystem informatics. At the level of sensor technologies, this paper describes two projects: (a) wireless, battery-free sensor networks for forests and (b) rapid throughput automated arthropod population counting. At the level of data preparation and data cleaning, this paper describes the application of linear gaussian dynamic Bayesian networks to automated anomaly detection in temperature data streams. Finally, the paper describes two educational activities: (a) a summer institute in ecosystem informatics and (b) an interdisciplinary Ph.D. program in Ecosystem Informatics for mathematics, computer science, and the ecosystem sciences.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Mitchell, Tom M.},
doi = {10.1007/978-3-540-75488-6_2},
eprint = {0-387-31073-8},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mitchell - 2009 - Machine learning(2).pdf:pdf},
isbn = {9781577354260},
issn = {10450823},
pages = {8--13},
pmid = {18292226},
title = {{Machine learning}},
year = {2009}
}
@article{Mishra2014iACT,
abstract = {Approximate computing has recently emerged as a paradigm for enabling energy efficient software and hardware implementations by exploiting the inherent resiliency in applications to impreciseness in their underlying computations. Much of the previous work in this area has demonstrated the potential for significant energy and performance improvements, but these works largely consist of ad hoc techniques that are applied to a small number of similar applications. Mainstream adoption of approximate computing requires a deeper understanding of the inherent application resilience and the codesign hardware to go with the software. This dictates tools and methods that can help programmers reason about the scope and behavior of approximations in applications. To this end, this paper discusses an open source toolkit, called iACT (Intel's Approximate Computing Toolkit) to analyze and study the scope of approximations in applications. Our toolkit consists of a compiler, runtime and a simulated hardware test bed. We discuss the design of this toolkit and present examples of how to use this toolkit in this paper. As an example on how to use this toolkit, we include two different applications and analyze the scope of approximate computing in these.},
annote = {Introduces and explains use of iACT- Intel Approximate Computing Toolkit. Looks like this could be a useful tool for checking approximation results

},
author = {Mishra, Asit K. and Barik, Rajkishore and Paul, Somnath},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mishra, Barik, Paul - 2014 - iACT A Software-Hardware Framework for Understanding the Scope of Approximate Computing(4).pdf:pdf;:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mishra, Barik, Paul - 2014 - iACT A Software-Hardware Framework for Understanding the Scope of Approximate Computing(5).pdf:pdf},
journal = {Wacas},
title = {{iACT: A Software-Hardware Framework for Understanding the Scope of Approximate Computing}},
year = {2014}
}
@misc{Mijat2003,
author = {Mijat, Roberto (ARM)},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mijat - 2003 - Unleashing the benefits of GPU Computing with ARM Mali(2).pdf:pdf},
issn = {14689340},
keywords = {GPU,Mali T6xx},
mendeley-tags = {GPU,Mali T6xx},
number = {1},
pages = {35--38},
title = {{Unleashing the benefits of GPU Computing with ARM Mali}},
volume = {8},
year = {2003}
}
@article{Metz2015,
author = {Metz, J},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Metz - 2015 - Creating Higher Performance Solid State Storage with Non-Volatile Memory Express (NVMe)(2).pdf:pdf},
number = {April},
title = {{Creating Higher Performance Solid State Storage with Non-Volatile Memory Express (NVMe)}},
year = {2015}
}
@article{Metwalli2019,
author = {Metwalli, Sara Ayman and Hara-azumi, Yuko},
file = {:C$\backslash$:/Users/D/Documents/Uni/Papers/a34-metwalli.pdf:pdf},
number = {3},
title = {{SSA-AC : Static Significance Analysis for Approximate}},
volume = {24},
year = {2019}
}
@article{Mcmorrow1827PCBBPTbit,
author = {Mcmorrow, Scott},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mcmorrow - 1827 - PCB Materials for Terabit Backplanes Presented by(2).pdf:pdf},
title = {{PCB Materials for Terabit Backplanes Presented by :}},
year = {1827}
}
@article{Mayer2015DatacentreDemands,
author = {Mayer, Bethany},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mayer - Unknown - Move to the cloud Going Mobile Internet Of things.pdf:pdf},
title = {{Move to the cloud Going Mobile Internet Of things}}
}
@article{Matuska2012,
author = {Matuska, Slavomir and Hudec, Robert and Benco, Miroslav},
doi = {10.1109/ELEKTRO.2012.6225575},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Matuska, Hudec, Benco - 2012 - The comparison of CPU time consumption for image processing algorithm in Matlab and OpenCV(2).pdf:pdf},
isbn = {9781467311793},
journal = {Proc. 9th Int. Conf. ELEKTRO 2012},
keywords = {Matlab,OpenCV,computer vision,mutli-threading},
pages = {75--78},
title = {{The comparison of CPU time consumption for image processing algorithm in Matlab and OpenCV}},
year = {2012}
}
@article{Matharu2015CRAN,
author = {Matharu, Harpinder Singh},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Matharu - 2015 - Motivations for Ethernet Fronthauling in CRAN(2).pdf:pdf},
title = {{Motivations for Ethernet Fronthauling in CRAN}},
year = {2015}
}
@article{Martina,
author = {Martin, Trevor and Specialist, A R M Technical},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Martin, Specialist - Unknown - Public key encryption with mbed TLS(2).pdf:pdf},
title = {{Public key encryption with mbed TLS.}}
}
@article{Martin,
author = {Martin, Trevor and Specialist, A R M Technical},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Martin, Specialist - Unknown - Cryptography and secure communications for microcontrollers with mbed TLS(2).pdf:pdf},
title = {{Cryptography and secure communications for microcontrollers with mbed TLS.}}
}
@article{Martin2008,
author = {Martin, T and Dierks, T. and Rescorla, E.},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Martin, Dierks, Rescorla - 2008 - The Transport Layer Security Protocol.pdf:pdf},
journal = {Internet Eng. Task Force RFC},
title = {{The Transport Layer Security Protocol}},
url = {http://tools.ietf.org/html/rfc5246},
year = {2008}
}
@article{Martin2015,
annote = {next WDM},
author = {Martin, Arlon},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Martin - 2015 - Design Issues Copper and Optical Cables Options for 100GbE and Beyond A new gen of 100G switches is coming !(2).pdf:pdf},
number = {April},
title = {{Design Issues : Copper and Optical Cables Options for 100GbE and Beyond A new gen of 100G switches is coming !}},
year = {2015}
}
@article{MarkoKurtInformationWeek2013,
abstract = {Bits and bytes are the basics of most business, and business is good. So much so that even the immense capacity of advanced data centers can't keep up with demand. That might seem like a contrarian takeaway from the Information Week 2013 State of the Data Center Survey, given that all respondents are involved with management or decision-making at organizations with data centers of 1,000 square feet or larger. In fact the percentage of respondents whose operating centers are at least 25,000 square feet jumped four points from our 2012 survey, to 15{\%}. A growing number of enterprises are using consolidation as an opportunity to reboot their data center strategies, replacing a pile of small, inefficient facilities with two or three megacenters. The standard measure for data center efficiency,the equivalent of an EPA mileage rating for cars, is power-usage effectiveness -- the lower the better.},
annote = {Survey of data centre owners},
author = {{Marko Kurt (Information Week)}},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Marko Kurt (Information Week) - 2013 - Data Centre Decision Time.pdf:pdf},
journal = {Inf. Week},
keywords = {data centre,power usage},
mendeley-tags = {data centre,power usage},
number = {1368},
pages = {9--16},
title = {{Data Centre Decision Time}},
year = {2013}
}
@article{Manners2017,
author = {Manners, David},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Manners - 2017 - Manufacturers told to design in security.pdf:pdf},
title = {{Manufacturers told to design in security}},
year = {2017}
}
@article{Mallela,
author = {Mallela, Chandra},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mallela - Unknown - A Unified Synchronization Solution for Ethernet(2).pdf:pdf},
title = {{A Unified Synchronization Solution for Ethernet}}
}
@inproceedings{Mahajan2016,
abstract = {—Conventionally, an approximate accelerator replaces every invocation of a frequently executed region of code without considering the final quality degradation. However, there is a vast decision space in which each invocation can either be delegated to the accelerator—improving performance and efficiency–or run on the precise core—maintaining quality. In this paper we introduce MITHRA, a co-designed hardware-software solution, that navigates these tradeoffs to deliver high performance and efficiency while lowering the final quality loss. MITHRA seeks to identify whether each individual accelerator invocation will lead to an undesirable quality loss and, if so, directs the processor to run the original precise code. This identification is cast as a binary classification task that requires a cohesive co-design of hardware and software. The hardware component performs the classification at runtime and exposes a knob to the software mechanism to control quality tradeoffs. The software tunes this knob by solving a statistical optimization problem that maximizes benefits from approximation while providing statistical guarantees that final quality level will be met with high confidence. The software uses this knob to tune and train the hardware classifiers. We devise two distinct hardware classifiers, one table-based and one neural network based. To understand the efficacy of these mechanisms, we compare them with an ideal, but infeasible design, the oracle. Results show that, with 95{\%} confidence the table-based design can restrict the final output quality loss to 5{\%} for 90{\%} of unseen input sets while pro-viding 2.5× speedup and 2.6× energy efficiency. The neural design shows similar speedup however, improves the efficiency by 13{\%}. Compared to the table-based design, the oracle improves speedup by 26{\%} and efficiency by 36{\%}. These results show that MITHRA performs within a close range of the oracle and can effectively navigate the quality tradeoffs in approximate acceleration.},
author = {Mahajan, Divya and Yazdanbaksh, Amir and Park, Jongse and Thwaites, Bradley and Esmaeilzadeh, Hadi},
booktitle = {Proc. - 2016 43rd Int. Symp. Comput. Arch. ISCA 2016},
doi = {10.1109/ISCA.2016.16},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mahajan et al. - 2016 - Towards Statistical Guarantees in Controlling Quality Tradeoffs for Approximate Acceleration(2).pdf:pdf},
isbn = {9781467389471},
issn = {1873-2968},
keywords = {Approximate computing,accelerators,quality control,statistical compiler optimization,statistical guarantees},
pmid = {22209716},
title = {{Towards Statistical Guarantees in Controlling Quality Tradeoffs for Approximate Acceleration}},
year = {2016}
}
@article{Macri2015,
author = {Macri, Joe and Officer, Chief Technology},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Macri, Officer - 2015 - AMD ' s Next Generation GPU and High Bandwidth Memory Architecture FURY.pdf:pdf},
number = {August},
title = {{AMD ' s Next Generation GPU and High Bandwidth Memory Architecture : FURY}},
year = {2015}
}
@article{MacMillen2000,
abstract = {The automation of the design of electronic systems and circuits [electronic design automation (EDA)] has a history of strong innovation. The EDA business has profoundly influenced the integrated circuit (IC) business and vice-versa. This paper reviews the technologies, algorithms, and methodologies that have been used in EDA tools and the business impact of these technologies. In particular, we focus on four areas that have been key in defining the design methodologies over time: physical design, simulation/verification, synthesis, and test. We then look briefly into the future. Design will evolve toward more software programmability or some other kind of field configurability like field programmable gate arrays (FPGAs). We discuss the kinds of tool sets needed to support design in this environment},
author = {MacMillen, Don},
doi = {10.1109/43.898825},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/MacMillen - 2000 - An industrial view of electronic design automation.pdf:pdf},
isbn = {0278-0070},
issn = {02780070},
journal = {IEEE Trans. Comput. Des. Integr. Circuits Syst.},
keywords = {Emulation,Formal verification,High-level synthesis,Logic synthesis,Physical synthesis,Placement,RTL synthesis,Routing,Simulation,Simulation acceleration,Testing,Verification},
number = {12},
pages = {1428--1447},
title = {{An industrial view of electronic design automation}},
volume = {19},
year = {2000}
}
@article{Littman2015,
abstract = {Reinforcement learning is a branch of machine learning concerned with using experience gained through interacting with the world and evaluative feedback to improve a system's ability to make behavioural decisions. It has been called the artificial intelligence problem in a microcosm because learning algorithms must act autonomously to perform well and achieve their goals. Partly driven by the increasing availability of rich data, recent years have seen exciting advances in the theory and practice of reinforcement learning, including developments in fundamental technical areas such as generalization, planning, exploration and empirical methodology, leading to increasing applicability to real-life problems.},
author = {Littman, Michael L.},
doi = {10.1038/nature14540},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Littman - 2015 - Reinforcement learning improves behaviour from evaluative feedback.pdf:pdf},
isbn = {0028-0836},
issn = {14764687},
journal = {Nature},
number = {7553},
pages = {445--451},
pmid = {26017443},
title = {{Reinforcement learning improves behaviour from evaluative feedback}},
volume = {521},
year = {2015}
}
@inproceedings{Lin2013,
abstract = {Approximate computing has gained significant attention due to the popularity of multimedia applications. In this paper, we propose a novel inaccurate 4:2 counter that can effectively reduce the partial product stages of the Wallace Multiplier. Compared to the normal Wallace multiplier, our proposed multiplier can reduce 10.74{\%} of power consumption and 9.8{\%} of delay on average, with an error rate from 0.2{\%} to 13.76{\%} The accuracy of amplitude is higher than 99{\%} In addition, we further enhance the design with error-correction units to provide accurate results. The experimental results show that the extra power consumption of correct units is lower than 6{\%} on average. Compared to the normal Wallace multiplier, the average latency of our proposed multiplier with EDC is 6{\%} faster when the bit-width is 32, and the power consumption is still 10{\%} lower than that of the Wallace multiplier.},
author = {Lin, C and Lin, I},
booktitle = {IEEE 31st Int. Conf. Comput. Des. ICCD 2013},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin, Lin - 2013 - High Accuracy Approximate Multiplier With Error Correction.pdf:pdf},
isbn = {9781479929870},
pages = {33--38},
title = {{High Accuracy Approximate Multiplier With Error Correction}},
year = {2013}
}
@article{ARM-PSA2017,
abstract = {This is an introduction to Arm's Platform Security Architecture (PSA) specifications. This paper provides an overview of the evolution of the security technology, with some high level details. For further information, refer to the various specifications from Arm, referenced in this document.},
author = {Limited, Arm and {ARM Limited}},
number = {October},
pages = {1--18},
title = {{Arm {\textregistered}Platform Security Architecture Overview}},
url = {http://pages.arm.com/rs/312-SAX-488/images/PSA-Introductory-Architecture-Overview.pdf?mkt{\_}tok=eyJpIjoiTW1SbVpUWmlORGswWVdNMiIsInQiOiJ4NWpNNk0rbHlRVW1LVWtnS3R2S2tIeE9yWEtpMTVpVUo0SCtwMGVWN0JKWkk3WkFRa2lKM2kyYW5BMGtucU92VDhHNzhhalJSMzE3cFNqc2hCb3dxNW1Eajc2Y},
volume = {1},
year = {2017}
}
@article{ARM-PSA2017,
abstract = {This is an introduction to Arm's Platform Security Architecture (PSA) specifications. This paper provides an overview of the evolution of the security technology, with some high level details. For further information, refer to the various specifications from Arm, referenced in this document.},
author = {Limited, Arm and {ARM Limited}},
file = {:C$\backslash$:/Users/D/Documents/Uni/Papers/PSA-Introductory-Architecture-Overview.pdf:pdf;:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Limited, ARM Limited - 2017 - Arm {\textregistered} Platform Security Architecture Overview.pdf:pdf},
number = {October},
pages = {1--18},
title = {{Arm {\textregistered} Platform Security Architecture Overview}},
url = {http://pages.arm.com/rs/312-SAX-488/images/PSA-Introductory-Architecture-Overview.pdf?mkt{\_}tok=eyJpIjoiTW1SbVpUWmlORGswWVdNMiIsInQiOiJ4NWpNNk0rbHlRVW1LVWtnS3R2S2tIeE9yWEtpMTVpVUo0SCtwMGVWN0JKWkk3WkFRa2lKM2kyYW5BMGtucU92VDhHNzhhalJSMzE3cFNqc2hCb3dxNW1Eajc2Y},
volume = {1},
year = {2017}
}
@incollection{Limited2013,
annote = {read up to 2.2},
author = {Limited, ARM R M},
file = {:C$\backslash$:/Users/D/Documents/Uni/Papers/DEN0018A{\_}neon{\_}programmers{\_}guide.pdf:pdf},
isbn = {9781439806104},
keywords = {NEON},
title = {{NEON Programmer's Guide}},
year = {2013}
}
@techreport{Limited2018,
author = {Limited, ARM},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Limited - 2018 - Running AlexNet on Raspberry Pi with Compute Library 1 Overview.pdf:pdf},
keywords = {alexnet,artificial intelligence,compute library,machine learning,neural network,raspberry pi},
pages = {1--10},
title = {{Running AlexNet on Raspberry Pi with Compute Library 1 Overview}},
year = {2018}
}
@article{Licciardo2017,
author = {Licciardo, Gian Domenico and Cappetta, Carmine and DiBenedetto, Luigi},
doi = {10.3390/computers6020019},
file = {:C$\backslash$:/Users/D/Documents/Uni/Papers/71647fee00901841ddb707ce383be9a9091c.pdf:pdf},
journal = {Computers},
keywords = {2d filtering modules,gaussian filtering,multiplier,visual search},
number = {2},
pages = {19},
title = {{Design of a Convolutional Two-Dimensional Filter in FPGA for Image Processing Applications}},
volume = {6},
year = {2017}
}
@article{Li2014ToF,
abstract = {3D Time-of-Flight (TOF) technology is revolutionizing the machine vision industry by providing 3D imaging using a low-cost CMOS pixel array together with an active modulated light source. Compact construction, easy-of-use, together with high accuracy and frame-rate makes TOF cameras an attractive solution for a wide range of applications. In this article, we will cover the basics of TOF operation, and compare TOF with other 2D/3D vision technologies. Then various applications that benefit from TOF sensing, such as gesturing and 3D scanning and printing, are explored. Finally, resources that help readers get started with Texas Instruments' 3D TOF solution are provided.},
author = {Li, Larry},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li - 2014 - Time-of-Flight Camera–An Introduction.pdf:pdf},
journal = {Larray Li},
number = {January},
title = {{Time-of-Flight Camera–An Introduction}},
url = {http://www.ti.com/lit/wp/sloa190b/sloa190b.pdf{\%}5Cnhttp://www.tij.co.jp/jp/lit/wp/sloa190/sloa190.pdf},
year = {2014}
}
@article{Li2006,
abstract = {Previous proposals for power-aware thread-level parallelism on chip multiprocessors (CMPs) mostly focus on multiprogrammed workloads. Nonetheless, parallel computation of a single application is critical in light of the expanding performance demands of important future workloads. This work addresses the problem of dynamically optimizing power consumption of a parallel application that executes on a many-core CMP under a given performance constraint. The optimization space is two-dimensional, allowing changes in the number of active processors and applying dynamic voltage/frequency scaling. We demonstrate that the particular optimum operating point depends nontrivially on the power-performance characteristics of the CMP, the application's behavior, and the particular performance target. We present simple, low-overhead heuristics for dynamic optimization that significantly cut down on the search effort along both dimensions of the optimization space. In our evaluation of several parallel applications with different performance targets, these heuristics quickly lock on a configuration that yields optimal power savings in virtually all cases.},
author = {Li, Jian and Martinez, Jose F. and Mart, F and Li, Jian and Martinez, Jose F. and Mart, F and Li, Jian and Martinez, Jose F. and Mart, F},
doi = {10.1109/HPCA.2006.1598114},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2006 - Dynamic power-performance adaptation of parallel computation on chip multiprocessors(2).pdf:pdf},
isbn = {0780393686},
issn = {15300897},
journal = {Proc. - Int. Symp. High-Performance Comput. Archit.},
pages = {77--87},
title = {{Dynamic power-performance adaptation of parallel computation on chip multiprocessors}},
volume = {2006},
year = {2006}
}
@article{Li2015,
abstract = {Approximate computing has been recognized as an effective low power technique for applications with intrinsic error tolerance, such as image processing and machine learning. Existing efforts on this front are mostly focused on approximate circuit design, approximate logic synthesis or processor architecture approximation techniques. This work aims at how to make good use of approximate circuits at system and block level. In particular, approximation aware scheduling, functional unit allocation and binding algorithms are developed for data intensive applications. Simple yet credible error models, which are essential for precision control in the optimizations, are investigated. The algorithms are further extended to include bitwidth optimization in fixed point computations. Experimental results, including those from Verilog simulations, indicate that the proposed techniques facilitate desired energy savings under latency and accuracy constraints.},
author = {Li, Chaofan and Luo, Wei and Sapatnekar, S S and Hu, Jiang},
doi = {10.1145/2744769.2744863},
isbn = {9781450335201},
issn = {0738100X},
journal = {Des. Autom. Conf. (DAC), 2015 52nd ACM/EDAC/IEEE},
title = {{Joint precision optimization and high level synthesis for approximate computing}},
year = {2015}
}
@article{Leys2013,
abstract = {A survey revealed that researchers still seem to encounter difficulties to cope with outliers. Detecting outliers by determining an interval spanning over the mean plus/minus three standard deviations remains a common practice. However, since both the mean and the standard deviation are particularly sensitive to outliers, this method is problematic. We highlight the disadvantages of this method and present the median absolute deviation, an alternative and more robust measure of dispersion that is easy to implement. We also explain the procedures for calculating this indicator in SPSS and R software. ?? 2013 Elsevier Inc.},
author = {Leys, Christophe and Ley, Christophe and Klein, Olivier and Bernard, Philippe and Licata, Laurent},
doi = {10.1016/j.jesp.2013.03.013},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Leys et al. - 2013 - Detecting outliers Do not use standard deviation around the mean, use absolute deviation around the median.pdf:pdf},
isbn = {0022-1031},
issn = {00221031},
journal = {J. Exp. Soc. Psychol.},
keywords = {MAD,Median absolute deviation,Outlier},
number = {4},
pages = {764--766},
pmid = {20228874},
publisher = {Elsevier Inc.},
title = {{Detecting outliers: Do not use standard deviation around the mean, use absolute deviation around the median}},
url = {http://dx.doi.org/10.1016/j.jesp.2013.03.013},
volume = {49},
year = {2013}
}
@article{Lee2008,
author = {Lee, E. A.},
doi = {10.1109/ISORC.2008.25},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee - 2008 - Cyber Physical Systems Design Challenges.pdf:pdf},
isbn = {978-0-7695-3132-8},
journal = {11th IEEE Int. Symp. Object Component-Oriented Real-Time Distrib. Comput.},
keywords = {Cyber-physical systems, real time, embedded system},
pages = {363--369},
title = {{Cyber Physical Systems: Design Challenges}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4519604},
year = {2008}
}
@article{Lawlor2003,
abstract = {We present a high performance GPU programming language, based on OpenCL, that is embedded in C++. Our embedding provides shared data structures, typesafe kernel invocation, and the ability to more naturally interleave CPU and GPU functions, similar to CUDA but with the portability of OpenCL. For expressivity, our language provides the FILL abstraction that releases control over data writes to the runtime system, which both improves expres- sivity and eliminates the chance of memory race conditions. We benchmark our new language EPGPU on NVIDIA and AMD hard- ware for several small examples. 1.},
author = {Lawlor, Orion Sky},
file = {:C$\backslash$:/Users/D/Documents/Uni/Papers/lawlor{\_}epgpu{\_}2011.pdf:pdf},
title = {{Embedding OpenCL in C ++ for Expressive GPU Programming}},
year = {2003}
}
@article{Lavia,
author = {Lavia, Anthony Tony},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lavia - Unknown - Ethernet and the IoT Pre-Conference Seminar Session IoT-11 An Introduction(2).pdf:pdf},
title = {{Ethernet and the IoT Pre-Conference Seminar Session IoT-11 : An Introduction}}
}
@article{Lai2018,
abstract = {Deep Neural Networks are becoming increasingly popular in always-on IoT edge devices performing data analytics right at the source, reducing latency as well as energy consumption for data communication. This paper presents CMSIS-NN, efficient kernels developed to maximize the performance and minimize the memory footprint of neural network (NN) applications on Arm Cortex-M processors targeted for intelligent IoT edge devices. Neural network inference based on CMSIS-NN kernels achieves 4.6X improvement in runtime/throughput and 4.9X improvement in energy efficiency.},
archivePrefix = {arXiv},
arxivId = {1801.06601},
author = {Lai, Liangzhen and Suda, Naveen and Chandra, Vikas},
eprint = {1801.06601},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lai, Suda, Chandra - 2018 - CMSIS-NN Efficient Neural Network Kernels for Arm Cortex-M CPUs.pdf:pdf},
pages = {1--10},
title = {{CMSIS-NN: Efficient Neural Network Kernels for Arm Cortex-M CPUs}},
url = {http://arxiv.org/abs/1801.06601},
year = {2018}
}
@article{Kumar2015,
author = {Kumar, Anil},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kumar - 2015 - Scaling Networking Solutions for IoT Challenges and Opportunities(2).pdf:pdf},
number = {April},
title = {{Scaling Networking Solutions for IoT Challenges and Opportunities}},
year = {2015}
}
@article{Krishnamurthy2015IoT,
author = {Krishnamurthy, Subi},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Krishnamurthy - 2015 - IoT(2).pdf:pdf},
title = {{IoT}},
year = {2015}
}
@article{Kockanat2015,
abstract = {Recently, two dimensional (2D) adaptive filter, which can self-adjust the filter coefficients by using an optimization algorithm driven by an error function, has attracted much attention by researchers and practitioners, because 2D adaptive filtering can be employed in many image processing applications, such as image denoising, enhancement and deconvolution. In this paper, a novel 2D artificial bee colony (2D-ABC) adaptive filter algorithm was firstly proposed and to the best of our knowledge, there is no study describing 2D adaptive filter algorithm based on metaheuristic algorithms in the literature. At the first stage, in order to analyze the performance and computational efficiency of the novel 2D-ABC adaptive filter algorithm, it was used in the 2D adaptive noise cancellation (ANC) as recommend in literature. For a fair comparison, the competitor 2D adaptive filter algorithms were applied to the same 2D-ANC setup under same condition, such as same Gaussian noise, same filter order or same test images. The results of the novel 2D-ABC adaptive filter algorithm were compared with those of the 2D affine projection algorithms (APA), 2D normalized least mean square (NLMS) and 2D least mean square (LMS) adaptive filter algorithms. At the second stage, to demonstrate the robustness of the novel 2D-ABC adaptive filter algorithm, it was implemented for speckle noise filtering on noisy clinical ultrasound images. The results show that the novel 2D-ABC adaptive filter algorithm has a better performance than the other classical adaptive filter algorithms and its denoising efficiency is quite well on noisy images with different characteristics.},
annote = {Discusses an Adaptive 2D filter used in an iterative process, Artificial Bee Colony (ABC) algorithm, to extract image data from noisy backgrounds. aimed at biomedical applications and is therefore a high computational application.},
author = {Kockanat, Serdar and Karaboga, Nurhan},
doi = {10.1016/j.dsp.2015.02.010},
file = {:C$\backslash$:/Users/D/Documents/Uni/Papers/2.2D-ABC.pdf:pdf},
issn = {10512004},
journal = {DSP},
keywords = {2D FIR digital filter,Adaptive filter algorithm,Artificial bee colony algorithm,Image denoising,Optimization},
pages = {140--153},
pmid = {2666277},
publisher = {Elsevier Inc.},
title = {{A novel 2D-ABC adaptive filter algorithm: A comparative study}},
url = {http://dx.doi.org/10.1016/j.dsp.2015.02.010},
volume = {40},
year = {2015}
}
@incollection{Knight,
author = {Knight, Steven},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Knight - Unknown - SCons 2.4.1 User Guide.pdf:pdf},
title = {{SCons 2.4.1 User Guide}}
}
@article{Kim2018,
author = {Kim, Sunghyun and Kim, Youngmin},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Kim - 2018 - Novel XNOR-based Approximate Computing for Energy-efficient Image Processors(2).pdf:pdf},
number = {5},
pages = {602--608},
title = {{Novel XNOR-based Approximate Computing for Energy-efficient Image Processors}},
volume = {18},
year = {2018}
}
@article{Kim2007,
abstract = {Power-aware scheduling problem has been a recent issue in cluster systems not only for operational cost due to electricity cost, but also for system reliability. As recent commodity processors support multiple operating points under various supply voltage levels, Dynamic Voltage Scaling (DVS) scheduling algorithms can reduce power consumption by controlling appropriate voltage levels. In this paper, we provide power-aware scheduling algorithms for bag-of-tasks applications with deadline constraints on DVS-enabled cluster systems in order to minimize power consumption as well as to meet the deadlines specified by application users. A bag-of-tasks application should finish all the sub-tasks before the deadline, so that the DVS scheduling scheme should consider the deadline as well. We provide the DVS scheduling algorithms for both time-shared and space-shared resource sharing policies. The simulation results show that the proposed algorithms reduce much power consumption compared to static voltage schemes.},
author = {Kim, Kyong Hoon and Buyya, Rajkumar and Kim, Jong},
doi = {10.1109/CCGRID.2007.85},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Buyya, Kim - 2007 - Power aware scheduling of bag-of-tasks applications with deadline constraints on DVS-enabled clusters.pdf:pdf},
isbn = {0769528333},
journal = {Proc. - Seventh IEEE Int. Symp. Clust. Comput. Grid, CCGrid 2007},
pages = {541--548},
title = {{Power aware scheduling of bag-of-tasks applications with deadline constraints on DVS-enabled clusters}},
year = {2007}
}
@article{Khudia2015Rumba,
abstract = {Approximate computing can be employed for an emerging class of applications from various domains such as multimedia, machine learning and computer vision. The approximated output of such applications, even though not 100{\%} numerically correct, is often either useful or the difference is unnoticeable to the end user. This opens up a new design dimension to trade off application performance and energy consumption with output correctness. However, a largely unaddressed challenge is quality control: how to ensure the user experience meets a prescribed level of quality. Current approaches either do not monitor output quality or use sampling approaches to check a small subset of the output assuming that it is representative. While these approaches have been shown to produce average errors that are acceptable, they often miss large errors without any means to take corrective actions. To overcome this challenge, we propose Rumba for online detection and correction of large approximation errors in an approximate accelerator-based computing environment. Rumba employs continuous lightweight checks in the accelerator to detect large approximation errors and then fixes these errors by exact re-computation on the host processor. Rumba employs computationally inexpensive output error prediction models for efficient detection. Computing patterns amenable for approximation (e.g., map and stencil) are usually data parallel in nature and Rumba exploits this property for selective correction. Overall, Rumba is able to achieve 2.1x reduction in output error for an unchecked approximation accelerator while maintaining the accelerator performance gains at the cost of reducing the energy savings from 3.2x to 2.2x for a set of applications from different approximate computing domains.},
annote = {Approximate computing can be employed for an emerging class of applications from various domains such as multimedia, machine learning and computer vision. The approximated output of such applications, even though not 100{\%} numerically correct, is often either useful or the difference is unnoticeable to the end user.

Seems to be a rather complicated, perhaps advanced, error checking network for approximation. Perhaps more suited to application to application as a debugging tool on an a new process that may require debugging for large errors!},
author = {Khudia, Daya S and Zamirai, Babak and Samadi, Mehrzad and Mahlke, Scott},
doi = {10.1145/2749469.2750371},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Khudia et al. - 2015 - Rumba An Online Quality Management System for Approximate Computing(2).pdf:pdf},
isbn = {9781450334020},
issn = {10636897},
journal = {Proc. 42nd Annu. Int. Symp. Comput. Archit. - ISCA '15},
pages = {554--566},
title = {{Rumba An Online Quality Management System for Approximate Computing}},
url = {http://doi.acm.org/10.1145/2749469.2750371{\%}5Cnhttp://dl.acm.org/citation.cfm?doid=2749469.2750371},
year = {2015}
}
@article{Khan2012,
abstract = {This paper focuses on the implementation of a DVFS based Dynamic Stretch$\backslash$nto Fit power strategy on a ARM based platform. The actual execution$\backslash$ntime (AET) of tasks within an application, implemented on a processor$\backslash$nis always less or equal to the worst execution time (WCET). The time$\backslash$nslack produced by AET and WCET of one task can be utilized by the$\backslash$nother tasks for their execution. We take advantage of this slack$\backslash$nin our DSF power strategy to decrease processors frequency for the$\backslash$nremaining tasks during their execution. We analyze Energy gains by$\backslash$nimplementing our DVFS based DSF power strategy on different ARM based$\backslash$nprocessors (ARM1176JZF-S, CortexA9) and our results showed significant$\backslash$namount of gains up to 52{\%} for different execution conditions.},
author = {Khan, Jabran and Bilavarn, Sebastien and Belleudy, C{\'{e}}cile},
doi = {10.1109/FTFC.2012.6231734},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Khan, Bilavarn, Belleudy - 2012 - Energy analysis of a DVFS based power strategy on ARM platforms(2).pdf:pdf},
isbn = {9781467308205},
journal = {2012 IEEE Faibl. Tens. Faibl. Consomm. FTFC 2012},
pages = {2--5},
title = {{Energy analysis of a DVFS based power strategy on ARM platforms}},
year = {2012}
}
@article{Karakonstantis2011,
abstract = {In this paper, we propose a design paradigm for energy efficient and variation-aware operation of next-generation multicore heterogeneous platforms. The main idea behind the proposed approach lies on the observation that not all operations are equally important in shaping the output quality of various applications and of the overall system. Based on such an observation, we suggest that all levels of the software design stack, including the programming model, compiler, operating system (OS) and run- time system should identify the critical tasks and ensure correct operation of such tasks by assigning them to dynamically adjusted reliable cores/units. Specifically, based on error rates and operating conditions identified by a sense-and-adapt (SeA) unit, the OS selects and sets the right mode of operation of the overall system. The run-time system identifies the critical/less-critical tasks based on special directives and schedules them to the appropriate units that are dynamically adjusted for highly-accurate/approximate operation by tuning their voltage/frequency. Units that execute less significant operations can operate at voltages less than what is required for correct operation and consume less power, if required, since such tasks do not need to be always exact as opposed to the critical ones. Such scheme can lead to energy efficient and reliable operation, while reducing the design cost and overheads of conventional circuit/micro-architecture level techniques. Categories,},
author = {Karakonstantis, Georgios and Bellas, Nikolaos and Antonopoulos, Christos and Tziantzioulis, Georgios and Gupta, Vaibhav and Roy, Kaushik},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karakonstantis et al. - 2011 - Significance Driven Computation on Next-Generation Unreliable Platforms.pdf:pdf},
isbn = {9781450306362},
journal = {DAC},
keywords = {approximate computing,energy efficient,software},
pages = {290--291},
title = {{Significance Driven Computation on Next-Generation Unreliable Platforms}},
year = {2011}
}
@book{Kaehler2016,
author = {Kaehler, Adrian and Bradski, Gary},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaehler, Bradski - 2016 - Learning OpenCV 3.pdf:pdf},
isbn = {9781491937990},
title = {{Learning OpenCV 3}},
year = {2016}
}
@book{Joshi2016,
author = {Joshi, Prateek},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Joshi - 2016 - OpenCV with Python By Example.pdf:pdf},
isbn = {9781785283932},
title = {{OpenCV with Python By Example}},
year = {2016}
}
@article{Jordan2015,
abstract = {Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today's most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Jordan, M. I. and Mitchell, T. M.},
doi = {10.1126/science.aaa8415},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jordan, Mitchell - 2015 - Machine learning Trends, perspectives, and prospects(2).pdf:pdf},
isbn = {0036-8075, 0036-8075},
issn = {0036-8075},
journal = {Sci. (80-. ).},
keywords = {classifier,dt,supervised machine learning,svm},
number = {6245},
pages = {255--260},
pmid = {26185243},
title = {{Machine learning: Trends, perspectives, and prospects}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.aaa8415},
volume = {349},
year = {2015}
}
@article{Jo2014,
abstract = {The state-of-the-art ARM processors provide multiple cores and SIMD instructions. OpenCL is a promising programming model for utilizing such parallel processing capability because of its SPMD programming model and built-in vector support. Moreover, it provides portability between multicore ARM processors and accelerators in embedded systems. In this paper, we introduce the design and implementation of an efficient OpenCL framework for multicore ARM processors. Computational tasks in a program are implemented as OpenCL kernels and run on all CPU cores in parallel by our OpenCL framework. Vector operations and built-in functions in OpenCL kernels are optimized using the NEON SIMD instruction set. We evaluate our OpenCL framework using 37 benchmark applications. The result shows that our approach is effective and promising.},
author = {Jo, Gangwon and Taft, Gordon and Jung, Wookeun and Lee, Jaejin and Jeon, Won Jong},
doi = {10.1145/2568058.2568064},
file = {:C$\backslash$:/Users/D/Documents/Uni/Papers/wpmvp2014.pdf:pdf},
isbn = {9781450326537},
keywords = {embedded system,multicore,opencl,simd},
pages = {33--40},
title = {{OpenCL framework for ARM processors with NEON support}},
year = {2014}
}
@inproceedings{Jiang2016,
abstract = {—A multiplier has a significant impact on the speed and power dissipation of an arithmetic processor. Precise results are not always required in many algorithms, such as those for classification and recognition in data processing. Moreover, many errors do not make an obvious difference in applications such as image processing due to the perceptual limitations of human beings. Error-tolerant algorithms and applications have promoted the development of approximate multipliers to tradeoff accuracy for speed, implementation area and/or power efficiency. This paper briefly reviews the current designs of approximate multipliers and provides a comparative evaluation of their error and circuit characteristics. Image sharpening is performed using the considered approximate multipliers to assess their performance in such applications.},
author = {Jiang, Honglan and Liu, Cong and Maheshwari, Naman and Lombardi, Fabrizio and Han, Jie},
doi = {10.1145/2950067.2950068},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang et al. - 2016 - A Comparative Evaluation of Approximate Multipliers.pdf:pdf},
isbn = {9781450343305},
keywords = {Accuracy,Approximate computing,Multiplier},
pages = {191--196},
title = {{A Comparative Evaluation of Approximate Multipliers}},
year = {2016}
}
@article{Ji2011,
abstract = {TCP/IP stack is the workhorse protocol of the Internet. The processing of the TCP/IP over Ethernet is traditionally accomplished by software running on the central processor, CPU or microprocessor. As the speed of Ethernet grows from 10Mbits/s to 10Gbits/s, the CPU can't afford the large amount of TCP/IP protocol processing required. So the TCP/IP protocol processor becomes an efficient and a must method to offload CPU burdens. This paper proposes a TCP/IP Internet protocol processing system architecture, which is to implement transport layer, network layer and data link layer, and support configurable number of connections. The design is implemented on Altera StratixIV FPGA. The performance estimation shows that our system can provide high-speed TCP/IP transmission rate up to 4Gbps as receiver and 40Gbps as sender.},
author = {Ji, Yong and Hu, Qing Sheng},
doi = {10.1109/WCSP.2011.6096913},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ji, Hu - 2011 - 40Gbps multi-connection TCPIP offload engine.pdf:pdf},
isbn = {9781457710100},
journal = {2011 Int. Conf. Wirel. Commun. Signal Process. WCSP 2011},
keywords = {FPGA,Multi connections,TCP/IP offload engine},
pages = {1--5},
publisher = {IEEE},
title = {{40Gbps multi-connection TCP/IP offload engine}},
year = {2011}
}
@article{Jayakumar2016,
abstract = {It is projected that, within the coming decade, there will be more than 50 billion smart objects connected to the Internet of Things (IoT). These smart objects, which connect the physical world with the world of computing infrastructure, are expected to pervade all aspects of our daily lives and revolutionize a number of application domains such as healthcare, energy conservation, transportation, etc. In this paper, we present an overview of the challenges involved in designing energy-efficient IoT edge devices and describe recent research that has proposed promising solutions to address these challenges. First, we outline the challenges involved in efficiently supplying power to an IoT device. Next, we discuss the role of emerging memory technologies in making IoT devices energy-efficient. Finally, we discuss the potential impact that approximate computing can have in increasing the energy-efficiency of wearables and other compute-intensive IoT devices.},
author = {Jayakumar, Hrishikesh and Raha, Arnab and Kim, Younghyun and Sutar, Soubhagya and Lee, Woo Suk and Raghunathan, Vijay},
doi = {10.1109/ASPDAC.2016.7428027},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jayakumar et al. - 2016 - Energy-efficient system design for IoT devices.pdf:pdf},
isbn = {9781467395694},
journal = {Proc. Asia South Pacific Des. Autom. Conf. ASP-DAC},
pages = {298--301},
title = {{Energy-efficient system design for IoT devices}},
volume = {25-28-Janu},
year = {2016}
}
@inproceedings{Intilop2015,
annote = {All about Ethernet acceleration by FPGA},
author = {Intilop},
booktitle = {Ethernet Technol. summit},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Intilop - 2015 - Full TCP {\&} UDP Offload Engines with 1 - 16K Sessions Network Traffic and Application Acceleration in High Performanc(2).pdf:pdf},
keywords = {TCP,UDP,accellerator,fPGA},
mendeley-tags = {TCP,UDP,accellerator,fPGA},
number = {April},
pages = {1--19},
title = {{Full TCP {\&} UDP Offload Engines with 1 - 16K Sessions Network Traffic and Application Acceleration in High Performance Network Servers and Storage Solutions}},
year = {2015}
}
@misc{Instruments2013,
author = {Instruments, Texas},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Instruments - 2013 - INA231EVM Evaluation Board and Software Tutorial.pdf:pdf},
number = {February},
title = {{INA231EVM Evaluation Board and Software Tutorial}},
year = {2013}
}
@article{Imani,
author = {Imani, Mohsen and Peroni, Daniel and Rosing, Tajana},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Imani, Peroni, Rosing - Unknown - CFPU Configurable Floating Point Multiplier for Energy-Efficient Computing.pdf:pdf},
isbn = {9781450349277},
keywords = {2016,acm reference format,approximate computing,cfpu,configurable,daniel peroni and tajana,energy efficiency,floating point unit,mohsen imani,rosing,•Hardware  Reconfigurable logic applications,•Theory of computation  Stochastic approximation},
title = {{CFPU : Configurable Floating Point Multiplier for Energy-Efficient Computing}}
}
@article{OV23850,
author = {Imaging, Advanced Digital},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Imaging - Unknown - DSC-Like Performance and Advanced Digital Imaging Features for Next-Generation Flagship Smartphones OV23850.pdf:pdf},
title = {{DSC-Like Performance and Advanced Digital Imaging Features for Next-Generation Flagship Smartphones OV23850}}
}
@article{Ilyadis2015,
author = {Ilyadis, Nick and Corporation, Broadcom and Group, Networking},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ilyadis, Corporation, Group - 2015 - Understanding the True Cost of SDN It Starts with the Silicon {\ldots}. Nick Ilyadis Broadcom Corporati(2).pdf:pdf},
number = {April},
title = {{Understanding the True Cost of SDN : It Starts with the Silicon {\ldots}. Nick Ilyadis Broadcom Corporation VP / CTO of Infrastructure and Networking Group}},
year = {2015}
}
@article{Ii2011,
author = {Ii, For Quartus and Program, Altera Monitor and Ii, Nios and Program, Monitor and Program, The Monitor and Windows, Microsoft and Uart, Jtag and Ii, Develop Nios and Ab-, Hardware and De, Altera and Ii, Quartus and Design, Embedded},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ii et al. - 2011 - Altera Monitor Program Tutorial Installing the Monitor Program.pdf:pdf},
journal = {Program},
number = {May},
pages = {1--39},
title = {{Altera Monitor Program Tutorial Installing the Monitor Program}},
year = {2011}
}
@inproceedings{Hughes2015SeagateKinetic,
author = {Hughes, James},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hughes - Unknown - Seagate Kinetic Open Storage Platform(2).pdf:pdf},
keywords = {Storage capacity evolution},
mendeley-tags = {Storage capacity evolution},
title = {{Seagate Kinetic Open Storage Platform}}
}
@article{Huang,
author = {Huang, Dave and Engineer, Staff Solutions and Systems, Cadence Design},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang, Engineer, Systems - Unknown - Overcoming Ethernet SoC Verification Challenges(2).pdf:pdf},
title = {{Overcoming Ethernet SoC Verification Challenges}}
}
@article{Hu2017,
abstract = {To overcome the high computing cost associated with high-dimensional digital image descriptor matching, this paper presents a massively parallel approximate nearest neighbor search (ANNS) on K-dimensional tree (KD-tree) on the modern massively parallel architectures (MPA). The proposed algorithm is of comparable quality to traditional sequential counterpart on central processing unit (CPU). However, it achieves a high speedup factor of 121 when applied to high-dimensional real-world image descriptor datasets. The algorithm is also studied for factors that impact its performance to obtain the optimal runtime configurations for various datasets. The performance of the proposed parallel ANNS algorithm is also verified on typical 3D image matching scenarios. With the classical local image descriptor signature of histograms of orientations (SHOT), the parallel image descriptor matching can achieve speedup of up to 128. Our implementation will potentially benefit realtime image descriptor matching in high dimensions.},
author = {Hu, Linjia and Nooshabadi, Saeid},
doi = {10.1016/j.jvcir.2017.01.013},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hu, Nooshabadi - 2017 - Massive parallelization of approximate nearest neighbor search on KD-tree for high-dimensional image descriptor.pdf:pdf},
issn = {10959076},
journal = {J. Vis. Commun. Image Represent.},
keywords = {Approximate nearest neighbor search,CUDA,GPU,Image descriptor matching,KD-tree,Parallel algorithm},
pages = {106--115},
publisher = {Elsevier Inc.},
title = {{Massive parallelization of approximate nearest neighbor search on KD-tree for high-dimensional image descriptor matching}},
url = {http://dx.doi.org/10.1016/j.jvcir.2017.01.013},
volume = {44},
year = {2017}
}
@article{Houssein2015,
author = {Houssein, Ahmet and President, Senior Vice and Unit, Ethernet Business},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Houssein, President, Unit - 2015 - Redefining the Data Center Edge(2).pdf:pdf},
pages = {1--36},
title = {{Redefining the Data Center Edge}},
year = {2015}
}
@inproceedings{Hou2007,
address = {Minneapolis, MN, USA},
author = {Hou, Xiaodi and Zhang, Liqing},
booktitle = {IEEE Conf. Comput. Vis. Pattern Recognit.},
doi = {10.1109/CVPR.2007.383267},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hou, Zhang - 2007 - Saliency Detection A Spectral Residual Approach Xiaodi.pdf:pdf},
isbn = {1424411807},
keywords = {Computational modeling,Humans,Image analysis,Image coding,Machine vision,Object detection,Object recognition,Redundancy,Statistical distributions,Visual system},
publisher = {IEEE},
title = {{Saliency Detection: A Spectral Residual Approach}},
year = {2007}
}
@article{Horner2015-40-100G,
annote = {shows costs involved in SoC},
author = {Horner, Rita},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Horner - Unknown - Enabling 40G 100G SoC Designs Addressing the Challenges of Design(2).pdf:pdf},
keywords = {soc cost},
mendeley-tags = {soc cost},
title = {{Enabling 40G / 100G SoC Designs Addressing the Challenges of Design}}
}
@article{Holmquist2015-100GInterop,
author = {Holmquist, Neil},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Holmquist - 2015 - Verifying Interoperability of 100G Optical Technologies(2).pdf:pdf},
number = {April},
pages = {1--7},
title = {{Verifying Interoperability of 100G Optical Technologies}},
year = {2015}
}
@article{hoffman2011,
abstract = {We present PowerDial, a system for dynamically adapting applica- tion behavior to execute successfully in the face of load and power fluctuations. PowerDial transforms static configuration parameters into dynamic knobs that the PowerDial control system can manip- ulate to dynamically trade off the accuracy of the computation in return for reductions in the computational resources that the appli- cation requires to produce its results. These reductions translate directly into performance improvements and power savings. Our experimental results show that PowerDial can enable our benchmark applications to execute responsively in the face ofpower caps that would otherwise significantly impair responsiveness. They also show that PowerDial can significantly reduce the number of machines required to service intermittent load spikes, enabling re- ductions in power and capital costs},
annote = {Power balancing tool for Data centres, mixed strategies for servers with low or high idle power in order to optimise power performance and latency.},
author = {Hoffmann, Henry and Sidiroglou, Stelios and Misailovic, Michael Carbin Sasa and Agarwal, Anant and Rinard, Martin},
doi = {10.1111/j.1365-2478.1977.tb01199.x},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoffmann et al. - 2011 - Dynamic Knobs for Responsive Power-Aware Computing.pdf:pdf},
isbn = {9781450302661},
issn = {13652478},
journal = {ASPLOS'11,},
keywords = {accuracy-aware computing,power-aware computing,self-aware},
title = {{Dynamic Knobs for Responsive Power-Aware Computing}},
year = {2011}
}
@article{Hengstler2007,
abstract = {Surveillance is one of the promising applications to which smart camera motes forming a vision-enabled network can add increasing levels of intelligence. We see a high degree of in-node processing in combination with distributed reasoning algorithms as the key enablers for such intelligent surveillance systems. To put these systems into practice still requires a considerable amount of research ranging from mote architectures, pixel-processing algorithms, up to distributed reasoning engines. This paper introduces MeshEye, an energy-efficient smart camera mote architecture that has been designed with intelligent surveillance as the target application in mind. Special attention is given to MeshEye's unique vision system: a low-resolution stereo vision system continuously determines position, range, and size of moving objects entering its field of view. This information triggers a color camera module to acquire a high-resolution image sub-array containing the object, which can be efficiently processed in subsequent stages. It offers reduced complexity, response time, and power consumption over conventional solutions. Basic vision algorithms for object detection, acquisition, and tracking are described and illustrated on real- world data. The paper also presents a basic power model that estimates lifetime of our smart camera mote in battery-powered operation for intelligent surveillance event processing.},
annote = {Uses multiple cameras to detect motion with low res detect and apply to high res. as per my initial thoughts. Quite intensive bit of processing. Showing it's age with VGA camera as the main viewer and sub sampled kilopixel stereo cameras for feature extraction.},
author = {Hengstler, Stephan and Prashanth, Daniel and Fong, Sufen and Aghajan, Hamid},
doi = {10.1109/IPSN.2007.4379696},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hengstler et al. - 2007 - MeshEye A Hybrid-Resolution Smart Camera Mote for Applications in Distributed Intelligent Surveillance.pdf:pdf},
isbn = {978-1-59593-638-7},
journal = {2007 6th Int. Symp. Inf. Process. Sens. Networks},
pages = {360--369},
title = {{MeshEye: A Hybrid-Resolution Smart Camera Mote for Applications in Distributed Intelligent Surveillance}},
url = {http://ieeexplore.ieee.org/document/4379696/},
year = {2007}
}
@article{Hemsoth,
author = {Hemsoth, Nicole and {Prickett Morgan}, Timothy},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hemsoth, Prickett Morgan - Unknown - FPGA FRONTIERS(3).pdf:pdf},
title = {{FPGA FRONTIERS :}}
}
@article{Hartmann2016,
abstract = {The design of a complex power supply infrastructure and the satisfaction of the concentrated high energy demand is a constant challenge for data centre owners. Furthermore, economics of the operation and ideas of sustainability are getting more and more attention lately as well. Green data centres are designed to minimize the effect on the natural environment. One of the primary goals of such units is to decrease the energy needs both for the computing infrastructure and the supporting systems (e.g. thermal management), which can be achieved either by increasing the efficiency of processes or by integrating renewable energy sources. Present paper introduces results of an ongoing research and development cooperation between the Centre for Energy Research of the Hungarian Academy of Sciences and Persecutor Ltd., supported by the Hungarian Government's “PIAC{\_}13” programme. The project aims to develop a new, highly energy efficient data centre infrastructure, which is able to supply IT needs at a high-level, while minimizing power losses. Present paper focuses on decreasing power losses in the distribution and power conversion infrastructure. A power loss model, capable of supporting the optimal selection of different subsystems of the technological chain, is introduced and two case studies are shown to demonstrate the effect of different loss reduction options.},
author = {Hartmann, B{\'{a}}lint and Farkas, Csaba},
doi = {10.1016/j.enbuild.2016.06.037},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hartmann, Farkas - 2016 - Energy Efficient Data Centre Infrastructure − Development of a Power Loss Model.pdf:pdf},
issn = {03787788},
journal = {Energy Build.},
keywords = {PDS,UPS,data centre,power consumption},
mendeley-tags = {PDS,UPS,data centre,power consumption},
pages = {692--699},
publisher = {Elsevier B.V.},
title = {{Energy Efficient Data Centre Infrastructure − Development of a Power Loss Model}},
url = {http://dx.doi.org/10.1016/j.enbuild.2016.06.037},
volume = {127},
year = {2016}
}
@article{Hardy2015,
author = {Hardy, Stephen},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hardy - 2015 - Is serial 100G ready for 400 Gigabit Ethernet standardization.pdf:pdf},
title = {{Is serial 100G ready for 400 Gigabit Ethernet standardization?}},
year = {2015}
}
@article{Handbook2009,
author = {Handbook, The and Candidates, Nomination},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Handbook, Candidates - 2009 - University Handbook for Examiners of Research Degrees by Theses(2).pdf:pdf},
number = {August},
pages = {1--17},
title = {{University Handbook for Examiners of Research Degrees by Theses}},
year = {2009}
}
@inproceedings{han2013approximate,
abstract = {Approximate computing has recently emerged as a promising approach to energy-efficient design of digital systems. Approximate computing relies on the ability of many systems and applications to tolerate some loss of quality or optimality in the computed result. By relaxing the need for fully precise or completely deterministic operations, approximate computing techniques allow substantially improved energy efficiency. This paper reviews recent progress in the area, including design of approximate arithmetic blocks, pertinent error and quality measures, and algorithm-level techniques for approximate computing.},
author = {Han, Jie and Orshansky, Michael},
booktitle = {Test Symp. (ETS), 2013 18th IEEE Eur.},
doi = {10.1109/ETS.2013.6569370},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Han, Orshansky - 2013 - Approximate computing An emerging paradigm for energy-efficient design(8).pdf:pdf},
isbn = {9781467363778},
issn = {1530-1877},
keywords = {adder,approximate computing,low-energy design,multiplier,probabilistic computing,stochastic computation},
organization = {IEEE},
pages = {1--6},
title = {{Approximate computing: An emerging paradigm for energy-efficient design}},
url = {http://ieeexplore.ieee.org/xpls/abs{\%}7B{\_}{\%}7Dall.jsp?arnumber=6569370{\%}0Ahttp://ieeexplore.ieee.org/xpls/abs{\%}7B{\%}25{\%}7D7B{\%}7B{\_}{\%}7D{\%}7B{\%}25{\%}7D7Dall.jsp?arnumber=6569370},
year = {2013}
}
@book{Hall2009Py3AB,
abstract = {Who is this book forNonprogrammers who want to learn Python programming without taking a detour via a computer science department.},
author = {Hall, Tim and Stacey, J-P},
doi = {10.1007/978-1-4302-1633-9},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hall, Stacey - 2009 - Python 3 for Absolute Beginners‎.pdf:pdf},
isbn = {9781430216322},
issn = {9781430216339 1430216336},
keywords = {hon 3 for absolute},
pages = {300},
title = {{Python 3 for Absolute Beginners‎}},
year = {2009}
}
@article{Gustlin,
author = {Gustlin, Mark and Xilinx, Faisal Dada},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xilinx - Unknown - 400GbE Standards Update(2).pdf:pdf},
title = {{What is FlexEthernet ?}}
}
@article{Guide2015a,
author = {Guide, User},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guide - 2015 - Mali Graphics Debugger 3.1.0(2).pdf:pdf},
title = {{Mali Graphics Debugger 3.1.0}},
year = {2015}
}
@article{Guide2015,
author = {Guide, User},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guide - 2015 - OPT8241 Evaluation Module(2).pdf:pdf},
number = {October},
title = {{OPT8241 Evaluation Module}},
year = {2015}
}
@article{Guide2016,
author = {Guide, Developer},
file = {:C$\backslash$:/Users/D/Documents/Uni/Papers/arm{\_}mali{\_}gpu{\_}opencl{\_}developer{\_}guide{\_}100614{\_}0302{\_}00{\_}en.pdf:pdf},
pages = {2015--2017},
title = {{ARM {\textregistered} Mali ™ GPU OpenCL}},
year = {2016}
}
@incollection{Guide2017,
author = {Guide, Developer},
file = {:C$\backslash$:/Users/D/Documents/Uni/Papers/arm{\_}mali{\_}gpu{\_}opencl{\_}devguide{\_}ANNOTATED{\_}100614{\_}0303{\_}00{\_}en.pdf:pdf},
keywords = {Mali Developer Tools,Mali Graphics plus GPU Compute,Mali Software,Mali-T604,Mali-T624,Mali-T628,Mali-T760,OpenCL},
pages = {2015--2017},
title = {{ARM{\textregistered} Mali™ GPU OpenCL Developer Guide}},
year = {2017}
}
@article{Guide2016,
author = {Guide, Developer},
pages = {2015--2017},
title = {{ARM {\textregistered}Mali ™ GPU OpenCL}},
year = {2016}
}
@article{Grigorian2014,
abstract = {—Prior art in approximate computing has extensively studied computational resilience to imprecision. However, existing approaches often rely on static techniques, which potentially compromise coverage and reliability. Our approach, on the other hand, decouples error analysis of the approximate accelerator from quality analysis of the overall application. We use high-level, application-specific metrics, or Light-Weight Checks (LWCs), to gain coverage by exploiting imprecision tolerance at the applica-tion level. Unlike metrics that compare approximate solutions to exact ones, LWCs can be leveraged dynamically for error analysis and recovery. The resulting methodology adapts to output quality at runtime, providing guarantees on worst-case application-level error. To ensure platform agnosticism, these light-weight metrics are integrated directly into the application, enabling compatibility with any approximate acceleration technique. Our results present a case study of dynamic error control for inverse kinematics. Using software-based neural acceleration with LWC support, we demonstrate improvements in coverage, reliability, and overall performance.},
annote = {Light weight Checks, (analysis) recommends UIQI for images.},
author = {Grigorian, Beayna and Reinman, Glenn},
doi = {10.1109/AHS.2014.6880184},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grigorian, Reinman - 2014 - Dynamically adaptive and reliable approximate computing using light-weight error analysis(3).pdf:pdf},
isbn = {9781479953561},
journal = {Proc. 2014 NASA/ESA Conf. Adapt. Hardw. Syst. AHS 2014},
keywords = {Adaptive Design,Approximate Computing,Dynamic Reliability,Error Control,Platform Agnosticism,Recovery},
pages = {248--255},
title = {{Dynamically adaptive and reliable approximate computing using light-weight error analysis}},
year = {2014}
}
@article{Granmo2018,
abstract = {Although simple individually, artificial neurons provide state-of-the-art performance when interconnected in deep networks. Unknown to many, there exists an arguably even simpler and more versatile learning mechanism, namely, the Tsetlin Automaton. Merely by means of a single integer as memory, it learns the optimal action in stochastic environments. In this paper, we introduce the Tsetlin Machine, which solves complex pattern recognition problems with easy-to-interpret propositional formulas, composed by a collective of Tsetlin Automata. To eliminate the longstanding problem of vanishing signal-to-noise ratio, the Tsetlin Machine orchestrates the automata using a novel game. Our theoretical analysis establishes that the Nash equilibria of the game are aligned with the propositional formulas that provide optimal pattern recognition accuracy. This translates to learning without local optima, only global ones. We argue that the Tsetlin Machine finds the propositional formula that provides optimal accuracy, with probability arbitrarily close to unity. In four distinct benchmarks, the Tsetlin Machine outperforms both Neural Networks, SVMs, Random Forests, the Naive Bayes Classifier and Logistic Regression. It further turns out that the accuracy advantage of the Tsetlin Machine increases with lack of data. The Tsetlin Machine has a significant computational performance advantage since both inputs, patterns, and outputs are expressed as bits, while recognition of patterns relies on bit manipulation. The combination of accuracy, interpretability, and computational simplicity makes the Tsetlin Machine a promising tool for a wide range of domains, including safety-critical medicine. Being the first of its kind, we believe the Tsetlin Machine will kick-start completely new paths of research, with a potentially significant impact on the AI field and the applications of AI.},
archivePrefix = {arXiv},
arxivId = {1804.01508},
author = {Granmo, Ole-Christoffer},
eprint = {1804.01508},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Granmo - 2018 - The Tsetlin Machine - A Game Theoretic Bandit Driven Approach to Optimal Pattern Recognition with Propositional Logic(2).pdf:pdf},
keywords = {bandit problem,game theory,interpretable pattern recognition,online learning,propo-,sitional logic,tsetlin automata games},
pages = {1--36},
title = {{The Tsetlin Machine - A Game Theoretic Bandit Driven Approach to Optimal Pattern Recognition with Propositional Logic}},
url = {http://arxiv.org/abs/1804.01508},
year = {2018}
}
@article{Granath2015,
author = {Granath, Derek},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Granath - 2015 - Network Analytics for More Intelligent SDN(2).pdf:pdf},
number = {April},
title = {{Network Analytics for More Intelligent SDN}},
year = {2015}
}
@article{Goiri2015,
abstract = {We propose and evaluate a framework for creating and running approximation-enabled MapReduce programs. Specifically, we propose approximation mechanisms that fit naturally into the MapReduce paradigm, including input data sampling, task dropping, and accepting and running a precise and a user-defined approximate version of the MapReduce code. We then show how to leverage statistical theories to compute error bounds for popular classes of MapReduce programs when approximating with input data sampling and/or task dropping. We implement the proposed mechanisms and error bound estimations in a prototype system called ApproxHadoop. Our evaluation uses MapReduce applications from different domains, including data analytics, scientific computing, video encoding, and machine learning. Our results show that ApproxHadoop can significantly reduce application execution time and/or energy consumption when the user is willing to tolerate small errors. For example, ApproxHadoop can reduce runtimes by up to 32x when the user can tolerate an error of 1{\%} with 95{\%} confidence. We conclude that our framework and system can make approximation easily accessible to many application domains using the MapReduce model.},
author = {Goiri, Inigo and Bianchini, Ricardo and Nagarakatte, Santosh and Nguyen, Thu D.},
doi = {10.1145/2694344.2694351},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goiri et al. - 2015 - ApproxHadoop Bringing Approximations to MapReduce Frameworks.pdf:pdf},
isbn = {978-1-4503-2835-7},
issn = {0362-1340},
journal = {ACM SIGPLAN Not.},
keywords = {MapReduce,approximation,extreme value theory,multi-stage sampling},
number = {4},
pages = {383--397},
title = {{ApproxHadoop: Bringing Approximations to MapReduce Frameworks}},
url = {http://dl.acm.org/citation.cfm?doid=2694344.2694351{\%}0Ahttp://dl.acm.org/citation.cfm?doid=2694344.2694351},
volume = {50},
year = {2015}
}
@article{Goergen2015,
author = {Goergen, Joel and Systems, Cisco},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goergen, Systems - 2015 - Interconnects Past , Present , and Future(2).pdf:pdf},
keywords = {Ethernet Technology Summit April 2015},
number = {April},
title = {{Interconnects Past , Present , and Future}},
year = {2015}
}
@article{Godtliebsen2004,
abstract = {This paper develops a methodology for finding which features in a noisy image are strong enough to be distinguished from background noise. It is based on scale-space, i.e. a family of smooths of the image. Pixel locations having statistically significant gradient and/or curvature are highlighted by colored symbols. The gradient version is enhanced by displaying regions of significance with streamlines. The usefulness of the new methodology is illustrated by the analysis of simulated and real images. ?? 2004 Elsevier B.V. All rights reserved.},
author = {Godtliebsen, F. and Marron, J. S. and Chaudhuri, Probal},
doi = {10.1016/j.imavis.2004.05.002},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Godtliebsen, Marron, Chaudhuri - 2004 - Statistical significance of features in digital images.pdf:pdf},
issn = {02628856},
journal = {Image Vis. Comput.},
keywords = {Curvature,Data centre,Gradient,Kernel smoothing,Scale space,SiZer,Statistical significance,power consumption},
mendeley-tags = {Data centre,power consumption},
number = {13},
pages = {1093--1104},
title = {{Statistical significance of features in digital images}},
volume = {22},
year = {2004}
}
@article{Gensh2015,
abstract = {Power and energy consumption is a crucial factor for modern computer systems. The optimal operation of the system could be attained only if the interplay between performance and energy consumption was considered during the design. We performed several experiments with the Ordoid-XU3 board. The results of these experiments will help to model runtime for high performance and energy efficient system operations.},
author = {Gensh, Rem and Aalsaud, Ali and Rafiev, Ashur and Xia, Fei and Iliasov, Alexei and Romanovsky, Alexander and Xakovlev, Alex},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gensh et al. - 2015 - Experiments with Odroid-XU3 Board(2).pdf:pdf},
journal = {Tech. Rep. Ser.},
number = {No. CS-TR-1471},
title = {{Experiments with Odroid-XU3 Board}},
year = {2015}
}
@article{Gebregiorgis2017,
author = {Gebregiorgis, Anteneh and Kiamehr, Saman and Tahoori, Mehdi B},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gebregiorgis, Kiamehr, Tahoori - 2017 - Error Propagation Aware Timing Relaxation For Approximate Near Threshold Computing.pdf:pdf},
isbn = {9781450349277},
title = {{Error Propagation Aware Timing Relaxation For Approximate Near Threshold Computing}},
year = {2017}
}
@book{Gaster2012b,
abstract = {Heterogeneous Computing with OpenCL teaches OpenCL and parallel programming for complex systems that may include a variety of device architectures: multi-core CPUs, GPUs, and fully-integrated Accelerated Processing Units (APUs) such as AMD Fusion technology. Designed to work on multiple platforms and with wide industry support, OpenCL will help you more effectively program for a heterogeneous future. Written by leaders in the parallel computing and OpenCL communities, this book will give you hands-on OpenCL experience to address a range of fundamental parallel algorithms. The authors explore memory spaces, optimization techniques, graphics interoperability, extensions, and debugging and profiling. Intended to support a parallel programming course, Heterogeneous Computing with OpenCL includes detailed examples throughout, plus additional online exercises and other supporting materials. Explains principles and strategies to learn parallel programming with OpenCL, from understanding the four abstraction models to thoroughly testing and debugging complete applications. Covers image processing, web plugins, particle simulations, video editing, performance optimization, and more. Shows how OpenCL maps to an example target architecture and explains some of the tradeoffs associated with mapping to various architectures Addresses a range of fundamental programming techniques, with multiple examples and case studies that demonstrate OpenCL extensions for a variety of hardware platforms. Introduction to Parallel Programming -- Introduction to OpenCL -- OpenCL Device Architectures -- Basic OpenCL Examples -- Understanding OpenCL's Concurrency and Execution Model -- Dissecting a CPU/GPU OpenCL Implementation -- OpenCL Case Study: Convolution -- OpenCL Case Study: Video Processing -- OpenCL Case Study: Histogram -- OpenCL Case Study: Mixed Particle Simulation -- OpenCL Extensions -- OpenCL Profiling and Debugging -- WebCL. Introduction to parallel programming -- Introduction to OpenCL -- OpenCL device architectures -- Basic OpenCL examples -- Understanding OpenCL's concurrency and execution model -- Dissecting an CPU/GPU OpenCL implementation -- OpenCL case study : convolution -- OpenCL case study : video processing -- OpenCL case study : histogram -- OpenCL case study : mixed particle simulation -- OpenCL extensions -- OpenCL profiling and debugging -- WebCL.},
author = {Gaster, Benedict and Howes, Lee and Kaeli, David R. and Mistry, Perhaad and Schaa, Dana},
booktitle = {Heterog. Comput. with OpenCL},
doi = {10.1016/B978-0-12-387766-6.00027-X},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gaster et al. - 2012 - Heterogeneous Computing with OpenCL.pdf:pdf},
isbn = {9780123877666},
pages = {277},
publisher = {Morgan Kaufmann},
title = {{Heterogeneous Computing with OpenCL}},
url = {http://www.sciencedirect.com/science/article/pii/B978012387766600027X},
year = {2012}
}
@inproceedings{Gaster2013,
abstract = {With the success of programming models such as Khronos' OpenCL, heterogeneous computing is going mainstream. However, these models are low-level, even when considering them as systems programming models. For example, OpenCL is effectively an extended subset of C99, limited to the type unsafe procedural abstraction that C has provided for more than 30 years. Computer systems programming has for more than two decades been able to do a lot better. One successful case in point is the systems programming language C++, known for its strong(er) type system, templates, and object-oriented abstraction features. In this paper we introduce OpenCL C++, an object-oriented programming model (based on C++11) for heterogeneous computing and an alternative for developers targeting OpenCL enabled devices. We show that OpenCL C's address space qualifiers, and by implication Embedded C's, can be lifted into C++'s type system. A novel application of C++11's new type inference features (auto/decltype) with respect to address space qualifiers allows natural and generic use of the this pointer. We qualitatively show that OpenCL C++ is a simpler and a more expressive development platform than its OpenCL C counter part.},
annote = {ACM paper

GPGPU-6 Proceedings of the 6th Workshop on General Purpose Processor Using Graphics Processing Units 
Pages 86-95},
author = {Gaster, Benedict R. and Howes, Lee},
booktitle = {OpenCL C++},
doi = {10.1145/2458523.2458532},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gaster, Howes - 2013 - OpenCL C(2).pdf:pdf},
isbn = {9781450320177},
keywords = {C++,GPGPU,GPU,OpenCL,parallel programming},
pages = {86--95},
title = {{OpenCL C++}},
url = {http://dl.acm.org/citation.cfm?id=2458523.2458532 http://dl.acm.org/citation.cfm?doid=2458523.2458532},
year = {2013}
}
@article{Gao2014,
author = {Gao, Wei and Richard, C{\'{e}}dric and Bermudez, J.C.M. C M and Huang, Jianguo},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gao et al. - 2014 - Convex Combination of Kernel Adaptive Filters(2).pdf:pdf},
isbn = {9781479936946},
journal = {2014 IEEE Int. Work. Mach. Learn. Signal Process.},
number = {1},
title = {{Convex Combination of Kernel Adaptive Filters}},
year = {2014}
}
@article{Gandhi2015,
author = {Gandhi, Prashant},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gandhi - 2015 - Applying Hyper-scale Networking Principles to Normal-Scale Data Centers(2).pdf:pdf},
number = {April},
pages = {1--31},
title = {{Applying Hyper-scale Networking Principles to Normal-Scale Data Centers}},
year = {2015}
}
@article{Furrer2017,
author = {Furrer, Simeon and Lantz, Mark A. and Reininger, Peter and Pantazi, Angeliki and Rothuizen, Hugo E. and Cideciyan, Roy D. and Cherubini, Giovanni and Haeberle, Walter and Eleftheriou, Evangelos and Tachibana, Junichi and Sekiguchi, Noboru and Aizawa, Takashi and Endo, Tetsuo and Ozaki, Tomoe and Sai, Teruo and Hiratsuka, Ryoichi and Mitamura, Satoshi and Yamaguchi, Atsushi},
doi = {10.1109/TMAG.2017.2727822},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Furrer et al. - 2017 - 201 Gbin² Recording Areal Density on Sputtered Magnetic Tape.pdf:pdf},
issn = {0018-9464},
journal = {IEEE Trans. Magn.},
pages = {1--1},
title = {{201 Gb/in² Recording Areal Density on Sputtered Magnetic Tape}},
url = {http://ieeexplore.ieee.org/document/7984852/},
year = {2017}
}
@inproceedings{Fiennes2015,
author = {Fiennes, Hugo and Imp, Electric},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fiennes, Imp - 2015 - Securing the IoT {\ldots} from the device to the cloud Why be so concerned about IoT security(2).pdf:pdf},
keywords = {Security,attacks,hacking},
mendeley-tags = {Security,attacks,hacking},
number = {December},
title = {{Securing the IoT {\ldots} from the device to the cloud Why be so concerned about IoT security ?}},
year = {2015}
}
@article{Feller2015,
author = {Feller, Scott},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Feller - 2015 - of 100G Data Center Optical Interconnects(2).pdf:pdf},
number = {April},
title = {{of 100G Data Center Optical Interconnects}},
year = {2015}
}
@article{February2013,
author = {February, Alice},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/February - 2013 - The Relationship Between the UNIVAC Computer and Evolutionary Programming(2).pdf:pdf},
number = {1},
pages = {1--2},
title = {{The Relationship Between the UNIVAC Computer and Evolutionary Programming}},
volume = {3},
year = {2013}
}
@article{ExadataOracleM7,
annote = {E10k had 64 processors, this has 256!!},
author = {Exadata, Oracle and Server, Storage and Solaris, Oracle},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Exadata, Server, Solaris - Unknown - Oracle SuperCluster M7(2).pdf:pdf},
title = {{Oracle SuperCluster M7}}
}
@inproceedings{Esposito2017,
abstract = {{\textcopyright} 2017 IEEE. Sacrificing exact calculations to improve digital circuit performance is at the foundation of approximate computing. In this paper, an approximate multiply-and-accumulate (MAC) unit is introduced. The MAC partial product terms are compressed by using simple OR gates as approximate counters; moreover, to further save energy, selected columns of the partial product terms are not formed. A compensation term is introduced in the proposed MAC, to reduce the overall approximation error. A MAC unit, specialized to perform 2D convolution, is designed following the proposed approach and implemented in TSMC 40nm technology in four different configurations. The proposed circuits achieve power savings more than 60{\%}, compared to standard, exact MAC, with tolerable image quality degradation.},
author = {Esposito, Darjn and Strollo, Antonio G.M. and Alioto, Massimo},
booktitle = {PRIME 2017 - 13th Conf. PhD Res. Microelectron. Electron. Proc.},
doi = {10.1109/PRIME.2017.7974112},
file = {:C$\backslash$:/Users/D/Documents/Uni/Papers/PID4776555{\_}research{\_}gate.pdf:pdf},
isbn = {9781509065073},
keywords = {Approximate MAC Unit,Approximate computing,Error compensation.,Imprecise hardware,Truncated multipliers},
title = {{Low-power approximate MAC unit}},
year = {2017}
}
@article{Esmaeilzadeh2012,
abstract = {This paper describes a learning-based approach to the acceleration of approximate programs. We describe the {\$}\backslash{\$}emph{\{}Parrot transformation{\}}, a program transformation that selects and trains a neural network to mimic a region of imperative code. After the learning phase, the compiler replaces the original code with an invocation of a low-power accelerator called a {\$}\backslash{\$}emph{\{}neural processing unit{\}} (NPU). The NPU is tightly coupled to the processor pipeline to accelerate small code regions. Since neural networks produce inherently approximate results, we define a programming model that allows programmers to identify approximable code regions-code that can produce imprecise but acceptable results. Offloading approximable code regions to NPUs is faster and more energy efficient than executing the original code. For a set of diverse applications, NPU acceleration provides whole-application speedup of 2.3x and energy savings of 3.0x on average with quality loss of at most 9.6{\%}. {\textcopyright}2012 IEEE.},
author = {Esmaeilzadeh, Hadi and Sampson, Adrian and Ceze, Luis and Burger, Doug},
doi = {10.1109/MICRO.2012.48},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Esmaeilzadeh et al. - 2012 - Neural Acceleration for General Purpose Approximate Programs.pdf:pdf},
isbn = {978-1-4673-4819-5},
issn = {1072-4451},
journal = {Proc. th 2012 25th Annu. IEEE/ACM Intenational Symp. Microarchitecture},
keywords = {NPUs,Parrot algorithmic transformation,accelerators,approximate computing,neural networks,neural processing units},
number = {3},
pages = {16--27},
title = {{Neural Acceleration for General Purpose Approximate Programs}},
url = {http://dl.acm.org/citation.cfm?id=2457519},
volume = {33},
year = {2012}
}
@techreport{Elmshirst2017,
author = {Elmshirst, Orlando},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Elmshirst - 2017 - Our energy insights(2).pdf:pdf},
pages = {0--4},
title = {{Our energy insights}},
year = {2017}
}
@article{Duraisamy,
author = {Duraisamy, Karthi and Lu, Hao},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Duraisamy, Lu - Unknown - Accelerating Graph Community Detection with Approximate Updates via an Energy-Efficient NoC.pdf:pdf},
keywords = {graph community detection,wireless network-on-chip},
title = {{Accelerating Graph Community Detection with Approximate Updates via an Energy-Efficient NoC}}
}
@article{Duben2015,
abstract = {In this paper, we demonstrate that disproportionate gains are possible through a simple devise for injecting inexactness or approximation into the hardware architecture of a computing system with a general purpose template including a complete memory hierarchy. The focus of the study is on energy savings possible through this approach in the context of large and challenging applications. We choose two such from different ends of the computing spectrum—the IGCM model for weather and climate modeling which embodies significant features of a high-performance computing workload, and the ubiquitous PageRank algorithm used in Internet search. In both cases, we are able to show in the affirmative that an inexact system outperforms its exact counterpart in terms of its efficiency quantified through the relative metric of operations per virtual Joule (OPVJ)—a relative metric that is not tied to particular hardware technology. As one example, the IGCM application can be used to achieve savings through inexactness of (almost) a factor of 3 in energy without compromising the quality of the forecast, quantified through the forecast error metric, in a noticeable manner. As another example finding, we show that in the case of PageRank, an inexact system is able to outperform its exact counterpart by close to a factor of 1.5 using the OPVJ metric.},
annote = {a preliminary study of the opportunities present in achieving energy savings through novel approaches such as inexactness.

applications.
Inexactness might lower the quality of an application by a certain amount but in return for energy gains. Now, if we reinvest these gains in a different part of the application—computing the IGCM at a finer scale or increasing the number of iterations of PageRank—the overall result could in fact be better than with the original exact solution that is typically viewed as a gold standard.},
author = {D{\"{u}}ben, Peter and Schlachter, Jeremy and ., Parishkrati and Yenugula, Sreelatha and Augustine, John and Enz, Christian and Palem, K. and Palmer, T. N.},
doi = {10.7873/DATE.2015.1116},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/D{\"{u}}ben et al. - 2015 - Opportunities for Energy Efficient Computing A Study of Inexact General Purpose Processors for High-Performance(2).pdf:pdf},
isbn = {9783981537048},
issn = {15301591},
journal = {Des. Autom. Test Eur. Conf. Exhib. (DATE), 2015},
pages = {764--769},
title = {{Opportunities for Energy Efficient Computing: A Study of Inexact General Purpose Processors for High-Performance and Big-Data Applications}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7092489},
year = {2015}
}
@article{Doyle2015,
author = {Doyle, Jeff},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Doyle - 2015 - SDN and Open Ethernet Switches Empower Modern Data Center Networks(2).pdf:pdf},
number = {April},
pages = {1--17},
title = {{SDN and Open Ethernet Switches Empower Modern Data Center Networks}},
year = {2015}
}
@article{Divakar2015,
author = {Divakar, M P},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Divakar - 2015 - Ethernet Technology Summit 2015 Session IOT-12 IoT Networking Session IOT-12 IoT Networking(2).pdf:pdf},
title = {{Ethernet Technology Summit 2015 Session IOT-12 : IoT Networking Session IOT-12 : IoT Networking}},
year = {2015}
}
@article{Davies2018,
author = {Davies, Tilman M. and Baddeley, Adrian},
doi = {10.1007/s11222-017-9772-4},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Davies, Baddeley - 2018 - Fast computation of spatially adaptive kernel estimates.pdf:pdf},
issn = {15731375},
journal = {Stat. Comput.},
keywords = {Bandwidth selection,Edge correction,Fourier transform,Intensity,Scale space,Spatial point process},
number = {4},
pages = {937--956},
publisher = {Springer US},
title = {{Fast computation of spatially adaptive kernel estimates}},
volume = {28},
year = {2018}
}
@article{Davidson2016,
author = {Davidson, Scott},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Davidson - 2016 - Good Enough Computing(3).pdf:pdf},
pages = {95054},
title = {{Good Enough Computing}},
year = {2016}
}
@article{Danelljan2017,
abstract = {In recent years, Discriminative Correlation Filter (DCF) based methods have significantly advanced the state-of-the-art in tracking. However, in the pursuit of ever increasing tracking performance, their characteristic speed and real-time capability have gradually faded. Further, the increasingly complex models, with massive number of trainable parameters, have introduced the risk of severe over-fitting. In this work, we tackle the key causes behind the problems of computational complexity and over-fitting, with the aim of simultaneously improving both speed and performance. We revisit the core DCF formulation and introduce: (i) a factorized convolution operator, which drastically reduces the number of parameters in the model; (ii) a compact generative model of the training sample distribution, that significantly reduces memory and time complexity, while providing better diversity of samples; (iii) a conservative model update strategy with improved robustness and reduced complexity. We perform comprehensive experiments on four benchmarks: VOT2016, UAV123, OTB-2015, and TempleColor. When using expensive deep features, our tracker provides a 20-fold speedup and achieves a 13.0{\%} relative gain in Expected Average Overlap compared to the top ranked method in the VOT2016 challenge. Moreover, our fast variant, using hand-crafted features, operates at 60 Hz on a single CPU, while obtaining 65.0{\%} AUC on OTB-2015.},
archivePrefix = {arXiv},
arxivId = {1611.09224},
author = {Danelljan, Martin and Bhat, Goutam and {Shahbaz Khan}, Fahad and Felsberg, Michael},
doi = {10.1109/CVPR.2017.733},
eprint = {1611.09224},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Danelljan et al. - 2017 - ECO Efficient convolution operators for tracking.pdf:pdf},
isbn = {9781538604571},
journal = {Proc. - 30th IEEE Conf. Comput. Vis. Pattern Recognition, CVPR 2017},
pages = {6931--6939},
title = {{ECO: Efficient convolution operators for tracking}},
volume = {2017-Janua},
year = {2017}
}
@inproceedings{DAmbrosia2015,
author = {D'Ambrosia, John},
booktitle = {IEEE Commun. Mag.},
doi = {10.1109/MCOM.2010.5434372},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/D'Ambrosia - 2010 - 100 Gigabit Ethernet and Beyond(2).pdf:pdf},
issn = {01636804},
keywords = {400G Enet},
mendeley-tags = {400G Enet},
number = {3},
pages = {6--12},
title = {{100 Gigabit Ethernet and Beyond}},
volume = {48},
year = {2015}
}
@article{Dahir2015,
abstract = {Implementing Dynamic Voltage and Frequency Scaling (DVFS) is a non-trivial task on FPGAs and requires knowledge about the feasible voltage and frequency (VF) ranges as a first step. The VF feasible ranges depend not only on the size of the critical path in the design but also on the inter- and intra-die variability on the FPGA die. Moreover, the variations in the configuration of the FPGA highly affect feasible VF ranges. Therefore, it is crucial to characterise feasibility by studying the relationship between feasible VF regions and these sources of variability in FPGAs. In this paper we employ a self-checking multiplier which uses residue codes and DVFS implemented on the programmable logic component of a Xilinx Zynq ZC702 device as an error-detection circuit to study these feasible regions. Results show that, as expected, feasible VF ranges vary with FPGA configuration. More interestingly, significant variation of the feasible VF regions is found for different dies. These results highlight the necessity of dynamic self-testing as a part of an adaptive DVFS implementation on FPGAs. Employing the techniques presented in this work enables the implementation of efficient adaptive on-line DVFS on programmable logic while ensuring reliability.},
author = {Dahir, Nizar and Campos, Pedro and Tempesti, Gianluca and Trefzer, Martin and Tyrrell, Andrew},
doi = {10.1109/FPL.2015.7293998},
file = {:C$\backslash$:/Users/D/Documents/Uni/Papers/07293998.pdf:pdf},
isbn = {9780993428005},
journal = {25th Int. Conf. F. Program. Log. Appl. FPL 2015},
pages = {2--5},
title = {{Characterisation of feasibility regions in FPGAS under adaptive DVFS}},
year = {2015}
}
@article{Cyclone2012,
author = {Cyclone, The},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cyclone - 2012 - Key Advantages of Cyclone V Devices.pdf:pdf},
title = {{Key Advantages of Cyclone V Devices}},
year = {2012}
}
@article{Cui2012a,
abstract = {Motion saliency detection aims at finding the semantic regions in a video sequence. It is an important pre-processing step in many vision applications. In this paper, we propose a new algorithm, Temporal Spectral Residual, for fast motion saliency detection. Different from conventional motion saliency detection algorithms that use complex mathematical models, our goal is to find a good tradeoff between the computational efficiency and accuracy. The basic observation for salient motions is that on the cross section along the temporal axis of a video sequence, the regions of moving objects contain distinct signals while the background area contains redundant information. Thus our focus in this paper is to extract the salient information on the cross section, by utilizing the off-the-shelf method Spectral Residual, which is a 2D image saliency detection method. Majority voting strategy is also introduced to generate reliable results. Since the proposed method only involves Fourier spectrum analysis, it is computationally efficient. We validate our algorithm on two applications: background subtraction in outdoor video sequences under dynamic background and left ventricle endocardium segmentation in MR sequences. Compared with some state-of-art algorithms, our algorithm achieves both good accuracy and fast computation, which satisfies the need as a pre-processing method. {\textcopyright} 2012 Elsevier B.V.},
author = {Cui, Xinyi and Liu, Qingshan and Zhang, Shaoting and Yang, Fei and Metaxas, Dimitris N.},
doi = {10.1016/j.neucom.2011.12.033},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cui et al. - 2012 - Temporal Spectral Residual for fast salient motion detection.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Computer vision,Motion saliency detection,Spectral Residual},
pages = {24--32},
publisher = {Elsevier},
title = {{Temporal Spectral Residual for fast salient motion detection}},
url = {http://dx.doi.org/10.1016/j.neucom.2011.12.033},
volume = {86},
year = {2012}
}
@article{Cui2012,
author = {Cui, Xinyi and Liu, Qingshan and Zhang, Shaoting and Yang, Fei and Metaxas, Dimitris N},
doi = {10.1016/j.neucom.2011.12.033},
file = {:C$\backslash$:/Users/D/Documents/Uni/Papers/1-s2.0-S0925231212001099-main.pdf:pdf},
issn = {0925-2312},
journal = {Neurocomputing},
keywords = {Computer vision,Motion saliency detection,Spectral Residual},
pages = {24--32},
publisher = {Elsevier},
title = {{Neurocomputing Temporal Spectral Residual for fast salient motion detection}},
url = {http://dx.doi.org/10.1016/j.neucom.2011.12.033},
volume = {86},
year = {2012}
}
@article{Corporation,
annote = {Where are FPGAs heading?},
author = {Corporation, Altera},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Corporation - Unknown - Implementing Ultra-Fast Ethernet In a FPGA Altera Corporation Overview.pdf:pdf},
title = {{Implementing Ultra-Fast Ethernet In a FPGA Altera Corporation Overview}}
}
@article{Connors2013,
abstract = {Open Compute Language (OpenCL) has been proposed as a platform-independent, parallel execution model to target heterogeneous systems, including multiple central processing units, graphics processing units (GPUs), and digital signal processors (DSPs). OpenCL parallelism scales with the available resources and hardware generational improvements due to the data-parallel nature of its kernels. Such parallel expressions must adhere to a rigid execution model, essentially forcing the run-time system to behave as a batch-scheduler for small, local workgroups of a larger global problem. In many scenarios, especially in the real-time computing environments of mobile computing, a mobile system must adapt to system constraints and problem characteristics. This paper investigates the concept of Adaptive OpenCL (ACL) to explore algorithm support for dynamically adapting data-model properties and runtime machine characteristics. We show that certain algorithms can be structured to dynamically balance problem correctness and performance. 1.},
author = {Connors, Dan and Dunn, Kyle and Wiencrot, Jeff},
file = {:C$\backslash$:/Users/D/Documents/Uni/Papers/ADAPT2013.pdf:pdf},
isbn = {9781450320221},
title = {{Adaptive OpenCL ( ACL ) Execution in GPU Architectures}},
year = {2013}
}
@article{Conference2015,
author = {Conference, Iot Design},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Conference - 2015 - Disruptive semiconductor technologies for the IoT.pdf:pdf},
title = {{Disruptive semiconductor technologies for the IoT}},
year = {2015}
}
@article{Cisco2013,
author = {Cisco},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cisco - 2013 - Cisco Application Centric Infrastructure.pdf:pdf},
pages = {2014--2016},
title = {{Cisco Application Centric Infrastructure}},
url = {http://www.cisco.com/c/dam/en/us/products/collateral/switches/nexus-9000-series-switches/at-a-glance-c45-730001.pdf},
year = {2013}
}
@misc{Cisco,
abstract = {Global Cloud Index of world wide statistics for cloud traffic forecasts. Covers regional also.},
author = {Cisco},
keywords = {cloud,data centres,global,regional,traffic},
mendeley-tags = {cloud,data centres,global,regional,traffic},
title = {{Cisco GCI Highlights Tool}},
url = {https://www.cisco.com/c/en/us/solutions/service-provider/gci-highlights-tool/index.html}
}
@article{Christmann2012,
abstract = {Wireless sensor nodes that are self-powered by extracting their energy from their environment are a new opportunity for monitoring purpose. Since the available energy is not constant over time and due to very low harvested power levels, efficient energy and power management strategies are mandatory for improving their autonomy. At system level, scheduling algorithms are proposed to efficiently use multi power path architectures and avoid as much as possible the use of batteries. A data- and energy-driven architecture and its associated algorithm are presented achieving high efficiency due to fully adaptive scheme.},
annote = {A treatise on all the elements of an energy harvesting Wirelss sensor network challenges. Gives a fair bit of detail on the various elements. Targets a complex adaptive power storage scenario utilising supercaps and battery, thus powering loads via supercaps and utilising excess power Vcap {\textgreater}Vmax to charge battery and using battery when Vcap {\textless} Vmin},
author = {Christmann, J. F. and Beign{\'{e}}, E. and Condemine, C. and Willemin, J. and Piguet, C.},
doi = {10.1145/2228360.2228550},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Christmann et al. - 2012 - Energy harvesting and power management for autonomous sensor nodes.pdf:pdf},
isbn = {9781450311991},
issn = {0738-100X},
journal = {Proc. 49th Annu. Des. Autom. Conf. - DAC '12},
pages = {1049},
title = {{Energy harvesting and power management for autonomous sensor nodes}},
url = {http://dl.acm.org/citation.cfm?doid=2228360.2228550},
year = {2012}
}
@article{Chow2005,
abstract = {A methodology for supporting dynamic voltage scaling (DVS) on commercial FPGAs is described. A logic delay measurement circuit (LDMC) is used to determine the speed of an inverter chain for various operating conditions at run time. A desired LDMC value, intended to match the critical path of the operating circuit plus a safety margin, is then chosen; a closed loop control scheme is used to maintain the desired LDMC value as chip temperature changes, by automatically adjusting the voltage applied to the FPGA. We describe experiments using this technique on various circuits at different clock frequencies and temperatures to demonstrate its utility and robustness. Power savings between 4{\%} and 54{\%} for the VINT supply are observed},
annote = {Fairly old attempt at DVS tuning in FPGA},
author = {Chow, C. T. and Tsui, L. S.M. and Leong, P. H.W. and Luk, W. and Wilton, S. J.E.},
doi = {10.1109/FPT.2005.1568543},
file = {:C$\backslash$:/Users/D/Documents/Uni/Papers/10.1.1.137.6370.pdf:pdf},
isbn = {0780394070},
journal = {Proc. - 2005 IEEE Int. Conf. F. Program. Technol.},
pages = {173--180},
title = {{Dynamic voltage scaling for commercial FPGAs}},
volume = {2005},
year = {2005}
}
@misc{chollet2015keras,
author = {Cholet, Francois},
title = {{Keras}},
url = {https://keras.io},
year = {2015}
}
@article{Chippa2014NewAvenues,
abstract = {Algorithms from several interesting application domains exhibit the property of inherent resilience to errors from extrinsic or intrinsic sources, offering entirely new avenues for performance and power optimization by relaxing the conventional requirement of exact (numerical or Boolean) equivalence between the specification and hardware implementation. We propose scalable effort hardware design as an approach to tap the reservoir of algorithmic resilience and translate it into highly efficient hardware implementations The basic tenet of the scalable effort design approach is to identify mechanisms at each level of design abstraction (circuit, architecture and algorithm) that can be used to vary the computational effort expended towards generation of the correct (exact) result, and expose them as control knobs in the implementation. These scaling mechanisms can be utilized to achieve improved energy efficiency while maintaining an acceptable (and often, near identical) level of quality of the overall result. A second major tenet of the scalable effort design approach is that fully exploiting the potential of algorithmic resilience requires synergistic cross-layer optimization of scaling mechanisms identified at different levels of design abstraction. We have implemented an energy-efficient SVM classification chip based on the proposed scalable effort design approach. We present results from post-layout simulations and demonstrate that scalable effort hardware can achieve large energy reductions (1.2X-2.2X with no impact on classification accuracy, and 2.2X-4.1X with modest reductions in accuracy) across various sets. Our results also establish that cross-layer optimization leads to much improved energy vs. quality tradeoffs compared to each of the individual techniques.},
author = {Chippa, Vinay Kumar and Mohapatra, Debabrata and Roy, Kaushik and Chakradhar, Srimat T. and Raghunathan, Anand},
doi = {10.1109/TVLSI.2013.2276759},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chippa et al. - 2014 - Scalable effort hardware design.pdf:pdf},
isbn = {9781450300025},
issn = {10638210},
journal = {IEEE Trans. Very Large Scale Integr. Syst.},
keywords = {Algorithms,Approximate computing,Design,Scalable effort hardware},
number = {9},
pages = {2004--2016},
title = {{Scalable effort hardware design}},
volume = {22},
year = {2014}
}
@inproceedings{Chippa2013Correctness,
abstract = {Computing today is largely not about calculating a precise numerical end result. Instead, computing platforms are increasingly used to execute applications (such as search, analytics, sensor data processing, recognition, mining, and synthesis) for which; "correctness"; is defined as producing results that are good enough, or of sufficient quality. These applications are often intrinsically resilient to a large fraction of their computations being executed in an imprecise or approximate manner. However, the design of computing platforms continues to be guided by the principle that every computation must be executed with the same strict notion of correctness. Approximate computing departs from this long-held dogma, and exploits intrinsic application resilience to improve the efficiency (energy or speed) of computing platforms. We describe an integrated approach to approximate computing in hardware that consists of three key components. First, we present an automatic resilience characterization framework that allows the designer to quantitatively evaluate the intrinsic resilience of an application, and to quickly assess the potential of various approximate computing techniques. We then describe scalable effort hardware, an approach to approximate computing wherein hardware is designed with various scaling mechanisms, or knobs that modulate the effort expended towards correctly performing an application's computations. Scaling mechanisms are identified at the algorithm, architecture, and circuit levels, and embodied in the hardware to provide a rich trade-off between computational accuracy and energy. Finally, dynamic effort scaling is proposed as a feedback control approach to modulate the scaling mechanisms at runtime in response to varying application requirements and data characteristics. To demonstrate the proposed concepts, we have designed and fabricated an energy-efficient Recognition and Mining (RM) processor in the TSMC 65nm process technology.- Our measurement results demonstrate that approximate computing leads to 2-20X energy savings with minimal impact on output quality across a range of applications.},
annote = {Brings together automatic resilience, scalable effort and dynamic effort scaling.

ASIC based solution.},
author = {Chippa, Vinay K. and Venkataramani, Swagath and Chakradhar, Srimat T. and Roy, Kaushik and Raghunathan, Anand},
booktitle = {Conf. Rec. - Asilomar Conf. Signals, Syst. Comput.},
doi = {10.1109/ACSSC.2013.6810241},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chippa et al. - 2013 - Approximate computing An integrated hardware approach(3).pdf:pdf},
isbn = {9781479923908},
issn = {10586393},
pages = {111--117},
title = {{Approximate computing: An integrated hardware approach}},
year = {2013}
}
@article{Chippa2013DES,
abstract = {Several current and emerging applications do not have a unique result for a given input; rather, functional correctness is defined in terms of output quality. Recently proposed design techniques exploit the inherent resilience of such applications and achieve improved efficiency (energy or performance) by foregoing correct execution of all the constituent computations. Hardware and software systems that are thus designed may be viewed as scalable effort systems, since they offer the capability to modulate the effort that they expend towards computation, thereby allowing for trade-offs between output quality and efficiency. We propose the concept of Dynamic Effort Scaling (DES), which refers to dynamic management of the control knobs that are exposed by scalable effort systems. We argue the need for DES by observing that the degree of resilience often varies significantly across applications, across datasets, and even within a dataset. We propose a general conceptual framework for DES by formulating it as a feedback control problem, wherein the scaling mechanisms are regulated with the goal of maintaining output quality at or above a specified limit. We present an implementation of Dynamic Effort Scaling for recognition and mining applications and evaluate it for the support vector machines and K-means clustering algorithms under various application scenarios and datasets. Our results clearly demonstrate the benefits of the proposed approach---statically setting the scaling mechanisms leads to either significant error overshoot or significant opportunities for energy savings left on the table unexploited. In contrast, DES is able to effectively regulate the output quality while maximally exploiting the time-varying resiliency in the workload.},
annote = {Dynamic Effort Scaling. Dynamic management of control knobs},
author = {Chippa, Vinay K. and Roy, Kaushik and Chakradhar, Srimat T. and Raghunathan, Anand},
doi = {10.1145/2465787.2465792},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chippa et al. - 2013 - Managing the Quality vs. Efficiency Trade-off Using Dynamic Effort Scaling(3).pdf:pdf},
issn = {15399087},
journal = {ACM Trans. Embed. Comput. Syst.},
keywords = {Dynamic effort scaling,K-means clustering,approximate computing,low power design,mining,recognition,scalable effort,support vector machines},
mendeley-tags = {Dynamic effort scaling},
number = {2s},
pages = {1--23},
title = {{Managing the Quality vs. Efficiency Trade-off Using Dynamic Effort Scaling}},
url = {http://dl.acm.org/citation.cfm?doid=2465787.2465792},
volume = {12},
year = {2013}
}
@article{Chippa2013Analysis,
abstract = {Approximate computing is an emerging design paradigm that enables highly efficient hardware and software implementa- tions by exploiting the inherent resilience of applications to in-exactness in their computations. Previous work in this area has demonstrated the potential for significant energy and per- formance improvements, but largely consists of ad hoc tech- niques that have been applied to a small number of applica- tions. Taking approximate computing closer to mainstream adoption requires (i) a deeper understanding of inherent ap- plication resilience across a broader range of applications (ii) tools that can quantitatively establish the inherent resilience of an application, and (iii) methods to quickly assess the poten- tial of various approximate computing techniques for a given application. We make two key contributions in this direction. Our primary contribution is the analysis and characterization of inherent application resilience present in a suite of 12 widely used applications from the domains of recognition, data min- ing, and search. Based on this analysis, we present several new insights into the nature of resilience and its relationship to var- ious key application characteristics. To facilitate our analysis, we propose a systematic framework for Application Resilience Characterization (ARC) that (a) partitions an application into resilient and sensitive parts and (b) characterizes the resilient parts using approximation models that abstract a wide range of approximate computing techniques. We believe that the key insights that we present can help shape further research in the area of approximate computing, while automatic resilience characterization frameworks such as ARC can greatly aid de- signers in the adoption approximate computing.},
annote = {ASIC based solutions, all four papers cover three distinct elements and the fourth "integrates" them into one solution},
author = {Chippa, Vinay K. and Chakradhar, Srimat T. and Roy, Kaushik and Raghunathan, Anand},
doi = {10.1145/2463209.2488873},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chippa et al. - 2013 - Analysis and characterization of inherent application resilience for approximate computing.pdf:pdf},
isbn = {9781450320719},
issn = {0738100X},
journal = {Proc. 50th Annu. Des. Autom. Conf. - DAC '13},
keywords = {approximate computing,inherent application resilience,re-,silience characterization},
number = {i},
pages = {1},
title = {{Analysis and characterization of inherent application resilience for approximate computing}},
url = {http://dl.acm.org/citation.cfm?doid=2463209.2488873},
year = {2013}
}
@article{Cheng2014,
abstract = {A self-adaptive software system modifies its behavior at runtime in response to changes within the system or in its execution environment. The fulfillment of the system requirements needs to be guaranteed even in the presence of adverse conditions and adaptations. Thus, a key challenge for self-adaptive software systems is assurance. Traditionally, confidence in the correctness of a system is gained through a variety of activities and processes performed at development time, such as design analysis and testing. In the presence of self-adaptation, however, some of the assurance tasks may need to be performed at runtime. This need calls for the development of techniques that enable continuous assurance throughout the software life cycle. Fundamental to the development of runtime assurance techniques is research into the use of models at runtime (M@RT). This chapter explores the state of the art for using M@RT to address the assurance of self-adaptive software systems. It defines what information can be captured by M@RT, specifically for the purpose of assurance, and puts this definition into the context of existing work. We then outline key research challenges for assurance at runtime and characterize assurance methods. The chapter concludes with an exploration of selected application areas where M@RT could provide significant benefits beyond existing assurance techniques for adaptive systems. {\textcopyright} 2014 Springer International Publishing.},
author = {Cheng, Betty H.C. C and Eder, Kerstin I. and Gogolla, Martin and Grunske, Lars and Litoiu, Marin and M{\"{u}}ller, Hausi A. and Pelliccione, Patrizio and Perini, Anna and Qureshi, Nauman A. and Rumpe, Bernhard and Schneider, Daniel and Trollmann, Frank and Villegas, Norha M.},
doi = {10.1007/978-3-319-08915-7_4},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng et al. - 2014 - Using models at runtime to address assurance for self-adaptive systems.pdf:pdf},
isbn = {9783319089140},
issn = {16113349},
journal = {Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)},
pages = {101--136},
title = {{Using models at runtime to address assurance for self-adaptive systems}},
volume = {8378 LNCS},
year = {2014}
}
@article{Chen2011,
abstract = {The development of real-time image quality assessment algorithms is an important direction on which little research has focused. This paper presents a design of real-time implementable full-reference image quality algorithms based on the SSIM index and multi-scale SSIM (MS-SSIM) index. The proposed algorithms, which modify SSIM/MS-SSIM to achieve speed, are tested on the LIVE image quality database and shown to yield performance commensurate with SSIM and MS-SSIM but with much lower computational complexity.},
author = {Chen, Ming Jun and Bovik, Alan C.},
doi = {10.1007/s11554-010-0170-9},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Bovik - 2011 - Fast structural similarity index algorithm.pdf:pdf},
isbn = {9781424442966},
issn = {18618200},
journal = {J. Real-Time Image Process.},
keywords = {Image quality assessment,Low complexity algorithm,Real time,SSIM,Video quality assessment},
number = {4},
pages = {281--287},
title = {{Fast structural similarity index algorithm}},
volume = {6},
year = {2011}
}
@article{Chen2012,
abstract = {The FPGA can be a tremendously efficient computational fabric for many applications. In particular, the performance to power ratios of FPGA make them attractive solutions to solve the problem of data centers that are constrained largely by power and cooling costs. However, the complexity of the FPGA design flow requires the programmer to understand cycle-accurate details of how data is moved and transformed through the fabric. In this paper, we explore techniques that allow programmers to efficiently use FPGAs at a level of abstraction that is closer to traditional software-centric approaches by using the emerging parallel language, OpenCL. Although the field of high level synthesis has evolved greatly in the last few decades, several fundamental parts were missing from the complete software abstraction of the FPGA. These include standard and portable methods of describing HW/SW codesign, memory hierarchy, data movement and control of parallelism. We believe that OpenCL addresses all of these issues and allows for highly efficient description of FPGA designs with a higher level of abstraction. We demonstrate this premise by examining the performance of a document filtering algorithm, implemented in OpenCL and automatically compiled to a Stratix IV 530 FPGA. We show that our implementation achieves 5.5× and 5.25× better performance per watt ratios than GPU and CPU implementations, respectively.},
author = {Chen, Doris and Singh, Deshanand},
doi = {10.1109/FPL.2012.6339171},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Singh - 2012 - Invited paper Using OpenCL to evaluate the efficiency of CPUS, GPUS and FPGAs for information filtering(2).pdf:pdf},
isbn = {9781467322560},
journal = {Proc. - 22nd Int. Conf. F. Program. Log. Appl. FPL 2012},
keywords = {CPUs,Efficiency,FPGAs,GPUs,Information Filtering,OpenCL},
pages = {5--12},
title = {{Invited paper: Using OpenCL to evaluate the efficiency of CPUS, GPUS and FPGAs for information filtering}},
year = {2012}
}
@inproceedings{Ceska2017,
abstract = {{\textcopyright} 2017 IEEE. We present a novel method allowing one to approximate complex arithmetic circuits with formal guarantees on the approximation error. The method integrates in a unique way formal techniques for approximate equivalence checking into a search-based circuit optimisation algorithm. The key idea of our approach is to employ a novel search strategy that drives the search towards promptly verifiable approximate circuits. The method was implemented within the ABC tool and extensively evaluated on functional approximation of multipliers (with up to 32-bit operands) and adders (with up to 128-bit operands). Within a few hours, we constructed a high-quality Pareto set of 32-bit multipliers providing trade-offs between the circuit error and size. This is for the first time when such complex approximate circuits with formal error guarantees have been derived, which demonstrates an outstanding performance and scalability of our approach compared with existing methods that have either been applied to the approximation of multipliers limited to 8-bit operands or statistical testing has been used only. Our approach thus significantly improves capabilities of the existing methods and paves a way towards an automated design process of provably-correct circuit approximations.},
author = {Ceska, Milan and Matyas, Jiri and Mrazek, Vojtech and Sekanina, Lukas and Vasicek, Zdenek and Vojnar, Tomas},
booktitle = {IEEE/ACM Int. Conf. Comput. Des. Dig. Tech. Pap. ICCAD},
doi = {10.1109/ICCAD.2017.8203807},
isbn = {9781538630938},
issn = {10923152},
title = {{Approximating complex arithmetic circuits with formal error guarantees: 32-bit multipliers accomplished}},
volume = {2017-Novem},
year = {2017}
}
@article{Canny1986,
abstract = {This paper describes a computational approach to edge detection. The success of the approach depends on the definition of a comprehensive set of goals for the computation of edge points. These goals must be precise enough to delimit the desired behavior of the detector while making minimal assumptions about the form of the solution. We define detection and localization criteria for a class of edges, and present mathematical forms for these criteria as functionals on the operator impulse response. A third criterion is then added to ensure that the detector has only one response to a single edge. We use the criteria in numerical optimization to derive detectors for several common image features, including step edges. On specializing the analysis to step edges, we find that there is a natural uncertainty principle between detection and localization performance, which are the two main goals. With this principle we derive a single operator shape which is optimal at any scale. The optimal detector has a simple approximate implementation in which edges are marked at maxima in gradient magnitude of a Gaussian-smoothed image. We extend this simple detector using operators of several widths to cope with different signal-to-noise ratios in the image. We present a general method, called feature synthesis, for the fine-to-coarse integration of information from operators at different scales. Finally we show that step edge detector performance improves considerably as the operator point spread function is extended along the edge.},
author = {Canny, J},
doi = {10.1109/TPAMI.1986.4767851},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Canny - 1986 - A computational approach to edge detection.pdf:pdf},
isbn = {978-1-4244-7657-2},
issn = {0162-8828},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
number = {6},
pages = {679--698},
pmid = {21869365},
title = {{A computational approach to edge detection.}},
volume = {8},
year = {1986}
}
@misc{burke2004Patent,
abstract = {A computer system comprising mass storage, a system bus connected to the mass storage, and a processor unit connected to the system bus. A library of standard functions is stored in the mass storage. Each library function is stored in at least one of two versions. The first version is obtained from compilation of firmware code, as is conventional. The second version is obtained from compilation of firmware code and comprises a set of configuration data for loading into a field programmable gate array (FPGA). The computer system is provided with a FPGA connected to the system bus which can be configured by the second versions of the library functions so that these can be performed in the FPGA, instead of in the processor. The apparatus and method are well suited to libraries of database search engine functions. Performance advantages can be obtained by executing function calls in the FPGA.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Burke, David},
doi = {10.1074/JBC.274.42.30033.(51)},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Burke - 2004 - Method and apparatus for executing standard functions in a computer system using a field programmable gate array(3).pdf:pdf},
isbn = {1111111111},
issn = {1111111111},
keywords = {GB2352548B (Grant),UK Version   Other versions  GB9917511D0 (Grant),UK Version Other versions GB9917511D0 (Grant),US Version},
mendeley-tags = {GB2352548B (Grant),UK Version   Other versions  GB9917511D0 (Grant),UK Version Other versions GB9917511D0 (Grant),US Version},
pmid = {15003161},
publisher = {UK PTO},
title = {{Method and apparatus for executing standard functions in a computer system using a field programmable gate array}},
url = {http://www.google.co.uk/patents/US6704816{\#}forward-citations https://patents.google.com/patent/GB2352548A/en?q=method+apparatus{\&}q=executing{\&}q=standard{\&}q=functions{\&}q=computer{\&}q=system{\&}q=using{\&}q=field+programmable+gate+array{\&}inventor=David+Burke{\&}country=GB},
year = {2004}
}
@inproceedings{Burke2016,
abstract = {Technological innovation, coupled with the affordability of digital systems, has revolutionised the industrial age of electronics. A growing number of electronic systems today typically integrate multiple interconnected cores with various other peripherals depending on the applications. Over the years, the dimensionality of these applications and their underlying hardware platforms have been increasing expansively. The applications are becoming more far-reaching, connected, pervasive and compute-intensive. The hardware platform design is also increasingly becoming more challenging due to their conflicting design trade-offs between power, performance and reliability. This is further exacerbated by additional maintainability and sustainability requirements, often with a short time-to-market needs. Such proliferation of applications and hardware systems design challenges are likely to affect and potentially define how we do research and impart education to current and future generations of electronic engineers. This paper highlights these key research and education challenges relevant in the era of heterogeneous many-core applications.},
author = {Burke, Dave and Shafik, Rishad A. and Yakovlev, Alex},
booktitle = {2016 11th Eur. Work. Microelectron. Educ. EWME 2016},
doi = {10.1109/EWME.2016.7496480},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Burke, Shafik, Yakovlev - 2016 - Challenges and opportunities in research and education of heterogeneous many-core applications(3).pdf:pdf},
isbn = {9781467385848},
keywords = {Heterogeneous systems,electronics education,hardware/software co-design,many-core applications,run-time management},
month = {jun},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Challenges and opportunities in research and education of heterogeneous many-core applications}},
year = {2016}
}
@article{BurkeCOREd,
abstract = {Technological innovation, coupled with the affordability of digital systems, has revolutionised the industrial age of electronics. A growing number of electronic systems today typically integrate multiple interconnected cores with various other peripherals depending on the applications. Over the years, the dimensionality of these applications and their underlying hardware platforms have been increasing expansively. The applications are becoming more far-reaching, connected, pervasive and compute-intensive. The hardware platform design is also increasingly becoming more challenging due to their conflicting design trade-offs between power, performance and reliability. This is further exacerbated by additional maintainability and sustainability requirements, often with a short time-to-market needs. Such proliferation of applications and hardware systems design challenges are likely to affect and potentially define how we do research and impart education to current and future generations of electronic engineers. This paper highlights these key research and education challenges relevant in the era of heterogeneous many-core applications.},
author = {Burke, Dave and Shafik, R.A. A Rishad A. and Yakovlev, Alex},
doi = {10.1109/EWME.2016.7496480},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Burke, Shafik, Yakovlev - 2016 - Challenges and Opportunities in Research and Education of Heterogeneous Many-Core Applications.pdf:pdf},
isbn = {9781467385848},
issn = {9781467385848},
journal = {2016 11th Eur. Work. Microelectron. Educ. EWME 2016},
keywords = {Heterogeneous systems,electronics education,hardware/software co-design,many-core applications,run-time management},
pages = {1--6},
title = {{Challenges and Opportunities in Research and Education of Heterogeneous Many-Core Applications}},
year = {2016}
}
@article{Burke2017,
abstract = {{\textcopyright}2017 ACM. With increasing resolutions the volume of data generated by image processing applications is escalating dramatically. When coupled with real-Time performance requirements, reducing energy con-sumption for such a large volume of data is proving challenging.},
author = {Burke, Dave and Jenkus, Dainius and Qiqieh, Issa and Shafik, Rishad and Das, Shidhartha and Yakovlev, Alex and Shaffk, R. and Das, Shidhartha and Yakovlev, Alex},
doi = {10.1145/3125502.3125554},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Burke et al. - 2017 - Significance-driven adaptive approximate computing for energy-efficient image processing applications(3).pdf:pdf},
isbn = {9781450351850},
journal = {Proc. Twelfth IEEE/ACM/IFIP Int. Conf. Hardware/Software Codesign Syst. Synth. Companion - CODES '17},
pages = {1--2},
title = {{Significance-driven adaptive approximate computing for energy-efficient image processing applications}},
url = {http://dl.acm.org/citation.cfm?doid=3125502.3125554},
volume = {1},
year = {2017}
}
@inproceedings{Burke2017a,
abstract = {{\textcopyright}2017 ACM. With increasing resolutions the volume of data generated by image processing applications is escalating dramatically. When coupled with real-Time performance requirements, reducing energy con-sumption for such a large volume of data is proving challenging.},
author = {Burke, Dave and Jenkus, Dainius and Qiqieh, Issa and Shafik, Rishad and Das, Shidhartha and Yakovlev, Alex},
doi = {10.1145/3125502.3125554},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Burke et al. - 2017 - Significance-driven adaptive approximate computing for energy-efficient image processing applications(4).pdf:pdf},
month = {oct},
pages = {1--2},
publisher = {Association for Computing Machinery (ACM)},
title = {{Significance-driven adaptive approximate computing for energy-efficient image processing applications}},
year = {2017}
}
@misc{burkeThesisSW,
abstract = {OpenCV software to demonstrate approximate significance to identify significant areas in an image thereby allowing a dual threshold based adaptive kernel filtering 3x3 or 5x5 based on the significance level in relation to the threshold. The percentage of data to be processed can be selected via slider system and additionally DVFS can be used on an odroid XU4 board to reduce power if there is sufficient slack time available},
author = {Burke, Dave},
title = {{Methods for Energy Efficient Image Processing using significance learning, software demonstrator}},
url = {https://github.com/1dbup/ThesisSW},
year = {2019}
}
@misc{SkyVideo,
abstract = {Application of demo software to a video clip},
author = {Burke, Dave},
title = {{Application of Adaptive Approximate Image Significance}},
url = {https://youtu.be/rx6hIXdVcI4}
}
@misc{SDAACVideo,
abstract = {A video showing how the demonstrator software works},
author = {Burke, Dave},
title = {{How does Approximate Significance Image Processing work?}},
url = {https://youtu.be/kbKhU7CvEb8}
}
@article{Bui2017,
abstract = {In recent years, power consumption has become one of the hottest research trends in computer science and industry. Most of the reasons are related to the operational budget and the environmental issues. In this paper, we would like to propose an energy-efficient solution for orchestrating the resource in cloud computing. In nature, the proposed approach firstly predicts the resource utilization of the upcoming period based on the Gaussian process regression method. Subsequently, the convex optimization technique is engaged to compute an appropriate quantity of physical servers for each monitoring window. This quantity of interest is calculated to ensure that a minimum number of servers can still provide an acceptable quality of service. Finally, a corresponding migrating instruction is issued to stack the virtual machines and turn off the idle physical servers to achieve the objective of energy savings. In order to evaluate the proposed method, we conduct the experiments using synthetic data from 29-day period of Google traces and real workload from the Montage open-source toolkit. Through the evaluation, we show that the proposed approach can achieve a significant result in reducing the energy consumption as well as maintaining the system performance.},
author = {Bui, Dinh-Mao and Yoon, YongIk and Huh, Eui-Nam and Jun, SungIk and Lee, Sungyoung},
doi = {10.1016/j.jpdc.2016.11.011},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bui et al. - 2017 - Energy efficiency for cloud computing system based on predictive optimization.pdf:pdf},
issn = {07437315},
journal = {J. Parallel Distrib. Comput.},
pages = {103--114},
publisher = {Elsevier Inc.},
title = {{Energy efficiency for cloud computing system based on predictive optimization}},
url = {http://dx.doi.org/10.1016/j.jpdc.2016.11.011},
volume = {102},
year = {2017}
}
@techreport{Buchner,
author = {B{\"{u}}chner, Frank},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/B{\"{u}}chner - Unknown - Unit Testing improves Software Quality Unit Testing and the Classification Tree Method.pdf:pdf},
title = {{Unit Testing improves Software Quality Unit Testing and the Classification Tree Method {\textgreater}}}
}
@inproceedings{Brebner2015,
author = {Brebner, Gordon},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brebner - 2015 - Implementing SDN in FPGAs over All Gigabit-Level Ethernet Standards Ethernet trend many rates(2).pdf:pdf},
keywords = {Public},
number = {April},
title = {{Implementing SDN in FPGAs over All Gigabit-Level Ethernet Standards Ethernet trend : many rates}},
year = {2015}
}
@article{Brady2013,
abstract = {Metrics commonly used to assess the energy efficiency of data centres are analysed through performing and critiquing a case study calculation of energy efficiency. Specifically, the metric Power Usage Effectiveness (PUE), which has become a de facto standard within the data centre industry, will be assessed. This is achieved by using open source specifications for a data centre in Prineville, Oregon, USA provided by the Open Compute Project launched by the social networking company Facebook. The usefulness of the PUE metric to the IT industry is critically assessed and it is found that whilst it is important for encouraging lower energy consumption in data centres, it does not represent an unambiguous measure of energy efficiency. ?? 2013 Elsevier Ltd. All rights reserved.},
author = {Brady, Gemma A. and Kapur, Nikil and Summers, Jonathan L. and Thompson, Harvey M.},
doi = {10.1016/j.enconman.2013.07.035},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brady et al. - 2013 - A case study and critical assessment in calculating power usage effectiveness for a data centre.pdf:pdf},
issn = {01968904},
journal = {Energy Convers. Manag.},
keywords = {Data center,Efficiency,Metric,PUE,Power},
pages = {155--161},
publisher = {Elsevier Ltd},
title = {{A case study and critical assessment in calculating power usage effectiveness for a data centre}},
url = {http://dx.doi.org/10.1016/j.enconman.2013.07.035},
volume = {76},
year = {2013}
}
@book{Bradski2008,
abstract = {Learning OpenCV puts you right in the middle of the rapidly expanding field of computer vision. Written by the creators of OpenCV, the widely used free open- source library, this book introduces you to computer vision and demonstrates how you can quickly build applications that enable computers to "see" and make decisions based on the data. Computer vision is everywhere - in security systems, manufacturing inspection systems, medical image analysis, Unmanned Aerial Vehicles, and more. It helps robot cars drive by themselves, stitches Google maps and Google Earth together, checks the pixels on your laptop's LCD screen, and makes sure the stitches in your shirt are OK. OpenCV provides an easy-to-use computer vision infrastructure along with a comprehensive library containing more than 500 functions that can run vision code in real time. With Learning OpenCV, any developer or hobbyist can get up and running with the framework quickly, whether it's to build simple or sophisticated vision applications. The book includes: A thorough introduction to OpenCV Getting input from cameras Transforming images Shape matching Pattern recognition, including face detection Segmenting images Tracking and motion in 2 and 3 dimensions Machine learning algorithms Hands-on exercises at the end of each chapter help you absorb the concepts, and an appendix explains how to set up an OpenCV project in Visual Studio. OpenCV is written in performance optimized C/C++ code, runs on Windows, Linux, and Mac OS X, and is free for commercial and research use under a BSD license. Getting machines to see is a challenging but entertaining goal. If you're intrigued by the possibilities, Learning OpenCV gets you started onbuilding computer vision applications of your own.},
author = {Bradski, Gary and Kaehler, Adrian},
booktitle = {OReilly Media Inc},
doi = {10.1109/MRA.2009.933612},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bradski, Kaehler - 2008 - Learning OpenCV Computer Vision with the OpenCV Library(2).pdf:pdf},
isbn = {0596516134},
issn = {10709932},
pages = {555},
title = {{Learning OpenCV: Computer Vision with the OpenCV Library}},
url = {http://www.amazon.com/dp/0596516134},
volume = {1},
year = {2008}
}
@article{Borgione2015,
author = {Borgione, Gaetano and Plumgrid, Distinguished Engineer},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Borgione, Plumgrid - 2015 - Open Source Networking for Cloud Data Centers Agenda  Open Source Clouds with OpenStack  Building Block(2).pdf:pdf},
keywords = {Borgione2015OSNforCDC},
mendeley-tags = {Borgione2015OSNforCDC},
number = {April},
title = {{Open Source Networking for Cloud Data Centers Agenda  Open Source Clouds with OpenStack  Building Blocks of Cloud Networking • Tenant Networks}},
year = {2015}
}
@article{Binkert2011,
abstract = {The gem5 simulation infrastructure is the merger of the best aspects of the M5 [4] and GEMS [9] simulators. M5 provides a highly configurable simulation framework, multiple ISAs, and diverse CPU models. GEMS complements these features with a detailed and flexible memory system, including support for multiple cache coherence protocols and interconnect models. Currently, gem5 supports most commercial ISAs (ARM, ALPHA, MIPS, Power, SPARC, and x86), including booting Linux on three of them (ARM, ALPHA, and x86). The project is the result of the combined efforts of many academic and industrial institutions, including AMD, ARM, HP, MIPS, Princeton, MIT, and the Universities of Michigan, Texas, and Wisconsin. Over the past ten years, M5 and GEMS have been used in hundreds of publications and have been downloaded tens of thousands of times. The high level of collaboration on the gem5 project, combined with the previous success of the component parts and a liberal BSD-like license, make gem5 a valuable full-system simulation tool.},
author = {Binkert, Nathan and Sardashti, Somayeh and Sen, Rathijit and Sewell, Korey and Shoaib, Muhammad and Vaish, Nilay and Hill, Mark D. M.D. D and Wood, David A. D.A. A and Beckmann, Bradford and Black, Gabriel and Reinhardt, Steven K. S.K. K and Saidi, Ali and Basu, Arkaprava and Hestness, Joel and Hower, Derek R. and Krishna, Tushar and Basil, A. and Hestness, Joel and Hower, Derek R. and Krishna, Tushar and Sardashti, Somayeh and Sen, Rathijit and Sewell, Korey and Shoaib, Muhammad and Vaish, Nilay and Hill, Mark D. M.D. D and Wood, David A. D.A. A},
doi = {10.1145/2024716.2024718},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Binkert et al. - 2011 - The gem5 Simulator.pdf:pdf},
isbn = {0163-5964},
issn = {0163-5964},
journal = {Comput. Archit. News},
number = {2},
pages = {1},
title = {{The gem5 Simulator}},
url = {http://www.engineeringvillage.com/blog/document.url?mid=inspec{\_}ef5502133370e5bf3M6a022061377553{\&}database=ins{\%}5Cnhttps://www.engineeringvillage.com/blog/document.url?mid=inspec{\_}ef5502133370e5bf3M6a022061377553{\&}database=ins{\%}5Cnhttp://dl.acm.org/citation.cfm},
volume = {39},
year = {2011}
}
@misc{BillJenkinsAltera,
abstract = {Slides 13-25 show how a simple software addition task can be broken down from six instructions to 4 registers and two additions.},
author = {{Bill Jenkins (Altera)}},
title = {{Altera OpenCL Overview webcast}},
url = {https://www.altera.com/webcasts/opencl-overview/presentation.html?utm{\_}source=Altera{\&}utm{\_}medium=webinar{\&}utm{\_}campaign=OpenCL{\_}15{\_}0{\&}utm{\_}content=NA{\_}OpenCL{\_}Overview{\_}Tutorial}
}
@article{Beitelmal2014,
abstract = {A thermodynamic approach for evaluating energy performance (productivity) of information technology (IT) servers and data centers is presented. This approach is based on the first law efficiency to deliver energy performance metrics defined as the ratio of the useful work output (server utilization) to the total energy expanded to support the corresponding computational work. These energy performance metrics will facilitate proper energy evaluation and can be used as indicators to rank and classify IT systems and data centers regardless of their size, capacity or physical location. The current approach utilizes relevant and readily available information such as the total facility power, the servers' idle power, the average servers' utilization, the cooling power and the total IT equipment power. Experimental simulations and analysis are presented for a single and a dual-core IT server, and similar analysis is extended to a hypothetical data center. The current results show that the server energy efficiency increases with increasing CPU utilization and is higher for a multi-processor server than for a single-processor server. This is also true at the data center level however with a lower relative performance indicator value than for the server level. {\textcopyright} 2014 Elsevier B.V.},
author = {Beitelmal, A. H. and Fabris, D.},
doi = {10.1016/j.enbuild.2014.04.036},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Beitelmal, Fabris - 2014 - Servers and data centers energy performance metrics.pdf:pdf},
isbn = {0378-7788},
issn = {03787788},
journal = {Energy Build.},
keywords = {Cooling,DC performance metric,DC productivity metric,Data center,Data centre: Power,Distribution,Efficiency metric,Energy,PUE,Power,Server efficiency},
mendeley-tags = {Data centre: Power,Distribution},
pages = {562--569},
publisher = {Elsevier B.V.},
title = {{Servers and data centers energy performance metrics}},
url = {http://dx.doi.org/10.1016/j.enbuild.2014.04.036},
volume = {80},
year = {2014}
}
@misc{beckett1998apparatus,
annote = {US Patent 5,852,502},
author = {Beckett, John Patrick},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Beckett - 1998 - Apparatus and method for digital camera and recorder having a high resolution color composite image output.pdf:pdf},
keywords = {WO1997046001A1},
mendeley-tags = {WO1997046001A1},
publisher = {Google Patents},
title = {{Apparatus and method for digital camera and recorder having a high resolution color composite image output}},
url = {https://patents.google.com/patent/WO1997046001A1/en?q=digital+camera+and+recorder{\&}assignee=Beckett},
year = {1998}
}
@article{Becker2015,
author = {Becker, David and Mcmullen, Bill and King, Trish Dunn},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Becker, Mcmullen, King - 2015 - BIG DATA , BIG DATA QUALITY PROBLEM(2).pdf:pdf},
isbn = {9781479999262},
keywords = {as well,big data,case studies for a,data,data quality,data quality problems,diverse set of big,domains,initiatives in different functional,of big data quality,returns to scale,team probed both traditional,the investigating},
pages = {2644--2653},
title = {{BIG DATA , BIG DATA QUALITY PROBLEM}},
year = {2015}
}
@article{Baum2016,
author = {Baum, Felix and Raghuraman, Arvind and Baum, Felix},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baum, Raghuraman, Baum - 2016 - Making Full use of Emerging ARM-based Heterogeneous Multicore SoCs Making Full use of Emerging ARM-based.pdf:pdf},
keywords = {ARM,ARM cores,Heterogeneous multicore systems,s},
title = {{Making Full use of Emerging ARM-based Heterogeneous Multicore SoCs Making Full use of Emerging ARM-based Heterogeneous Multicore SoCs}},
year = {2016}
}
@article{Batni2014,
abstract = {Storing digital information, ensuring the accuracy, steady and uninterrupted access to the data are considered as fundamental challenges in enterprise-class organizations and companies. In recent years, new types of storage systems such as solid state disks (SSD) have been introduced. Unlike hard disks that have mechanical structure, SSDs are based on flash memory and thus have electronic structure. Generally a SSD consists of a number of flash memory chips, some buffers of the volatile memory type, and an embedded microprocessor, which have been interconnected by a port. This microprocessor run a small file system which called flash translation layer (FTL). This software controls and schedules buffers, data transfers and all flash memory tasks. SSDs have some advantages over hard disks such as high speed, low energy consumption, lower heat and noise, resistance against damage, and smaller size. Besides, some disadvantages such as limited endurance and high price are still challenging. In this study, the effort is to combine two common technologies - SLC and MLC chips - used in the manufacture of SSDs in a single SSD to decrease the side effects of current SSDs. The idea of using multi-layer SSD is regarded as an efficient solution in this field.},
author = {Batni, Arash and Safaei, Farshad},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Batni, Safaei - 2014 - A NEW MULTI-TIERED SOLID STATE DISK USING SLC MLC COMBINED FLASH MEMORY.pdf:pdf},
journal = {Int. J. Comput. Sci. Eng. Inf. Technol.},
keywords = {FTL,Flash,MLC,SLC,SSD,cost comparison},
mendeley-tags = {FTL,Flash,MLC,SLC,SSD,cost comparison},
number = {2},
pages = {11--23},
title = {{A NEW MULTI-TIERED SOLID STATE DISK USING SLC / MLC COMBINED FLASH MEMORY}},
volume = {4},
year = {2014}
}
@article{Barroso2007,
abstract = {Energy-proportional designs would enable large energy savings in servers,potentially doubling their efficiency in real-life use.Achieving energy proportionality will require significant improvements in the energy usage profile of every system component, particularly the memory and disk subsystems.},
annote = {Googles' experience of server computation and power efficiency and the need to improve energy proportionality in data centres.},
author = {Barroso, Luiz Andr{\'{e}} and H{\"{o}}lzle, Urs},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Barroso, H{\"{o}}lzle - 2007 - The Case for Energy-Proportional Computing.pdf:pdf},
journal = {IEEE Comput.},
number = {12},
pages = {33--37},
title = {{The Case for Energy-Proportional Computing}},
volume = {40},
year = {2007}
}
@article{Bania2018,
abstract = {Data is generally represented by high dimensional feature vectors in many areas, such as pattern recognition, data mining and machine learning. Classification of useful knowledge in high dimensional data collections is an important and demanding area. Rough set theory, is a significant component of soft computing paradigm for data analysis based on classification of objects of interest into similarity classes, which are indiscernible with respect to some features. This theory offers fundamental concepts of attribute (feature) reduction. In this work supervised feature selection algorithms using Rough set theory which falls under filter method is studied. An enhanced version of Rough set theory based algorithm is proposed which exploits the lower approximation, dependency and significance measure of attributes. The experimental analysis for the proposed method is performed on five data sets of UCI machine learning repository. The performance of the reduced data set is measured by the classification accuracy and it is evaluated using WEKA classifier tool. Result analysis and comparison shows the efficiency of the proposed algorithm. {\textcopyright} 2017 IEEE.},
annote = {A mathematical exploration of approximate significance in large datasets for analysis by Neural networks, for use on Large Compute networks utilising Rough Set Theory.},
author = {Bania, Rubul Kumar},
doi = {10.1109/ICECDS.2017.8389865},
file = {:C$\backslash$:/Users/D/Documents/Uni/Papers/PID4942133.pdf:pdf},
isbn = {9781538618868},
journal = {2017 Int. Conf. Energy, Commun. Data Anal. Soft Comput. ICECDS 2017Bania2018},
keywords = {Feature selection,Filter,Lower approximate,Rough Set,Significance},
pages = {2309--2314},
title = {{An effective supervised filter based feature selection algorithm using rough set theory}},
year = {2018}
}
@article{Aziz2005,
author = {Aziz, Pervez M},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aziz - 2005 - Coding and Signal Processing for Magnetic Recording Systems.pdf:pdf},
isbn = {9781479923588},
pages = {194--199},
title = {{Coding and Signal Processing for Magnetic Recording Systems}},
year = {2005}
}
@article{Ayub2017,
author = {Ayub, Muhammad Kamran and Hasan, Osman and Shafique, Muhammad},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ayub, Hasan, Shafique - 2017 - Statistical Error Analysis for Low Power Approximate Adders(2).pdf:pdf},
isbn = {9781450349277},
journal = {Dac},
keywords = {accuracy,approximate computing,error,low power,performance,probabilis-,scalability,tic analysis},
title = {{Statistical Error Analysis for Low Power Approximate Adders}},
year = {2017}
}
@article{Ashlock2015QoSIoT,
author = {Ashlock, Hans and Marketing, Technical},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ashlock, Marketing - 2015 - Characterizing and Validating QoS in the Emerging IoT Quali Company Overview(2).pdf:pdf},
title = {{Characterizing and Validating QoS in the Emerging IoT Quali Company Overview}},
year = {2015}
}
@article{ARMSecurityTechnology2008,
author = {{ARM Security Technology}},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/ARM Security Technology - 2008 - Building a Secure System using {\{}TrustZone{\}} Technology.pdf:pdf},
title = {{Building a Secure System using {\{}TrustZone{\}} Technology}},
year = {2008}
}
@article{ARMLimited2014,
abstract = {ARM Architecture Reference Manual},
author = {{ARM Limited}},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/ARM Limited - 2014 - ARMv7-A and ARMv7-R edition.pdf:pdf},
keywords = {ARM11MPCore,ARM7EJ-S,Classic,Cortex-A},
pages = {2004--2012},
title = {{ARMv7-A and ARMv7-R edition}},
year = {2014}
}
@incollection{Limited2013,
annote = {From Duplicate 2 (NEON Programmer's Guide - Limited, ARM)

read up to 2.2},
author = {{ARM Limited}},
isbn = {9781439806104},
keywords = {NEON},
title = {{NEON Programmer's Guide}},
year = {2013}
}
@article{Arm2013,
author = {Arm},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arm - 2013 - ARM Cortex-A Series Programmer's Guide.pdf:pdf},
keywords = {Cortex-A},
title = {{ARM Cortex-A Series: Programmer's Guide}},
year = {2013}
}
@article{Arm2017,
abstract = {A battle is raging to keep systems secure as we race to realize the immense value data insights can bring. As part of this battle, technology companies have a responsibility to society that extends beyond just delivering products. In our Manifesto document, we describe how the threat to the data-driven world is increasing and detail technical directions we can follow to confront that risk. Beyond that, we explore the nature of that responsibility as guardians of the Information Revolution and discuss the Social Contract all technology providers need to rally behind.},
author = {Arm},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arm - 2017 - Security Manifesto.pdf:pdf},
pages = {14},
title = {{Security Manifesto}},
url = {https://pages.arm.com/iot-security-manifesto.html},
year = {2017}
}
@article{ARM2013,
author = {ARM},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/ARM - 2013 - Cortex-A15 Technical Reference Manual. Revision r4p0.pdf:pdf},
keywords = {Cortex-A,Cortex-A15},
title = {{Cortex-A15 Technical Reference Manual. Revision r4p0.}},
year = {2013}
}
@article{Ansel2011,
abstract = {Approximating ideal program outputs is a common technique for solving computationally difficult problems, for adhering to processing or timing constraints, and for performance optimization in situations where perfect precision is not necessary. To this end, programmers often use approximation algorithms, iterative methods, data resampling, and other heuristics. However, programming such variable accuracy algorithms presents difficult challenges since the optimal algorithms and parameters may change with different accuracy requirements and usage environments. This problem is further compounded when multiple variable accuracy algorithms are nested together due to the complex way that accuracy requirements can propagate across algorithms and because of the size of the set of allowable compositions. As a result, programmers often deal with this issue in an ad-hoc manner that can sometimes violate sound programming practices such as maintaining library abstractions. In this paper, we propose language extensions that expose trade-offs between time and accuracy to the compiler. The compiler performs fully automatic compile-time and installtime autotuning and analyses in order to construct optimized algorithms to achieve any given target accuracy. We present novel compiler techniques and a structured genetic tuning algorithm to search the space of candidate algorithms and accuracies in the presence of recursion and sub-calls to other variable accuracy code. These techniques benefit both the library writer, by providing an easy way to describe and search the parameter and algorithmic choice space, and the library user, by allowing high level specification of accuracy requirements which are then met automatically without the need for the user to understand any algorithm-specific parameters. Additionally, we present a new suite of benchmarks, written in our language, to examine the efficacy of our techniques. Our experimental results show that by relaxing accuracy requirements, we can easily obtain performance improvements ranging from 1.1{\&}{\#}x00D7; to orders of magnitude of speedup.},
author = {Ansel, Jason and Wong, Yee Lok and Chan, Cy and Olszewski, Marek and Edelman, Alan and Amarasinghe, Saman},
doi = {10.1109/CGO.2011.5764677},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ansel et al. - 2011 - Language and compiler support for auto-tuning variable-accuracy algorithms.pdf:pdf},
isbn = {9781612843551},
journal = {Proc. - Int. Symp. Code Gener. Optim. CGO 2011},
pages = {85--96},
title = {{Language and compiler support for auto-tuning variable-accuracy algorithms}},
year = {2011}
}
@misc{amit2014real,
abstract = {Exemplary method, system, and computer program product embodiments for real-time image recompression are pro vided. In one embodiment, by way of example only, com pressed segments that are identified as having used at least one of a plurality of inefficient compression operations are partially decoded. The compressed segments are recom pressed by employing context mapping. Additional system and computer program product embodiments are disclosed and provide related advantages.},
annote = {{\{}US{\}} Patent 8,682,091},
author = {Amit, Jonathan and Demidov, Lilia and Halowani, Nir and Shalev, Ori},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Amit et al. - 2014 - Real-time image compression(2).pdf:pdf},
isbn = {2009009274},
number = {19},
publisher = {Google Patents},
title = {{Real-time image compression}},
volume = {1},
year = {2014}
}
@inproceedings{Ambrosia2016,
author = {Ambrosia, John D and Kipp, Scott G},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ambrosia, Kipp - 2016 - 2016 Ethernet Roadmap.pdf:pdf},
keywords = {2016,400G,ethernet,roadmap},
mendeley-tags = {2016,400G,ethernet,roadmap},
title = {{2016 Ethernet Roadmap}},
year = {2016}
}
@inproceedings{Ambrosia2015EnMap,
annote = {Useful Ethernet speed roadmap},
author = {Ambrosia, John D and Kipp, Scott G},
booktitle = {Ethernet Technol. summit},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ambrosia, Kipp - 2015 - The 2015 Ethernet Roadmap.pdf:pdf},
title = {{The 2015 Ethernet Roadmap}},
year = {2015}
}
@article{Ambrosia2015a,
author = {Ambrosia, John D},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/D'Ambrosia - 2010 - 100 Gigabit Ethernet and Beyond(2).pdf:pdf},
keywords = {No Restrictions},
title = {{DIFFERENT STROKES FOR DIFFERENT FOLKS Regarding the Views Expressed}},
year = {2015}
}
@article{Altera2013,
author = {Altera},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Altera - 2013 - Cyclone V SoC Development Kit User Guide(3).pdf:pdf},
isbn = {1600321003},
keywords = {Cyclone V,FPGA,SoCFPGA,SoCkit,developme,development kit,transceiver},
mendeley-tags = {FPGA,SoCFPGA,SoCkit},
title = {{Cyclone V SoC Development Kit User Guide}},
volume = {4},
year = {2013}
}
@book{Altera2013CVSocSchem,
author = {Altera},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Altera - 2013 - Cyclone V SoC FPGA Development Kit Board D1 Schematics.pdf:pdf},
isbn = {1600321003},
keywords = {Cyclone V,FPGA,SoCFPGA,SoCkit,dev kit,development kit,transceiver},
mendeley-tags = {SoCFPGA,dev kit},
title = {{Cyclone V SoC FPGA Development Kit Board D1 Schematics}},
volume = {4},
year = {2013}
}
@article{Altera2013CVSocRefMan,
author = {Altera},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Altera - 2013 - Cyclone V SoC Development Board Reference Manual.pdf:pdf},
isbn = {1600321003},
keywords = {Cyclone V,FPGA,SoCFPGA,SoCkit,developme,development kit,ref man,transceiver},
mendeley-tags = {FPGA,SoCFPGA,SoCkit,ref man},
title = {{Cyclone V SoC Development Board Reference Manual}},
volume = {4},
year = {2013}
}
@article{Altera2014,
abstract = {The Altera SoC EDS is a comprehensive tool suite for embedded software development on Altera SoC devices. The Altera SoC EDS contains development tools, utility programs, run-time software, and application examples that enable firmware and application software development on the Altera SoC hardware platform.},
author = {Altera},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Altera - 2014 - Altera SoC Embedded Design Suite User Guide.pdf:pdf},
keywords = {FPGA,SoCFPGA,SoCkit},
mendeley-tags = {FPGA,SoCFPGA,SoCkit},
pages = {159},
title = {{Altera SoC Embedded Design Suite User Guide}},
url = {http://www.altera.com/literature/ug/ug{\_}soc{\_}eds.pdf},
year = {2014}
}
@article{ALTERA2011,
author = {ALTERA},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/ALTERA - 2011 - Embedded Peripherals IP User Guide.pdf:pdf},
journal = {Altera Corp.},
keywords = {FPGA,IP,Quartus II,SOPC Builder,embedded peripherals},
mendeley-tags = {FPGA,IP},
number = {June},
pages = {358},
title = {{Embedded Peripherals IP User Guide}},
url = {https://www.altera.com/literature/ug},
year = {2011}
}
@article{Almurib2018,
author = {Almurib, Haider A.F. F and Kumar, Thulasiraman Nandha and Lombardi, Fabrizio},
doi = {10.1109/TC.2017.2731770},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Almurib, Kumar, Lombardi - 2018 - Approximate DCT Image Compression Using Inexact Computing.pdf:pdf},
issn = {00189340},
journal = {IEEE Trans. Comput.},
keywords = {Approximate computing,DCT,image compression,inexact computing},
number = {2},
pages = {149--159},
title = {{Approximate DCT Image Compression Using Inexact Computing}},
volume = {67},
year = {2018}
}
@inproceedings{Massimo2017,
author = {Alioto, Massimo},
booktitle = {Des. Autom. Test Eur.},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Alioto - 2017 - Energy-Quality Scalable Adaptive VLSI Circuits and Systems beyond Approximate Computing.pdf:pdf},
organization = {IEEE},
pages = {(in press)},
title = {{Energy-Quality Scalable Adaptive VLSI Circuits and Systems beyond Approximate Computing}},
year = {2017}
}
@inproceedings{alawad2014energy,
abstract = {Energy efficiency and algorithmic robustness typically are volume of imagery data and the algorithmic complexity of conflicting circuit characteristics, yet with CMOS technol- image processing are expected to increase exponentially in ogy scaling towards 10-nm feature size, both become criti- the coming years [1]. Therefore, it is imperative to design cal design metrics simultaneously for modern logic circuits. new methodology for domain transformation in DSP, which This paper propose a novel computing scheme hinged on not only have high performance but also can achieve high probabilistic domain transformation aiming for both low power algorithmic robustness. operation and fault resilience. In such a computing paradigm, algorithm inputs are first encoded through probabilistic means, 2 Probabilistic Domain Transformation which translates the input values into a number of random In DSP, domain transformations can yield equivalent re- samples. Subsequently, light-weight operations, such as sim- sults to input signals that would be hard to obtained if its ple additions will be performed onto these random samples original domain is used. For example, Fourier transform, in order to generate new random variables. Finally, the re- maybe the most importance domain transformation in DSP, sulting random samples will be decoded probabilistically to considers the representation and analysis of analog signals give the final results. and systems in the frequency domain. This is because the To validate the effectiveness of this proposed computing frequency domain can reveal further characteristics of sig- scheme, we presents a high-performance reconfigurable dis- nals and systems, i.e., the frequency content of an arbitrary crete convolver specifically designed for FPGA-based image aperiodic (or non-periodic) signal often referred to as the and video processors. While the conventional multiplier- spectrum. based architecture can only achieve O(N2), the proposed ar- Inspired by the Fourier transform, we propose a new do- chitecture, through the proposed probabilistic domain trans- main transformation called Probabilistic Domain Transfor- formation, can achieve approximately O(N) in algorithmic mation(PDT). In this method, the input discrete signal x[n] complexity, therefore highly scalable and energy efficient. In will be first converted into a serial of random samples Tx[m]) addition, the PDT methodology makes the proposed archi- through the transform P({\textperiodcentered}). Corresponding to the system tecture highly fault-tolerant because information to be pro- function H[j$\omega$] in the Fourier transform, the random sam- cessed is encoded with probability density function instead ples Tx[m]) are processed by F({\textperiodcentered}), subsequently generating of its binary forms. As such, the local perturbations of its new random samples Ty[m]. Finally, these newly gener- computing accuracy or signal values are inconsequential to ated random samples Ty[m]) can be decoded as the final its overall results. The convolver prototype implemented results y[n] by P−1({\textperiodcentered}). Obviously, in order to successfully with Virtex 6 FPGA devices (XC6VLX550t) requires just perform the proposed PDT, we need to clearly define what 4.09 µs to perform a 128 × 128 convolution and dissipates P({\textperiodcentered}), P−1({\textperiodcentered}), and F({\textperiodcentered}) represent, respectively. only 166.63 nJ in dynamic energy consumption at 250 MHz. This new architecture can be exploited in all the real-time applications in which energy-efficient convolutions are re- Light-Weight quired and it can be realized with many other FPGA device Algorithm Probabilistic Algorithmic Probabilistic Algorithm families.},
author = {Alawad, Mohammed and Bai, Yu and DeMara, Ronald and Lin, Mingjie},
booktitle = {Proc. 2014 ACM/SIGDA Int. Symp. Field-programmable gate arrays},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Alawad et al. - 2014 - Energy-efficient multiplier-less discrete convolver through probabilistic domain transformation(2).pdf:pdf},
organization = {ACM},
pages = {185--188},
title = {{Energy-efficient multiplier-less discrete convolver through probabilistic domain transformation}},
year = {2014}
}
@article{Abdelkader2017,
abstract = {Advances in semiconductor manufacturing processes and large scale integration keep pushing demanding applications further away from centralized processing, and closer to the edges of the network (i.e. Edge Computing). It has become possible to perform complex in-network image processing using low-power embedded smart cameras, enabling a multitude of new collaborative image processing applications. This paper introduces OpenMV, a new low-power smart camera that lends itself naturally to wireless sensor networks and machine vision applications. The uniqueness of this platform lies in running an embedded Python3 interpreter, allowing its peripherals and machine vision library to be scripted in Python. In addition, its hardware is extensible via modules that augment the platform with new capabilities, such as thermal imaging and networking modules.},
annote = {utilises a dual embedded processor, Cortex-M7 with a lightweight version of Python, MicroPython2013, to implement a low-cost, low-power image processing platform. Provides a comprehensive library containing some advanced image processing algorithms that can be easily implemented via Python},
author = {Abdelkader, Ibrahim and El-Sonbaty, Yasser and El-Habrouk, Mohamed},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abdelkader, El-Sonbaty, El-Habrouk - 2017 - Openmv a Python Powered, Extensible Machine Vision Camera.pdf:pdf},
isbn = {9789898533661},
keywords = {Embedded,Image Processing,Machine Vision,Python,Smart Camera,WSNs},
pages = {71--78},
title = {{Openmv: a Python Powered, Extensible Machine Vision Camera}},
url = {https://arxiv.org/ftp/arxiv/papers/1711/1711.10464.pdf},
year = {2017}
}
@article{Abaya2014,
author = {Abaya, Wilson Feipeng and Basa, Jimmy and Sy, Michael and Abad, Alexander C and Dadios, Elmer P},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abaya et al. - 2014 - Low Cost Smart Security Camera with Night Vision Capability Using Raspberry Pi and OpenCV(2).pdf:pdf},
isbn = {9781479940202},
keywords = {Human detect,Rasp Pi,Smoke detect},
mendeley-tags = {Human detect,Rasp Pi,Smoke detect},
number = {November},
title = {{Low Cost Smart Security Camera with Night Vision Capability Using Raspberry Pi and OpenCV}},
year = {2014}
}
@article{Aalsaud,
author = {Aalsaud, Ali and Shafik, Rishad and Rafiev, Ashur and Xia, Fie and Yang, Sheng and Yakovlev, Alex},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aalsaud et al. - Unknown - Power-Aware Performance Adaptation of Concurrent Applications in Heterogeneous Many-Core Systems.pdf:pdf},
isbn = {9781450341851},
title = {{Power-Aware Performance Adaptation of Concurrent Applications in Heterogeneous Many-Core Systems}}
}
@article{Aalsaud2018,
author = {Aalsaud, Ali and Rafiev, Ashur and Xia, Fei and Shafik, Rishad and Yakovlev, Alex},
doi = {10.1109/PATMOS.2018.8464142},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aalsaud et al. - 2018 - Model-Free Runtime Management of Concurrent Workloads for Energy-Efficient Many-Core Heterogeneous Systems.pdf:pdf},
isbn = {9781538663653},
journal = {2018 IEEE 28th Int. Symp. Power Timing Model. Optim. Simulation, PATMOS 2018},
keywords = {concurrent applications,many-core systems,power-normalized performance,runtime optimization,workload classification},
pages = {206--213},
publisher = {IEEE},
title = {{Model-Free Runtime Management of Concurrent Workloads for Energy-Efficient Many-Core Heterogeneous Systems}},
year = {2018}
}
@misc{Altera,
author = {(Altera and Editor in chief), Ron Wilson},
title = {{New Layers Form within the Cloud}},
url = {http://systemdesign.altera.com/new-layers-form-within-cloud/?utm{\_}source=Altera{\&}utm{\_}medium=email{\&}utm{\_}campaign=System{\_}Design{\_}Journal{\&}utm{\_}content={\_}{\&}elqTrackId=6D5F133003E2F8530387341BAFD513B2{\&}elq=7ef8b002e1984a15b96b2609484b050b{\&}elqaid=2212{\&}elqat=1{\&}elqCampai},
urldate = {2016-05-31}
}
@misc{Alteraa,
annote = {Interesting article on Convolutional Neural Networks},
author = {(Altera), Ron Wilson},
keywords = {CNN,Ron Wilson},
mendeley-tags = {CNN},
title = {{Neural Networks and the Rings of Power}},
url = {http://systemdesign.altera.com/neural-networks-rings-power/?utm{\_}source=Altera{\&}utm{\_}medium=email{\&}utm{\_}campaign=System{\_}Design{\_}Journal{\&}utm{\_}content={\_}{\&}elqTrackId=03FEF7B2C34F465DD36EC4530B584B8D{\&}elq=bc3ee2fcb43a4237abdf1469c3ac3ecd{\&}elqaid=2228{\&}elqat=1{\&}elqCampaig},
urldate = {2016-06-06}
}
@article{,
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2015 - Enabling a Connected Ethernet Ecosystem.pdf:pdf},
title = {{Enabling a Connected Ethernet Ecosystem}},
year = {2015}
}
@article{,
annote = {Highlights latency of Ethernet packet processing},
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2015 - Flexible Policy-Based Switching for 100G Ethernet Using Programmable Logic Gateware Defined Networking {\textregistered}.pdf:pdf},
number = {April},
title = {{Flexible Policy-Based Switching for 100G Ethernet Using Programmable Logic Gateware Defined Networking {\textregistered}}},
year = {2015}
}
@article{,
file = {:C$\backslash$:/Users/D/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2015 - Evolving Ethernet Speeds A Verification Minefield.pdf:pdf},
number = {April},
title = {{Evolving Ethernet Speeds A Verification Minefield}},
year = {2015}
}
@misc{,
title = {{Stratix 10 Secure Device Manager Provides Best-in-Class FPGA and SoC Security}},
url = {about:blank},
urldate = {2015-11-27}
}
